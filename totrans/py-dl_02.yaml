- en: Chapter 2. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 神经网络
- en: In the previous chapter, we described several machine learning algorithms and
    we introduced different techniques to analyze data to make predictions. For example,
    we suggested how machines can use data of home selling prices to make predictions
    on the price for new houses. We described how large companies, such as Netflix,
    use machine learning techniques in order to suggest to users new movies they may
    like based on movies they have liked in the past, using a technique that is widely
    utilized in e-commerce by giants such as Amazon or Walmart. Most of these techniques,
    however, necessitate labeled data in order to make predictions on new data, and,
    in order to improve their performance, need humans to describe the data in terms
    of features that make sense.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们描述了几种机器学习算法，并介绍了不同的技术来分析数据以进行预测。例如，我们建议机器可以使用房屋销售价格的数据来预测新房屋的价格。我们描述了大型公司，例如Netflix，如何使用机器学习技术来建议用户他们可能喜欢的新电影，这是一种在电子商务中被亚马逊或沃尔玛等巨头广泛使用的技术。然而，大多数这些技术需要有标签的数据才能对新数据进行预测，并且为了提高性能，需要人类描述数据以符合有意义的特征。
- en: 'Humans are able to quickly extrapolate patterns and infer rules without having
    the data cleaned and prepared for them. It would then be desirable if machines
    could learn to do the same. As we have discussed, Frank Rosenblatt invented the
    perceptron back in 1957, over 50 years ago. The perceptron is to modern deep neural
    nets what unicellular organisms are to complex multi-cellular lifeforms, and yet
    it is quite important to understand and become familiar with how an artificial
    neuron works to better understand and appreciate the complexity we can generate
    by grouping many neurons together on many different layers to create deep neural
    networks. Neural nets are an attempt to mimic the functioning of a human brain
    and its ability to abstract new rules through simple observations. Though we are
    still quite far from understanding how human brains organize and process information,
    we already have a good understanding of how single human neurons work. Artificial
    neural networks attempt to mimic the same functionality, trading chemical and
    electrical messaging for numerical values and functions. Much progress has been
    made in the last decade, after neural networks had become popular and then been
    forgotten at least twice before: such resurgence is due in part to having computers
    that are getting faster, the use of **GPUs** (**Graphical Processing Units**)
    versus the most traditional use of **CPUs** (**Computing Processing Units**),
    better algorithms and neural nets design, and increasingly large datasets, as
    we will see in this book.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够快速推断出模式并推断出规则，而不需要对数据进行清理和准备。如果机器也能学会做同样的事情，那就太好了。正如我们所讨论的，弗兰克·罗森布拉特于50多年前的1957年发明了感知器。感知器对于现代深度神经网络而言就像单细胞生物对于复杂多细胞生物一样重要，但了解和熟悉人工神经元的工作原理对于更好地理解和欣赏通过在许多不同层上组合许多神经元来生成的复杂性至关重要。神经网络试图模仿人脑的功能以及通过简单观察抽象出新规则的能力。尽管我们对人类大脑如何组织和处理信息还知之甚少，但我们已经对单个人类神经元的工作原理有了很好的理解。人工神经网络试图模仿相同的功能，将化学和电信号传递交换成数值和函数。在过去的十年中取得了很大进展，神经网络变得流行之后至少两次被遗忘：这种复苏部分原因是计算机的运行速度越来越快，使用**GPU**（**图形处理单元**）而不是最传统的**CPU**（**计算处理单元**），更好的算法和神经网络设计，以及越来越大的数据集，正如我们将在本书中看到的。
- en: In this chapter, we will formally introduce what neural networks are, we will
    thoroughly describe how a neuron works, and we will see how we can stack many
    layers to create and use deep feed-forward neural networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将正式介绍神经网络是什么，我们将彻底描述神经元是如何工作的，并且我们将看到如何堆叠许多层来创建和使用深度前馈神经网络。
- en: Why neural networks?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用神经网络？
- en: Neural networks have been around for many years, and they have gone through
    several periods during which they have fallen in and out of favor. However, in
    recent years, they have steadily gained ground over many other competing machine
    learning algorithms. The reason for this is that advanced neural net architecture
    has shown accuracy in many tasks that has far surpassed that of other algorithms.
    For example, in the field of image recognition, accuracy may be measured against
    a database of 16 million images named ImageNet.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络存在很多年了，并经历了几个时期的兴衰。然而，近年来，它们一直在与许多其他竞争的机器学习算法相比稳步地取得进展。这是因为先进的神经网络架构在许多任务上显示出的准确性远远超过其他算法。例如，在图像识别领域，准确性可能是针对一个名为ImageNet的包含1600万图像的数据库进行衡量的。
- en: Prior to the introduction of deep neural nets, accuracy had been improving at
    a slow rate, but after the introduction of deep neural networks, accuracy dropped
    from an error rate of 40% in 2010 to less than 7% in 2014, and this value is still
    falling. The human recognition rate is still lower, but only at about 5%. Given
    the success of deep neural networks, all entrants to the ImageNet competition
    in 2013 used some form of deep neural network. In addition, deep neural nets "learn"
    a representation of the data, that is, not only learn to recognize an object,
    but also learn what the important features that uniquely define the identified
    object are. By learning to automatically identify features, deep neural nets can
    be successfully used in unsupervised learning, by naturally classifying objects
    with similar features together, without the need for laborious human labeling.
    Similar advances have also been reached in other fields, such as signal processing.
    Deep learning and using deep neural networks is now ubiquitously used, for example,
    in Apple's Siri. When Google introduced a deep learning algorithm for its Android
    operating system, it achieved a 25% reduction in word recognition error. Another
    dataset used for image recognition is the MNIST dataset that comprises examples
    of digits written in different handwriting. The use of deep neural networks for
    digit recognition can now achieve an accuracy of 99.79%, comparable to a human's
    accuracy. In addition, deep neural network algorithms are the closest artificial
    example of how the human brain works. Despite the fact that they are still probably
    a much more simplified and elementary version of our brain, they contain more
    than any other algorithm, the seed of human intelligence, and the rest of this
    book will be dedicated to studying different neural networks and several examples
    of different applications of neural networks will be provided.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入深度神经网络之前，准确性一直以较慢的速度改善，但在引入深度神经网络之后，准确性从2010年的40%的错误率下降到2014年的不到7%，而且这个数值仍在下降。人类的识别率仍然较低，约为5%。考虑到深度神经网络的成功，2013年ImageNet竞赛的所有参赛者都使用了某种形式的深度神经网络。此外，深度神经网络会“学习”数据的表示，即不仅学习识别对象，还学习识别被识别对象所具有的重要特征。通过学习自动识别特征，深度神经网络可以成功地用于无监督学习，通过自然地将具有相似特征的对象分类在一起，而无需费力地人工标记。在其他领域，例如信号处理，也取得了类似的进展。现在，深度学习和使用深度神经网络是普遍存在的，例如在苹果的Siri中使用。当谷歌为其Android操作系统引入深度学习算法时，错误识别率降低了25%。用于图像识别的另一个数据集是MNIST数据集，其中包含用不同手写方式书写的数字的示例。现在，使用深度神经网络进行数字识别的准确率可以达到99.79%，与人类的准确率相当。此外，深度神经网络算法是人工智能中最接近人脑工作方式的例子。尽管它们可能仍然是我们大脑的一个更简化、基本版本，但它们比任何其他算法更多地包含了人类智能的种子，本书的其余内容将致力于研究不同的神经网络以及提供几个不同应用神经网络的示例。
- en: Fundamentals
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础知识
- en: 'In the first chapter, we talked about three different approaches to machine
    learning: supervised learning, unsupervised learning, and reinforcement learning.
    Classical neural networks are a type of supervised machine learning, though we
    will see later that deep learning popularity is instead due to the fact that modern
    deep neural networks can be used in unsupervised learning tasks as well. In the
    next chapter, we will highlight the main differences between classical shallow
    neural networks and deep neural nets. For now, however, we will mainly concentrate
    on classical feed-forward networks that work in a supervised way. Our first question
    is, what exactly is a neural network? Probably the best way to interpret a neural
    network is to describe it as a mathematical model for information processing.
    While this may seem rather vague, it will become much clearer in the next chapters.
    A neural net is not a fixed program, but rather a model, a system that processes
    information, or inputs, in a somewhat bland analogy to how information is thought
    to be processed by biological entities. We can identify three main characteristics
    for a neural net:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了机器学习的三种不同方法：监督学习、无监督学习和强化学习。经典神经网络是一种监督式机器学习，尽管我们将在后面看到，现代深度神经网络的普及更多地归功于这样一个事实，即现代深度神经网络也可以用于无监督学习任务。在接下来的章节中，我们将重点介绍传统浅层神经网络和深度神经网络之间的主要区别。然而，现在我们将主要集中在以监督方式工作的经典前馈网络上。我们的第一个问题是，神经网络究竟是什么？也许解释神经网络的最佳方式是将其描述为信息处理的数学模型。虽然这可能听起来相当模糊，但在接下来的章节中，它将变得更加清晰。神经网络不是一个固定的程序，而是一个模型，一个处理信息或输入的系统，在某种程度上类似于生物实体被认为处理信息的方式。我们可以确定神经网络的三个主要特征：
- en: '**The neural net architecture**: This describes the set of connections (feed-forward,
    recurrent, multi- or single-layered, and so on) between the neurons, the number
    of layers, and the number of neurons in each layer.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络架构**：这描述了神经元之间的连接集合（前馈、循环、多层或单层等），层数以及每层中的神经元数量。'
- en: '**The learning**: This describes what is commonly defined as the training.
    Whether we use back-propagation or some kind of energy level training, it identifies
    how we determine the weights between neurons.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习**：这描述了我们通常定义为训练的内容。无论我们使用反向传播还是某种能量级训练，它都确定了我们如何确定神经元之间的权重。'
- en: '**The activity function**: This describes the function we use on the activation
    value that is passed onto each neuron, the neuron''s internal state, and it describes
    how the neuron works (stochastically, linearly, and so on) and under what conditions
    it will activate or fire, and the output it will pass on to neighboring neurons.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动函数**：这描述了我们在传递给每个神经元的激活值上使用的函数，神经元的内部状态，以及它描述了神经元的工作方式（随机、线性等）以及在什么条件下它将激活或触发，以及它将传递给相邻神经元的输出。'
- en: It should be noted, however, that some researchers would consider the activity
    function as part of the architecture; it may be easier, however, for a beginner
    to separate these two aspects for now. It needs to be remarked that artificial
    neural nets represent only an approximation of how a biological brain works. A
    biological neural net is a much more complex model; however, this should not be
    a concern. Artificial neural nets can still perform many useful tasks, in fact,
    as we will show later, an artificial neural net can indeed approximate to any
    degree we wish any function of the input onto the output.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，需要指出的是，一些研究人员会将活动函数视为架构的一部分；然而，对于初学者来说，现在将这两个方面分开可能更容易。需要注意的是，人工神经网络仅代表了生物大脑运作的近似方式。生物神经网络是一个更复杂的模型；然而，这不应该成为一个问题。人工神经网络仍然可以执行许多有用的任务，事实上，正如我们将在后面展示的那样，人工神经网络确实可以以我们希望的任何程度近似于输入到输出的任何函数。
- en: 'The development of neural nets is based on the following assumptions:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的发展基于以下假设：
- en: Information processing occurs, in its simplest form, over simple elements, called
    neurons
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息处理以其最简单的形式发生在称为神经元的简单元素上。
- en: Neurons are connected and exchange signals between them along connection links
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元之间相连并沿着连接链路交换信号。
- en: Connection links between neurons can be stronger or weaker, and this determines
    how information is processed
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元之间的连接可以更强或更弱，这决定了信息如何被处理。
- en: Each neuron has an internal state that is determined by all the incoming connections
    from other neurons
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元都有一个内部状态，该状态由来自其他神经元的所有传入连接确定。
- en: Each neuron has a different activity function that is calculated on the neuron
    internal state and determines its output signal
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元都有一个不同的活动函数，该函数是根据神经元的内部状态计算的，并确定其输出信号。
- en: In the next section, we shall define in detail how a neuron works and how it
    interacts with other neurons.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将详细定义神经元的工作原理以及它与其他神经元的交互方式。
- en: Neurons and layers
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元和层
- en: What is a neuron? A neuron is a processing unit that takes an input value and,
    according to predefined rules, outputs a different value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是神经元？神经元是一个处理单元，接收一个输入值，并根据预定义的规则输出一个不同的值。
- en: '![Neurons and layers](img/00027.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00027.jpeg)'
- en: In 1943, Warren McCullock and Walter Pitts published an article (W. S. McCulloch
    and W. Pitts. A Logical Calculus of the Ideas Immanent in Nervous Activity, The
    Bulletin of Mathematical Biophysics, 5(4):115–133, 1943) in which they described
    the functioning of a single biological neuron. The components of a biological
    neuron are the dendrites, the soma (the cell body), the axons, and the synaptic
    gaps. Under different names, these are also parts of an artificial neuron.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 1943年，Warren McCullock 和 Walter Pitts 发表了一篇文章（W. S. McCulloch 和 W. Pitts. A
    Logical Calculus of the Ideas Immanent in Nervous Activity, The Bulletin of Mathematical
    Biophysics, 5(4):115–133, 1943），其中描述了单个生物神经元的功能。生物神经元的组成部分包括树突、细胞体（细胞体）、轴突和突触间隙。在不同的名称下，这些也是人工神经元的组成部分。
- en: 'The dendrites bring the input from other neurons to the soma, the neuron''s
    body. The soma is where the inputs are processed and summed together. If the input
    is over a certain threshold, the neuron will "fire" and transmit a single output
    that is electrically sent through the axons. Between the axons of the transmitting
    neurons and the dendrites of the receiving neurons lies the synaptic gap that
    mediates chemically such impulses, altering their frequencies. In an artificial
    neural net, we model the frequency through a numerical weight: the higher the
    frequency, the higher the impulse and, therefore, the higher the weight. We can
    then establish an equivalence table between biological and artificial neurons
    (this is a very simplified description, but it works for our purposes):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 树突将来自其他神经元的输入传递到细胞体，即神经元的主体。细胞体是输入被处理并汇总在一起的地方。如果输入超过一定的阈值，神经元将会“发射”，并传递一个单一的输出，该输出通过轴突以电信方式发送。在传输神经元的轴突和接收神经元的树突之间存在着介导化学脉冲的突触间隙，从而改变其频率。在人工神经网络中，我们通过一个数值权重来模拟频率：频率越高，脉冲越强，因此权重越高。然后，我们可以建立生物和人工神经元之间的等价表格（这是一个非常简化的描述，但适用于我们的目的）：
- en: '![Neurons and layers](img/00028.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00028.jpeg)'
- en: Schematic correspondence between a biological and an artificial neuron
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元与人工神经元的示意对应关系
- en: 'We can therefore describe an artificial neuron schematically as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以如下示意地描述一个人工神经元：
- en: '![Neurons and layers](img/00029.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00029.jpeg)'
- en: At the center of this picture we have the neuron, or the soma, which gets an
    input (the activation) and sets the neuron's internal state that triggers an output
    (the activity function). The input comes from other neurons and it is mediated
    in intensity by the weights (the synaptic gap).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片的中心是神经元，或称为细胞体，它接收输入（激活）并设置神经元的内部状态，从而触发输出（活动函数）。输入来自其他神经元，并且通过权重（突触间隙）的强度来调节。
- en: The simple activation value for a neuron is given by ![Neurons and layers](img/00030.jpeg),
    where *x*[i] is the value of each input neuron, and *w*[i] is the value of the
    connection between the neuron *i* and the output. In the first chapter, in our
    introduction to neural networks, we introduced the bias. If we include the bias
    and want to make its presence explicit, we can rewrite the preceding equation
    as ![Neurons and layers](img/00031.jpeg). The bias effect is to translate the
    hyperplane defined by the weights so it will not necessarily go through the origin
    (and hence its name). We should interpret the activation value as the neuron's
    internal state value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的简单激活值由![神经元和层](img/00030.jpeg)给出，其中 *x*[i] 是每个输入神经元的值，*w*[i] 是神经元 *i* 与输出之间连接的值。在第一章中，我们在对神经网络的介绍中引入了偏差。如果我们包含偏差并希望使其存在明确化，我们可以将上述方程重写为![神经元和层](img/00031.jpeg)。偏差的效果是将由权重定义的超平面进行平移，使其不一定通过原点（因此得名）。我们应该将激活值解释为神经元的内部状态值。
- en: As we mentioned in the previous chapter, the activation value defined previously
    can be interpreted as the dot product between the vector *w* and the vector *x*.
    A vector *x* will be perpendicular to the weight vector *w* if *<w,x> = 0*, therefore
    all vectors *x* such that *<w,x> = 0* define a hyper-plane in **R**^n (where n
    is the dimension of *x*).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，先前定义的激活值可以解释为向量*w*和向量*x*的点积。 如果*<w,x> = 0*，则向量*x*将与权重向量*w*垂直，因此所有满足*<w,x>
    = 0*的向量*x*在**R**^n中定义一个超平面（其中n是*x*的维数）。
- en: Hence, any vector *x* satisfying *<w,x> > 0* is a vector on the side of the
    hyper-plane defined by *w*. A neuron is therefore a linear classifier, which,
    according to this rule, activates when the input is above a certain threshold
    or, geometrically, when the input is on one side of the hyper-plane defined by
    the vector of the weights.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何满足*<w,x> > 0*的向量*x*都是在由*w*定义的超平面的一侧的向量。 因此，神经元是线性分类器，根据此规则，在输入高于一定阈值时激活，或者在几何上，在由权重向量定义的超平面的一侧时激活输入。
- en: '![Neurons and layers](img/00032.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00032.jpeg)'
- en: A single neuron is a linear classifier
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元是线性分类器
- en: A neural network can have an indefinite number of neurons, but regardless of
    their number, in classical networks all the neurons will be ordered in layers.
    The input layer represents the dataset, the initial conditions. For example, if
    the input is a grey-scale image, the input layer is represented for each pixel
    by an input neuron with the inner value the intensity of the pixel. It should
    be noted, however, that the neurons in the input layer are not neurons as the
    others, as their output is constant and is equal to the value of their internal
    state, and therefore the input layer is not generally counted. A 1-layer neural
    net is therefore a simple neural net with just one layer, the output, besides
    the input layer. From each input neuron we draw a line connecting it with each
    output neuron and this value is mediated by the artificial synaptic gap, that
    is the weight *w*[i,j] connecting the input neuron *x*i to the output neuron *y*[j].
    Typically, each output neuron represents a class, for example, in the case of
    the MNIST dataset, each neuron represents a digit. The 1-layer neural net can
    therefore be used to make a prediction such as which digit the input image is
    representing. In fact, the set of output values can be regarded as a measure of
    the probability that the image represents the given class, and therefore the output
    neuron with the highest value will represent the prediction of the neural net.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以具有无限数量的神经元，但无论其数量如何，在传统网络中，所有神经元都将按层次排序。 输入层代表数据集，即初始条件。 例如，如果输入是灰度图像，则输入层由每个像素的输入神经元表示，其内部值为像素的强度。
    但是，应该注意，输入层中的神经元不像其他神经元那样，因为它们的输出是恒定的，等于它们的内部状态的值，因此通常不计算输入层。 因此，1层神经网络实际上是一个仅有一个层次的简单神经网络，即输出层之外的输入层。
    我们从每个输入神经元绘制一条线连接到每个输出神经元，并且这个值由人工突触间隙中介，即连接输入神经元*x*i到输出神经元*y*[j]的权重*w*[i,j]。
    通常，每个输出神经元代表一个类别，例如，在MNIST数据集的情况下，每个神经元代表一个数字。 因此，可以使用1层神经网络进行预测，例如，输入图像表示的是哪个数字。
    实际上，输出值集合可以视为图像表示给定类别的概率的度量，因此具有最高值的输出神经元将代表神经网络的预测。
- en: 'It must be noted that neurons in the same layer are never connected to one
    another, as in the following figure; instead they are all connected to each of
    the neurons in the next layer, and so on:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 必须注意，同一层中的神经元永远不会彼此连接，如下图所示； 相反，它们都连接到下一层的每个神经元，依此类推：
- en: '![Neurons and layers](img/00033.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00033.jpeg)'
- en: 'An example of a 1-layer neural network: the neurons on the left represent the
    input with bias b, the middle column represents the weights for each connection,
    while the neurons on the right represent the output given the weights *w*.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 1层神经网络的示例：左侧的神经元代表具有偏差b的输入，中间列代表每个连接的权重，而右侧的神经元代表给定权重*w*的输出。
- en: This is one of the necessary and defining conditions for classical neural networks,
    the absence of intra-layers connections, while neurons connect to each and every
    neuron in adjacent layers. In the preceding figure, we explicitly show the weights
    for each connection between neurons, but usually the edges connecting neurons
    implicitly represent the weights. The **1** represents the bias unit, the value
    1 neuron with connecting weight equal to the bias that we have introduced earlier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是经典神经网络的必要和定义条件之一，即不存在层内连接，而神经元连接到相邻层中的每个神经元。在前面的图中，我们明确显示了神经元之间每个连接的权重，但通常连接神经元的边隐含地代表权重。**1**代表偏置单元，值为1的神经元与之前引入的偏置相等的连接权重。
- en: As mentioned many times, 1-layer neural nets can only classify linearly separable
    classes; however, there is nothing that can prevent us from introducing more layers
    between the input and the output. These extra layers are called hidden layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如多次提到的，1层神经网络只能对线性可分类进行分类；但是，没有任何东西可以阻止我们在输入和输出之间引入更多层。这些额外的层称为隐藏层。
- en: '![Neurons and layers](img/00034.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![神经元和层](img/00034.jpeg)'
- en: Shown is a 3-layer neural network with two hidden layers. The input layer has
    k input neurons, the first hidden layer has n hidden neurons, and the second hidden
    layer has m hidden neurons. In principle it is possible to have as many hidden
    layers as desired. The output, in this example, is the two classes, *y*[1] and
    *y*[2]. On top the on always-on bias neuron. Each connection has its own weight
    w (not depicted for simplicity).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的是一个具有两个隐藏层的3层神经网络。输入层有k个输入神经元，第一个隐藏层有n个隐藏神经元，第二个隐藏层有m个隐藏神经元。原则上，可以有任意多个隐藏层。在本例中，输出是两个类别，*y*[1]和*y*[2]。顶部的始终开启的偏置神经元。每个连接都有自己的权重w（为简单起见未显示）。
- en: Different types of activation function
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同类型的激活函数
- en: Biologically, neuroscience has identified hundreds, perhaps more than a thousand
    different types of neurons (refer *The Future of the Brain*", by Gary Marcus and
    Jeremy Freeman) and therefore we should be able to model at least some different
    types of artificial neurons. This can be done by using different types of activity
    functions, that is, the function defined on the internal state of the neuron represented
    by the activation ![Different types of activation function](img/00030.jpeg) calculated
    on the input from all the input neurons.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从生物学角度来看，神经科学已经确定了数百，也许是上千种不同类型的神经元（参见*大脑的未来*，作者Gary Marcus和Jeremy Freeman），因此我们应该能够模拟至少一些不同类型的人工神经元。这可以通过使用不同类型的活动函数来完成，即定义在神经元内部状态上的函数，表示为从所有输入神经元计算的激活值![不同类型的激活函数](img/00030.jpeg)。
- en: 'The activity function is a function defined on *a(x)* and it defines the output
    of the neuron. The most common activity functions used are:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 活动函数是定义在*a(x)*上的函数，它定义了神经元的输出。最常用的活动函数包括：
- en: '![Different types of activation function](img/00035.jpeg): This function lets
    the activation value go through and it is called the identity function'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![不同类型的激活函数](img/00035.jpeg)：此函数允许激活值通过，并称为身份函数'
- en: '![Different types of activation function](img/00036.jpeg) : This function activates
    the neuron if the activation is above a certain value and it is called the threshold
    activity function'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![不同类型的激活函数](img/00036.jpeg)：如果激活值高于某个特定值，则此函数激活神经元，称为阈值活动函数'
- en: '![Different types of activation function](img/00037.jpeg): This function is
    one of the most commonly used as its output, which is bounded between 0 and 1,
    and it can be interpreted stochastically as the probability for the neuron to
    activate, and it is commonly called the logistic function or the logistic sigmoid.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![不同类型的激活函数](img/00037.jpeg)：这个函数是最常用的之一，因为它的输出被限制在0和1之间，可以随机解释为神经元激活的概率，通常称为逻辑函数或逻辑S形函数。'
- en: '![Different types of activation function](img/00038.jpeg): This activity function
    is called the bipolar sigmoid, and it is simply a logistic sigmoid rescaled and
    translated to have a range in (-1, 1).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![不同类型的激活函数](img/00038.jpeg)：这个活动函数被称为双极S形函数，它简单地是一个逻辑S形函数重新缩放和平移，使其范围在(-1,
    1)之间。'
- en: '![Different types of activation function](img/00039.jpeg): This activity function
    is called the hyperbolic tangent.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![不同类型的激活函数](img/00039.jpeg)：这个活动函数被称为双曲正切函数。'
- en: '![Different types of activation function](img/00040.jpeg): This activity function
    is probably the closest to its biological counterpart, it is a mix of the identity
    and the threshold function, and it is called the rectifier, or **ReLU**, as in
    **Rectfied Linear Unit**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![Different types of activation function](img/00040.jpeg)：这个活动函数可能是与其生物学相似度最高的，它是标识和阈值函数的混合体，被称为整流器，或**ReLU**，如
    **Rectfied Linear Unit**。'
- en: 'What are the main differences between these activation functions? Often, different
    activation functions work better for different problems. In general, the identity
    activity function or threshold function, while widely used at the inception of
    neural networks with such implementations such as the *perceptron* or the *Adaline*
    (adaptive linear neuron), has recently lost traction in favor of the logistic
    sigmoid, the hyperbolic tangent, or the ReLU. While the identity function and
    the threshold function are much simpler, and therefore were the preferred functions
    when computers did not have quite as much calculation power, it is often preferable
    to use non-linear functions, such as the sigmoid functions or the ReLU. It should
    also be noted that if we only used the linear activity function there is no point
    in adding extra hidden layers, as the composition of linear functions is still
    just a linear function. The last three activity functions differ in the following
    ways:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些激活函数之间的主要区别是什么？通常，不同的激活函数对不同的问题效果更好。一般来说，标识活动函数或阈值函数，在神经网络的实现初始阶段广泛使用，例如 *感知器*
    或 *Adaline*（自适应线性神经元），但最近逐渐失去了优势，转而使用 logistic sigmoid、双曲正切或 ReLU。虽然标识函数和阈值函数要简单得多，因此在计算机没有太多计算能力时是首选函数，但通常更倾向于使用非线性函数，例如
    sigmoid 函数或 ReLU。还应该注意，如果我们只使用线性活动函数，那么添加额外的隐藏层就没有意义，因为线性函数的组合仍然只是一个线性函数。最后三个活动函数在以下方面有所不同：
- en: Their range is different
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的范围不同。
- en: Their gradient may vanish as we increase x
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着*x*的增加，它们的梯度可能会消失。
- en: The fact that the gradient may vanish as we increase *x* and why it is important
    will be clearer later; for now, let's just mention that the gradient (for example,
    the derivative) of the function is important for the training of the neural network.
    This is similar to how, in the linear regression example we introduced in the
    first chapter, we were trying to minimize the function following it along the
    direction opposite to its derivative.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们增加*x*时，梯度可能消失的事实以及为什么这很重要，稍后会更清楚；现在，我们只需提一下，函数的梯度（例如，导数）对神经网络的训练很重要。这类似于在线性回归示例中我们在第一章介绍的，我们试图最小化函数，沿着与其导数相反方向进行。
- en: The range for the logistic function is (0,1), which is one reason why this is
    the preferred function for stochastic networks, that is, networks with neurons
    that may activate based on a probability function. The hyperbolic function is
    very similar to the logistic function, but its range is (-1, 1). In contrast,
    the ReLU has a range of (0, ![Different types of activation function](img/00041.jpeg)),
    so it can have a very large output.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: logistic 函数的范围是(0,1)，这是这个函数被选择作为随机网络的首选函数的一个原因，即具有可能根据概率函数激活的神经元的网络。双曲函数与 logistic
    函数非常相似，但其范围是(-1, 1)。相比之下，ReLU 的范围是(0, ![Different types of activation function](img/00041.jpeg))，因此它可能具有非常大的输出。
- en: However, more importantly, let's look at the derivative for each of the three
    functions. For a logistic function *f*, the derivative is *f * (1-f)*, while if
    *f* is the hyperbolic tangent, its derivative is *(1+f) * (1-f)*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更重要的是，让我们看一下这三个函数的导数。对于一个 logistic 函数 *f*，其导数是 *f * (1-f)*，而如果 *f* 是双曲正切，其导数是
    *(1+f) * (1-f)*。
- en: If *f* is the ReLU, the derivative is much simpler and it is simply ![Different
    types of activation function](img/00042.jpeg).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*f*是 ReLU，导数就简单得多，它简单地是 ![Different types of activation function](img/00042.jpeg)。
- en: Tip
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Let''s briefly see how we can calculate the derivative of the logistic sigmoid
    function. This can be quickly calculated by simply noticing that the derivative
    with respect to *a* of the ![Different types of activation function](img/00043.jpeg)
    function is given by the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看一下如何计算 logistic sigmoid 函数的导数。通过简单地注意到，相对于*a*的导数 ![Different types of
    activation function](img/00043.jpeg) 函数是以下形式的快速计算：
- en: '![Different types of activation function](img/00044.jpeg)![Different types
    of activation function](img/00045.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Different types of activation function](img/00044.jpeg)![Different types
    of activation function](img/00045.jpeg)'
- en: When we will talk about back-propagation, we will see that one of the problems
    for deep networks is that of the *vanishing gradient* (as mentioned previously),
    and the advantage of the ReLU activity function is that the derivative is constant
    and does not tend to *0* as *a* becomes large.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论反向传播时，我们会发现深层网络的一个问题是*梯度消失*（如之前提到的），而ReLU激活函数的优势在于导数是恒定的，当*a*变大时不趋于*0*。
- en: Typically all neurons in the same layer have the same activity function, but
    different layers may have different activity functions. But why are neural networks
    more than 1-layer deep (2-layer or more) so important? As we have seen, the importance
    of neural networks lies in their predictive power, that is, in their ability to
    approximate a function defined on the input with the required output. There exists
    a theorem, called the Universal Approximation Theorem, which states that any continuous
    functions on compact subsets of *R*[n] can be approximated by a neural network
    with at least one hidden layer. While the formal proof of such a theorem is too
    complex to be explained here, we will attempt to give an intuitive explanation
    only using some basic mathematics, and for this we will make use of the logistic
    sigmoid as our activity function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，同一层中的所有神经元具有相同的激活函数，但不同的层可能具有不同的激活函数。但为什么神经网络多于1层深（2层或更多）如此重要？正如我们所见，神经网络的重要性在于它们的预测能力，即能够逼近以输入为定义的函数与所需输出。存在一个定理，称为通用逼近定理，它指出在*R*[n]的紧致子集上的任何连续函数都可以由至少有一个隐藏层的神经网络逼近。虽然该定理的正式证明过于复杂无法在此进行解释，但我们将尝试仅使用一些基本数学给出直观的解释，并且对此我们会使用逻辑sigmoid作为我们的激活函数。
- en: 'The logistic sigmoid is defined as ![Different types of activation function](img/00046.jpeg)
    where ![Different types of activation function](img/00031.jpeg). Let''s now assume
    that we have only one neuron *x=x*[i]:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑sigmoid被定义为 ![不同类型的激活函数](img/00046.jpeg) 其中 ![不同类型的激活函数](img/00031.jpeg)。现在假设我们只有一个神经元
    *x=x*[i]：
- en: '![Different types of activation function](img/00047.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的激活函数](img/00047.jpeg)'
- en: On the left is a standard sigmoid with weight 1 and bias 0\. In the center is
    a sigmoid with weight 10, while on the right is a sigmoid with weight 10 and bias
    50.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是一个标准sigmoid，权重为1，偏差为0。中间是一个权重为10的sigmoid，右侧是一个权重为10且偏差为50的sigmoid。
- en: Then it can be easily shown that if *w* is very large, the logistic function
    becomes close to a step function. The larger *w* is, the more it resembles a step
    function at 0 with height 1\. On the other hand, *b* will simply translate the
    function, and the translation will be equal to the negative of the ratio *b/w*.
    Let's call *t = -b/w.*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后很容易证明，如果*w*非常大，逻辑函数会接近一个阶梯函数。*w*越大，它就越像一个高度为1的0阶梯函数。另一方面，*b*只会平移函数，并且平移将等于比值*b/w*的负数。让我们称*t
    = -b/w*。
- en: 'With this in mind, let''s now consider a simple neural net with one input neuron
    and one hidden layer with two neurons and only one output neuron in the output
    layer:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个基础知识，现在让我们考虑一个简单的神经网络，其中有一个输入神经元和一个具有两个神经元的隐藏层，输出层只有一个输出神经元：
- en: '![Different types of activation function](img/00048.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的激活函数](img/00048.jpeg)'
- en: X is mapped on the two hidden neurons with weights and biases such that on the
    top hidden neuron the ratio *–b/w* is *t*[1] while on the bottom hidden neuron
    it is *t*[2]. Both hidden neurons use the logistic sigmoid activation function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: X被映射到两个带有权重和偏差的隐藏神经元上，使得顶部隐藏神经元的比值 *–b/w* 是 *t*[1] 而底部隐藏神经元是 *t*[2]。两个隐藏神经元都使用逻辑sigmoid激活函数。
- en: The input x is mapped to two neurons, one with weight and bias such that the
    ratio is *t*[1] and the other such that the ratio is *t**[2]*. Then the two hidden
    neurons can be mapped to the output neuron with weights *w* and *–w*, respectively.
    If we apply the logistic sigmoid activity function to each hidden neuron, and
    the identity function to the output neuron (with no bias), we will get a step
    function, from *t*[1] to *t*[2], and height *w*, like the one depicted in the
    following figure*.* Since the series of step functions like the one in the figure
    can approximate any continuous function on a compact subset of **R**, this gives
    an intuition of why the Universal Approximation Theorem holds (this is, in simplified
    terms, the content of a mathematical theorem called "The simple function approximation
    theorem").
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输入x映射到两个神经元，一个具有权重和偏差，使得比例为*t*[1]，另一个具有比例为*t**[2]*的权重和偏差。然后这两个隐藏神经元可以映射到输出神经元，其权重分别为*w*和*–w*。如果对每个隐藏神经元应用逻辑S形活动函数，并对输出神经元应用恒等函数（没有偏差），我们将获得一个步函数，从*t*[1]到*t*[2]，高度为*w*，就像下图中所示的一样。由于类似图中的一系列步函数可以近似于**R**的紧致子集上的任何连续函数，这为什么普适逼近定理成立提供了直观（这是一个数学定理称为“简单函数逼近定理”的简化版本）。
- en: With a little more effort, this can be generalized to **R**[n].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多做一点努力，就可以推广到**R**[n]。
- en: '![Different types of activation function](img/00049.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![不同类型的激活函数](img/00049.jpeg)'
- en: 'The code to produce the preceding figure is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 生成上述图像的代码如下：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The back-propagation algorithm
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: We have seen how neural networks can map inputs onto determined outputs, depending
    on fixed weights. Once the *architecture* of the neural network has been defined
    (feed-forward, number of hidden layers, number of neurons per layer), and once
    the activity function for each neuron has been chosen, we will need to set the
    weights that in turn will define the internal states for each neuron in the network.
    We will see how to do that for a 1-layer network and then how to extend it to
    a deep feed-forward network. For a deep neural network the algorithm to set the
    weights is called the back-propagation algorithm, and we will discuss and explain
    this algorithm for most of this section, as it is one of the most important topics
    for multi-layer feed-forward neural networks. First, however, we will quickly
    discuss this for 1-layer neural networks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了神经网络是如何将输入映射到确定的输出，取决于固定的权重。一旦神经网络的*架构*被定义（前馈，隐藏层数量，每层神经元数量），以及一旦为每个神经元选择了活动函数，我们需要设置权重，这将定义网络中每个神经元的内部状态。我们将看到如何为1层网络设置这些权重，然后如何将其扩展到深度前馈网络。对于深度神经网络，用于设置权重的算法称为反向传播算法，我们将在本节中讨论和解释这个算法，因为这是多层前馈神经网络中最重要的主题之一。然而，我们将首先快速讨论一层神经网络的情况。
- en: 'The general concept we need to understand is the following: every neural network
    is an approximation of a function, therefore each neural network will not be equal
    to the desired function, and instead it will differ by some value. This value
    is called the error and the aim is to minimize this error. Since the error is
    a function of the weights in the neural network, we want to minimize the error
    with respect to the weights. The error function is a function of many weights;
    it is therefore a function of many variables. Mathematically, the set of points
    where this function is zero represents therefore a hypersurface and to find a
    minimum on this surface we want to pick a point and then follow a curve in the
    direction of the minimum.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要理解的一般概念是：每个神经网络都是函数的近似，因此每个神经网络都不等于期望的函数，而是会有一些差异。这个差异被称为误差，目标是最小化这个误差。由于误差是神经网络中的权重的函数，我们希望在权重方面最小化误差。误差函数是许多权重的函数；因此它是许多变量的函数。数学上，此函数为零的点集因此代表一个超曲面，为了在这个超曲面上找到最小值，我们需要选择一个点，然后沿着最小值方向跟随一条曲线。
- en: Linear regression
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: We have already introduced linear regression in the first chapter, but since
    we are now dealing with many variables, to simplify things we are going to introduce
    matrix notation. Let *x* be the input; we can think of *x* as a vector. In the
    case of linear regression, we are going to consider a single output neuron *y*;
    the set of weights *w* is therefore a vector of dimension the same as the dimension
    of *x*. The activation value is then defined as the inner product *<x, w>*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在第一章介绍了线性回归，但由于我们现在处理的是许多变量，为了简化事情，我们将引入矩阵表示法。让*x*是输入；我们可以将*x*视为一个向量。在线性回归的情况下，我们将考虑一个单输出神经元*y*；因此，权重*w*的集合是一个与*x*的维度相同的向量。然后，激活值被定义为内积*<x,
    w>*。
- en: Let's say that for each input value *x*, we want to output a target value *t*,
    while for each *x* the neural network will output a value *y*, defined by the
    activity function chosen, in this case the absolute value of the difference (*y-t*)
    represents the difference between the predicted value and the actual value for
    the specific input example *x*. If we have *m* input values *x*[i], each of them
    will have a target value *t*[i]. In this case, we calculate the error using the
    mean squared error ![Linear regression](img/00050.jpeg), where each *y*[i] is
    a function of *w*. The error is therefore a function of *w* and it is usually
    denoted with *J(w)*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设对于每个输入值*x*，我们想要输出一个目标值*t*，而对于每个*x*，神经网络将输出一个值*y*，由选择的激活函数定义，在这种情况下，绝对值差异（*y-t*）表示预测值和特定输入示例*x*的实际值之间的差异。如果我们有*m*个输入值*x*[i]，每个值都将有一个目标值*t*[i]。在这种情况下，我们使用均方误差计算误差![线性回归](img/00050.jpeg)，其中每个*y*[i]是*w*的函数。因此，误差是*w*的函数，并且通常用*J(w)*表示。
- en: 'As mentioned previously, this represents a hyper-surface of dimension equal
    to the dimension of *w* (we are implicitly also considering the bias), and for
    each *w*[j] we need to find a curve that will lead towards the minimum of the
    surface. The direction in which a curve increases in a certain direction is given
    by its derivative with respect to that direction, in this case by the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这表示了与*w*的维度相等的超曲面（我们隐式地也考虑了偏差），对于每个*w*[j]，我们需要找到一个曲线，该曲线将导致表面的最小值。曲线沿特定方向增加的方向由其对该方向的导数给出，在这种情况下由以下公式给出：
- en: '![Linear regression](img/00051.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00051.jpeg)'
- en: And in order to move towards the minimum we need to move in the opposite direction
    set by ![Linear regression](img/00052.jpeg) for each *w*[j].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了朝着最小值移动，我们需要按照![线性回归](img/00052.jpeg)设置的相反方向移动每个*w*[j]。
- en: 'Let''s calculate the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算以下：
- en: '![Linear regression](img/00053.jpeg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00053.jpeg)'
- en: If ![Linear regression](img/00054.jpeg), then ![Linear regression](img/00055.jpeg)
    and therefore
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果![线性回归](img/00054.jpeg)，那么![线性回归](img/00055.jpeg)，因此
- en: '![Linear regression](img/00056.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00056.jpeg)'
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The notation can sometimes be confusing, especially the first time one sees
    it. The input is given by vectors **x**^i, where the superscript indicates the
    i^(th) example. Since **x** and **w** are vectors, the subscript indicates the
    *j*^(th) coordinate of the vector. *y*^i then represents the output of the neural
    network given the input *x*^i, while *t*^i represents the target, that is, the
    desired value corresponding to the input **x**[i].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 符号有时可能令人困惑，特别是第一次看到它时。输入由向量**x**^i给出，其中上标表示第i个示例。由于**x**和**w**是向量，下标表示向量的*j*^th坐标。*y*^i然后表示给定输入**x**^i的神经网络输出，而*t*^i表示目标值，即与输入**x**[i]对应的期望值。
- en: 'In order to move towards the minimum, we need to move each weight in the direction
    of its derivative by a small amount l, called the *learning rate*, typically much
    smaller than 1, (say 0.1 or smaller). We can therefore redefine in the derivative
    and incorporate the "2 in the learning rate, to get the update rule given by the
    following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了朝向最小值移动，我们需要将每个权重按其导数方向移动一小步长l，称为*学习速率*，通常远小于1，（例如0.1或更小）。因此，我们可以重新定义导数并将“2”合并到学习率中，以获得以下给出的更新规则：
- en: '![Linear regression](img/00057.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00057.jpeg)'
- en: 'Or, more generally, we can write the update rule in matrix form as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，更一般地，我们可以将更新规则写成矩阵形式如下：
- en: '![Linear regression](img/00058.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00058.jpeg)'
- en: Here, ![Linear regression](img/00059.jpeg) (also called nabla) represents the
    vector of partial derivatives. This process is what is often called gradient descent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![线性回归](img/00059.jpeg)（也称为 nabla）代表偏导数的向量。这个过程通常被称为梯度下降。
- en: Tip
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '![Linear regression](img/00060.jpeg) is a vector of partial derivatives. Instead
    of writing the update rule for *w* separately for each of its components *wj*,
    we can write the update rule in matrix form where, instead of writing the partial
    derivative for each *j*, we use ![Linear regression](img/00061.jpeg) to indicate
    each partial derivative, for each *j*.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![线性回归](img/00060.jpeg)是偏导数的向量。我们可以将对*w*的更新规则分别写为每个其分量*wj*，也可以用矩阵形式写出更新规则，其中，用![线性回归](img/00061.jpeg)代替为每个*j*写出偏导数。'
- en: One last note; the update can be done after having calculated all the input
    vectors, however, in some cases, the weights can be updated after each example
    or after a defined preset number of examples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点；更新可以在计算所有输入向量后进行，但在某些情况下，权重可以在每个示例之后或在一定预设数量的示例后进行更新。
- en: Logistic regression
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: In logistic regression, the output is not continuous; rather it is defined as
    a set of classes. In this case, the activation function is not going to be the
    identity function like before, rather we are going to use the logistic sigmoid
    function. The logistic sigmoid function, as we have seen before, outputs a real
    value in (0,1) and therefore it can be interpreted as a probability function,
    and that is why it can work so well in a 2-class classification problem. In this
    case, the target can be one of two classes, and the output represents the probability
    that it be one of those two classes (say *t=1*).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，输出不是连续的；相反，它被定义为一组类。在这种情况下，激活函数不会像之前那样是恒等函数，而是我们将使用逻辑 sigmoid 函数。正如我们之前看到的，逻辑
    sigmoid 函数输出(0,1)中的实值，因此它可以被解释为概率函数，这也是为什么在2类分类问题中它可以运行得很好的原因。在这种情况下，目标可以是两个类别中的一个，而输出表示它是其中两个类别之一（比如*t=1*）的概率。
- en: Tip
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Again, the notation can be confusing. *t* is our target, and it can have, in
    this example, two values. These two values are often defined to be class 0 and
    class 1\. These values 0 and 1 are not to be confused with the values of the logistic
    sigmoid function, which is a continuous real-valued function between 0 and 1\.
    The real value of the sigmoid function represents the probability that the output
    be in class 0 or class 1.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，符号可能令人困惑。*t*是我们的目标，它可以在这个例子中有两个值。这两个值通常被定义为类别0和类别1。这些值0和1不应与逻辑 sigmoid 函数的值混淆，后者是介于0和1之间的连续实值函数。sigmoid
    函数的实际值表示输出属于类别0或类别1的概率。
- en: 'If *a* is the neuron activation value as defined previously, let''s denote
    with the s(*a*) the logistic sigmoid function, therefore, for each example x,
    the probability that the output be the class *y*, given the weights *w*, is:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*a*是之前定义的神经元激活值，让我们用s(*a*)表示逻辑 sigmoid 函数，因此，对于每个示例 x，给定权重*w*时输出为类别*y*的概率是：
- en: '![Logistic regression](img/00062.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00062.jpeg)'
- en: 'We can write that equation more succinctly as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更简洁地将方程写为如下形式：
- en: '![Logistic regression](img/00063.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00063.jpeg)'
- en: 'And since for each sample *x*^i the probabilities *P(t*[i]*|x*[i]*, w)* are
    independent, the global probability is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对每个样本*x*^i，概率*P(t*[i]*|x*[i]*, w)*是独立的，全局概率如下：
- en: '![Logistic regression](img/00064.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00064.jpeg)'
- en: 'If we take the natural log of the preceding equation (to turn products into
    sums), we get the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取前述方程的自然对数（将乘积变为和），我们得到如下结果：
- en: '![Logistic regression](img/00065.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00065.jpeg)'
- en: The object is now to maximize this log to obtain the highest probability of
    predicting the correct results. Usually, this is obtained, as in the previous
    case, by using gradient descent to minimize the cost function *J(w)* defined by
    *J(w)= -log(P(y¦* **x** *,w))*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 目标现在是最大化这个对数以获得预测正确结果的最高概率。通常，这是通过使用梯度下降最小化由*J(w)= -log(P(y¦* **x** *,w))*定义的损失函数*J(w)*来实现的，就像前面的情况一样。
- en: 'As before, we calculate the derivative of the cost function with respect to
    the weights *w*[j] to obtain:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，我们计算相对于权重*w*[j]的损失函数的导数，得到：
- en: '![Logistic regression](img/00066.jpeg)![Logistic regression](img/00067.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00066.jpeg)![逻辑回归](img/00067.jpeg)'
- en: Tip
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To understand the last equality, let''s remind the reader of the following
    facts:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解最后一个等式，让我们提醒读者以下事实：
- en: '![Logistic regression](img/00068.jpeg)![Logistic regression](img/00069.jpeg)![Logistic
    regression](img/00070.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00068.jpeg)![逻辑回归](img/00069.jpeg)![逻辑回归](img/00070.jpeg)'
- en: 'Therefore, by the chain rule:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据链式法则：
- en: '![Logistic regression](img/00071.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00071.jpeg)'
- en: 'Similarly:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 同样：
- en: '![Logistic regression](img/00072.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00072.jpeg)'
- en: 'In general, in case of a multi-class output **t**, with **t** a vector (*t*[1],
    *…, t*[n]), we can generalize this equation using ![Logistic regression](img/00073.jpeg)
    = ![Logistic regression](img/00074.jpeg) that brings to the update equation for
    the weights:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在多类输出**t**的情况下，其中**t**是一个向量（*t*[1]，*…， *t*[n]），我们可以使用![逻辑回归](img/00073.jpeg)
    = ![逻辑回归](img/00074.jpeg)来推导出权重的更新方程：
- en: '![Logistic regression](img/00075.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/00075.jpeg)'
- en: This is similar to the update rule we have seen for linear regression.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于我们对线性回归所看到的更新规则。
- en: Back-propagation
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: In the case of 1-layer, weight-adjustment is easy, as we can use linear or logistic
    regression and adjust the weights simultaneously to get a smaller error (minimizing
    the cost function). For multi-layer neural networks we can use a similar argument
    for the weights used to connect the last hidden layer to the output layer, as
    we know what we would like the output layer to be, but we cannot do the same for
    the hidden layers, as, a priori, we do not know what the values for the neurons
    in the hidden layers ought to be. What we do, instead, is calculate the error
    in the last hidden layer and estimate what it would be in the previous layer,
    propagating the error back from the last to the first layer, hence the name back-propagation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在单层情况下，权重调整很容易，因为我们可以使用线性或逻辑回归，并同时调整权重以获得更小的错误（最小化成本函数）。对于多层神经网络，我们可以对连接最后隐藏层与输出层的权重使用类似的论证，因为我们知道输出层的期望值，但我们无法对隐藏层做同样的操作，因为预先我们并不知道隐藏层神经元的值应该是什么。相反，我们计算最后一个隐藏层的误差，并估计前一层的误差，从最后一层向第一层反向传播误差，因此得名反向传播。
- en: 'Back-propagation is one of the most difficult algorithms to understand at first,
    but all is needed is some knowledge of basic differential calculus and the chain
    rule. Let''s introduce some notation first. We denote with *J* the cost (error)*,*
    with *y* the activity function that is defined on the activation value *a* (for
    example, y could be the logistic sigmoid)*,* which is a function of the weights
    *w* and the input *x*. Let''s also define *w*[i,j], the weight between the *i*^(th)
    input value, and the *j*th output. Here we define input and output more generically
    than for a 1-layer network: if *w*[i,j] connects a pair of successive layers in
    a feed-forward network, we denote as "input" the neurons on the first of the two
    successive layers, and "output" the neurons on the second of the two successive
    layers. In order not to make the notation too heavy, and have to denote on which
    layer each neuron is, we assume that the *i*th input *y*[i] is always in the layer
    preceding the layer containing the *j*^(th) output *y*[j].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是最难理解的算法之一，但所需的只是对基本微分和链式法则的一些知识。首先让我们引入一些符号。我们用*J*表示成本（误差）*，*用*y*表示被定义为在激活值*a*上的活动函数（例如，y可以是逻辑Sigmoid），它是权重*w*和输入*x*的函数。让我们也定义*w*[i,j]，*i*^(th)输入值和*j*th输出之间的权重。在这里，我们比对1层网络更泛化地定义输入和输出：如果*w*[i,j]连接一个前馈网络中的连续两层，我们将"输入"称为第一层包含的神经元，"输出"称为第二层包含的神经元。为了不使符号过于繁重，并且不必指出每个神经元在哪一层，我们假设第*i*个输入*y*[i]始终在包含第*j*^(th)输出
    *y*[j]的层之前的那一层。
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that the letter *y* is used to both denote an input and the output of the
    activity function. *y*[j] is the input to the next layer, and *y*[j] is the output
    of the activity function, but it is then also the input to the next layer. Therefore
    we can think of the *y*[j]*'s* as functions of the *y*[j]*'s*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，字母*y*既用于表示输入，也用于表示活动函数的输出。 *y*[j]是下一层的输入，而 *y*[j]是活动函数的输出，但它也是下一层的输入。因此，我们可以将
    *y*[j]*'s* 视为 *y*[j]*'s* 的函数。
- en: We also use subscripts *i* and *j*, where we always have the element with subscript
    *i* belonging to the layer preceding the layer containing the element with subscript
    *j*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用下标*i*和*j*，其中我们始终将带有下标*i*的元素归属于包含下标*j*的元素的层之前的那一层。
- en: '![Back-propagation](img/00076.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00076.jpeg)'
- en: Figure 10
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图10
- en: In this example, layer 1 represents the input, and *layer 2* the output, so
    *w*[i,j] is a number connecting the *y*[j]value in a layer, and the *y*[j] value
    in the following layer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第 1 层表示输入，*第 2 层*表示输出，所以 *w*[i,j] 是连接一层中的 *y*[j] 值和下一层中的 *y*[j] 值的数值。
- en: 'Using this notation, and the chain-rule for derivatives, for the last layer
    of our neural network we can write the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个符号表示法和导数的链式法则，我们可以为我们神经网络的最后一层写出以下结果：
- en: '![Back-propagation](img/00077.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00077.jpeg)'
- en: 'Since we know that ![Back-propagation](img/00078.jpeg), we have the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道 ![反向传播](img/00078.jpeg)，我们有以下结果：
- en: '![Back-propagation](img/00079.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00079.jpeg)'
- en: If *y* is the logistic sigmoid defined previously, we get the same result we
    have already calculated at the end of the previous section, since we know the
    cost function and we can calculate all derivatives.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *y* 是之前定义的逻辑 S 型函数，我们将得到与前一节末尾已经计算过的相同结果，因为我们知道成本函数，我们可以计算所有的导数。
- en: 'For the previous layers the same formula holds:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的层，相同的公式成立：
- en: '![Back-propagation](img/00077.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00077.jpeg)'
- en: In fact, *a* *j* is the activity function, which, as we know, is a function
    of the weights. The *y*[j] value, which is the activity function of the neuron
    in the "second" layer, is a function of its activation value, and, of course,
    the cost function is a function of the activity function we have chosen.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，*a* *j* 是活动函数，我们知道，这是权重的一个函数。*y*[j] 值，是“第二”层神经元的活动函数，是其激活值的函数，当然，成本函数是我们选择的活动函数的函数。
- en: Tip
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Even though we have several layers, we always concentrate on pairs of successive
    layers and so, perhaps abusing the notation somewhat, we always have a "first"
    layer and a "second" layer, as in *Figure 10*, which is the "input" layer and
    the "output" layer.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们有几个层，我们总是集中在连续层对中，因此，或许有点滥用符号，我们总是有一个“第一”层和一个“第二”层，就像*图 10* 中的那样，它是“输入”层和“输出”层。
- en: Since we know that ![Back-propagation](img/00078.jpeg) and we know that ![Back-propagation](img/00080.jpeg)
    is the derivative of the activity function that we can calculate, all we need
    to calculate is the derivative ![Back-propagation](img/00081.jpeg). Let's notice
    that this is the derivative of the error with respect to the activation function
    in the "second" layer, and, if we can calculate this derivative for the last layer,
    and have a formula that allows us to calculate the derivative for one layer assuming
    we can calculate the derivative for the next, we can calculate all the derivatives
    starting from the last layer and move backwards.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道 ![反向传播](img/00078.jpeg)，并且我们知道 ![反向传播](img/00080.jpeg) 是我们可以计算的活动函数的导数，我们只需要计算导数
    ![反向传播](img/00081.jpeg)。让我们注意到，这是相对于“第二”层的激活函数的误差的导数，如果我们可以计算出最后一层的这个导数，并且有一个允许我们计算下一层的导数的公式，我们可以从最后一层开始计算所有导数并向后移动。
- en: 'Let us notice that, as we defined by the *y[j]*, they are the activation values
    for the neurons in the "second" layer, but they are also the activity functions,
    therefore functions of the activation values in the first layer. Therefore, applying
    the chain rule, we have the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注意到，正如我们通过 *y[j]* 定义的那样，它们是“第二”层神经元的激活值，但它们也是活动函数，因此是第一层激活值的函数。因此，应用链式法则，我们有以下结果：
- en: '![Back-propagation](img/00082.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00082.jpeg)'
- en: And once again we can calculate both ![Back-propagation](img/00080.jpeg) and
    ![Back-propagation](img/00083.jpeg), so once we know ![Back-propagation](img/00081.jpeg)
    we can calculate ![Back-propagation](img/00084.jpeg), and since we can calculate
    ![Back-propagation](img/00081.jpeg) for the last layer, we can move backward and
    calculate ![Back-propagation](img/00084.jpeg) for any layer and therefore ![Back-propagation](img/00085.jpeg)
    for any layer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以计算 ![反向传播](img/00080.jpeg) 和 ![反向传播](img/00083.jpeg)，因此一旦我们知道 ![反向传播](img/00081.jpeg)，我们可以计算
    ![反向传播](img/00084.jpeg)，由于我们可以计算出最后一层的 ![反向传播](img/00081.jpeg)，我们可以向后移动并计算任何一层的
    ![反向传播](img/00084.jpeg)，因此可以计算出任何一层的 ![反向传播](img/00085.jpeg)。
- en: To summarize, if we have a sequence of layers where
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，如果我们有一系列层，其中
- en: '![Back-propagation](img/00086.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00086.jpeg)'
- en: We then have these two fundamental equations, where the summation in the second
    equation should read as the sum over all the outgoing connections from *y* *j*
    to any neuron ![Back-propagation](img/00087.jpeg) in the successive layer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有了这两个基本方程，第二个方程中的求和应该是对从*y* *j*到任何神经元![反向传播](img/00087.jpeg)的传出连接的总和。
- en: '![Back-propagation](img/00077.jpeg)![Back-propagation](img/00088.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00077.jpeg)![反向传播](img/00088.jpeg)'
- en: By using these two equations we can calculate the derivatives for the cost with
    respect to each layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这两个方程，我们可以计算对每层成本的导数。
- en: If we set ![Back-propagation](img/00089.jpeg), ![Back-propagation](img/00090.jpeg)
    represents the variation of the cost with respect to the activation value, and
    we can think of ![Back-propagation](img/00090.jpeg) as the error at the *y*[j]
    neuron. We can then rewrite
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设置![反向传播](img/00089.jpeg)，![反向传播](img/00090.jpeg)表示成本对激活值的变化，我们可以将![反向传播](img/00090.jpeg)看作是*y*[j]神经元的误差。我们可以重写
- en: '![Back-propagation](img/00091.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00091.jpeg)'
- en: 'This implies that ![Back-propagation](img/00092.jpeg). These two equations
    give an alternate way of seeing back-propagation, as the variation of the cost
    with respect to the activation value, and provide a formula to calculate this
    variation for any layer once we know the variation for the following layer:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着![反向传播](img/00092.jpeg)。这两个方程给出了看待反向传播的另一种方式，即成本对激活值的变化，并提供了一种计算这种变化的公式，以便我们知道了如何为以下层的任何层计算这种变化：
- en: '![Back-propagation](img/00093.jpeg)![Back-propagation](img/00094.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00093.jpeg)![反向传播](img/00094.jpeg)'
- en: 'We can also combine these equations and show that:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以组合这些方程并证明：
- en: '![Back-propagation](img/00095.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00095.jpeg)'
- en: The back-propagation algorithm for updating the weights is then given on each
    layer by
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 更新权重的反向传播算法然后在每一层上给出
- en: '![Back-propagation](img/00096.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/00096.jpeg)'
- en: In the last section, we will provide a code example that will help understand
    and apply these concepts and formulas.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节，我们将提供一个代码示例，以帮助理解和应用这些概念和公式。
- en: Applications in industry
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工业应用
- en: We mentioned in the previous chapter some examples where machine learning finds
    its applications. Neural networks, in particular, have many similar applications.
    We will review some of the applications for which they were used when they became
    popular in the late 1980's and early 1990's, after back-propagation had been discovered
    and deeper neural networks could be trained.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们提到了一些机器学习应用的例子。神经网络，特别是具有相似应用的神经网络。我们将回顾一些应用程序，它们在1980年代末和1990年代初变得流行之后使用了这些应用程序，后者发现了反向传播，并且可以训练更深的神经网络。
- en: Signal processing
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信号处理
- en: There are many applications of neural networks in the area of signal processing.
    One of the first applications of neural nets was to suppress echo on a telephone
    line, especially on intercontinental calls, as developed starting from 1957 by
    Bernard Widrow and Marcian Hoff. The *Adaline* makes use of the identity function
    as its activity function for training and seeks to minimize the mean squared error
    between the activation and the target value. The Adaline is trained to remove
    the echo from the signal on the telephone line by applying the input signal both
    to the *Adaline* (the filter) and the telephone line. The difference between the
    output from the telephone line and the output from the *Adaline* is the error,
    which is used to train the network and remove the noise (echo) from the signal.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在信号处理领域，神经网络有许多应用。神经网络最早的应用之一是抑制电话线上的回声，特别是在1957年由伯纳德·威德罗和马西安·霍夫开发，特别是在洲际电话中。*Adaline*
    使用恒等函数作为其训练的激活函数，并寻求最小化激活和目标值之间的均方误差。Adaline经过训练，通过将输入信号应用于* Adaline *（滤波器）和电话线来消除电话线上的回声。电话线输出和*
    Adaline *输出之间的差异是误差，用于训练网络并从信号中消除噪声（回声）。
- en: Medical
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 医疗
- en: The instant physician was developed by Anderson in 1986 and the idea behind
    it was to store a large number of medical records containing information about
    symptoms, diagnosis, and treatment for each case. The network is trained to make
    predictions on best diagnosis and treatment on different symptoms.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络是由安德森于1986年开发的，其背后的思想是存储有关每种情况的症状、诊断和治疗信息的大量医疗记录。该网络经过训练，可以对不同症状的最佳诊断和治疗进行预测。
- en: More recently, using deep neural networks, IBM worked on a neural network that
    could make predictions on possible heart failures, reading doctor's notes, similarly
    to an experienced cardiologist.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，IBM利用深度神经网络开发了一个神经网络，可以预测可能的心脏衰竭，阅读医生的笔记，类似于经验丰富的心脏病专家。
- en: Autonomous car driving
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: 'Nguyen and Widrow in 1989, and Miller, Sutton, and Werbos in 1990, developed
    a neural network that could provide steering directions to a large trailer truck
    backing up to a loading dock. The neural net is made up of two modules: the first
    module is able to calculate new positions using a neural net with several layers,
    by learning how the truck responds to different signals. This neural net is called
    the emulator. A second module, called the controller, learns to give the correct
    commands using the emulator to know its position. In recent years, autonomous
    car driving has made huge strides and it is a reality, though much more complex
    deep learning neural networks are used in conjunction with inputs from cameras,
    GPS, lidar, and sonar units.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 1989年，Nguyen和Widrow，以及1990年，Miller、Sutton和Werbos开发了一个可以为大型拖车提供倒车到装货码头的方向指示的神经网络。神经网络由两个模块组成：第一个模块能够使用多层的神经网络计算新的位置，通过学习卡车对不同信号的反应。这个神经网络称为仿真器。第二个模块称为控制器，通过使用仿真器来了解其位置，学习给出正确的指令。近年来，自动驾驶汽车已经取得了巨大进步，并且已经成为现实，尽管更复杂的深度学习神经网络与来自摄像头、GPS、激光雷达和声纳单元的输入一起使用。
- en: Business
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业
- en: In 1988, Collins, Ghosh, and Scofield developed a neural net that could be used
    to assess whether mortgage loans should be approved and given. Using data from
    mortgage evaluators, neural networks were trained to determine whether applicants
    should be given a loan. The input was a number of features, such as the number
    of years the applicant had been employed, income level, number of dependents,
    appraised value of the property, and so on.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 1988年，Collins、Ghosh和Scofield开发了一个神经网络，可以用来评估是否应该批准和发放抵押贷款。利用抵押贷款评估员的数据，神经网络被训练来确定是否应该给予申请人贷款。输入是一些特征，如申请人的就业年限、收入水平、受抚养人数、财产的评估价值等等。
- en: Pattern recognition
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式识别
- en: We have discussed this problem many times. One of the areas where neural networks
    have been applied is the recognition of characters. This, for example, can be
    applied to the recognition of digits, and it can be used for recognizing hand-written
    postal codes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次讨论了这个问题。神经网络已经被应用的一个领域是字符识别。比如，这可以用于数字的识别，也可以用于手写邮政编码的识别。
- en: Speech production
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音产生
- en: In 1986, Sejnowski and Rosenberg produced the widely known example of NETtalk
    that produced spoken words by reading written text. NETtalk's requirement is a
    set of examples of the written words and their pronunciation. The input includes
    both the letter being pronounced and the letters preceding it and following it
    (usually three) and the training is made using the most widely spoken words and
    their phonetic transcription. In its implementation, the net learns first to recognize
    vowels from consonants, then to recognize word beginnings and endings. It typically
    takes many passes before the words pronounced can become intelligible, and its
    progress sometimes resembles children's learning on how to pronounce words.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 1986年，Sejnowski和Rosenberg提出了广为人知的NETtalk示例，通过阅读书面文字来产生口语。NETtalk的要求是一组书面文字及其发音的示例。输入包括要发音的字母以及它前面和后面的字母（通常是三个），训练是使用最常见的单词及其语音转录进行的。在实现中，该网络首先学习识别元音和辅音，然后学习识别单词的开头和结尾。通常需要多次迭代才能使发音变得清晰，其进展有时类似于孩子学习如何发音单词。
- en: Code example of a neural network for the function xor
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个用于 xor 函数的神经网络的代码示例
- en: It is a well-known fact, and something we have already mentioned, that 1-layer
    neural networks cannot predict the function XOR. 1-layer neural nets can only
    classify linearly separable sets, however, as we have seen, the Universal Approximation
    Theorem states that a 2-layer network can approximate any function, given a complex
    enough architecture. We will now create a neural network with two neurons in the
    hidden layer and we will show how this can model the XOR function. However, we
    will write code that will allow the reader to simply modify it to allow for any
    number of layers and neurons in each layer, so that the reader can try simulating
    different scenarios. We are also going to use the hyperbolic tangent as the activity
    function for this network. To train the network, we will implement the back-propagation
    algorithm discussed earlier.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个众所周知的事实，也是我们已经提到过的，即单层神经网络无法预测XOR函数。单层神经网络只能对线性可分集进行分类，然而，正如我们所见，通用逼近定理指出，一个具有足够复杂架构的2层网络可以近似任何函数。我们现在将创建一个隐藏层中具有两个神经元的神经网络，并演示如何模拟XOR函数。但是，我们将编写代码，让读者可以简单地修改它以允许任意数量的层和每层的神经元，以便读者可以尝试模拟不同的情景。我们还将使用双曲正切函数作为此网络的活动函数。为了训练网络，我们将实现之前讨论过的反向传播算法。
- en: 'We will only need to import one library, `numpy`, though if the reader wished
    to visualize the results, we also recommend importing `matplotlib`. The first
    lines of code are therefore:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要导入一个库，`numpy`，尽管如果读者希望可视化结果，我们还建议导入`matplotlib`。因此，代码的前几行是：
- en: '[PRE1]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next we define our activity function and its derivative (we use `tanh(x)` in
    this example):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义我们的活动函数及其导数（在本示例中我们使用`tanh(x)`）：
- en: '[PRE2]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next we define the `NeuralNetwork` class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们定义`NeuralNetwork`类：
- en: '[PRE3]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To follow Python syntax, anything inside the `NeuralNetwork` class will have
    to be indented. We define the "constructor" of the `NeuralNetwork` class, that
    is its variables, which in this case will be the neural network architecture,
    that is, how many layers and how many neurons per layer, and we will also initialize
    at random the weights to be between negative 1 and positive 1\. `net_arch` will
    be a 1-dimensional array containing the number of neurons per each layer: for
    example [2,4,1] means an input layer with two neurons, a hidden layer with four
    neurons, and an output layer with one neuron.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循Python语法，`NeuralNetwork`类中的任何内容都必须缩进。我们定义了`NeuralNetwork`类的“构造函数”，即其变量，这在本例中将是神经网络的架构，即有多少层以及每层有多少个神经元，并且我们还将随机初始化权重为介于负1和正1之间。`net_arch`将是一个包含每层神经元数量的一维数组：例如[2,4,1]表示具有两个神经元的输入层，具有四个神经元的隐藏层和具有一个神经元的输出层。
- en: 'Since we are studying the XOR function, for the input layer we need to have
    two neurons, and for the output layer only one neuron:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在研究XOR函数，因此对于输入层，我们需要有两个神经元，对于输出层，只需要一个神经元：
- en: '[PRE4]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this code, we have defined the activity function to be the hyperbolic tangent
    and we have defined its derivative. We have also defined how many training steps
    there should be per epoch. Finally, we have initialized the weights, making sure
    we also initialize the weights for the biases that we will add later. Next, we
    need to define the `fit` function, the function that will train our network. In
    the last line, `nn` represents the `NeuralNetwork` class and `predict` is the
    function in the `NeuralNetwork` class that we will define later:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们已定义活动函数为双曲正切函数，并定义了其导数。我们还定义了每个时期应有多少个训练步骤。最后，我们初始化了权重，确保我们也初始化了稍后将添加的偏置的权重。接下来，我们需要定义`fit`函数，这个函数将训练我们的网络。在最后一行中，`nn`代表`NeuralNetwork`类，`predict`是我们稍后将定义的`NeuralNetwork`类中的函数：
- en: '[PRE5]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'All we have done here is to add a "1" to the input data (the always-on bias
    neuron) and set up code to print the result at the end of each epoch to keep track
    of our progress. We will now go ahead and set up our feed-forward propagation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的一切只是在输入数据中添加了一个“1”（始终开启的偏置神经元），并设置了代码以在每个时期结束时打印结果，以便跟踪我们的进度。我们现在将继续设置我们的前向传播：
- en: '[PRE6]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are going to update our weights after each step, so we randomly select one
    of the input data points, then we set up feed-forward propagation by setting up
    the activation for each neuron, then applying the `tanh(x)` on the activation
    value. Since we have a bias, we add the bias to our matrix y that keeps track
    of each neuron output value.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个步骤后更新我们的权重，因此我们随机选择其中一个输入数据点，然后设置前向传播，为每个神经元设置激活，然后在激活值上应用`tanh(x)`。由于我们有一个偏差，我们将偏差添加到我们的矩阵y中，该矩阵跟踪每个神经元的输出值。
- en: 'Now we do our back-propagation of the error to adjust the weights:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进行错误的反向传播以调整权重：
- en: '[PRE7]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This concludes our back-propagation algorithm; all that is left to do is to
    write a predict function to check the results:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们的反向传播算法；我们所要做的只是编写一个预测函数来检查结果：
- en: '[PRE8]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this point we just need to write the main function as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们只需要按照下面的主函数进行编写：
- en: '[PRE9]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice the use of `numpy.random.seed(0)`.This is simply to ensure that the weight
    initialization is consistent across runs to be able to compare results, but it
    is not necessary for the implementation of a neural net.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`numpy.random.seed(0)`的用法。这只是为了确保权重初始化在不同运行中的一致性，以便比较结果，但对于神经网络的实现并不是必需的。
- en: 'This ends the code, and the output should be a four-dimensional array, such
    as: (0.003032173692499, 0.9963860761357, 0.9959034563937, 0.0006386449217567)
    showing that the neural network is learning that the output should be (0,1,1,0).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了代码，结果应该是一个四维数组，例如：（0.003032173692499，0.9963860761357，0.9959034563937，0.0006386449217567），表明神经网络学会了输出应该是（0,1,1,0）。
- en: The reader can slightly modify the code we created in the `plot_decision_regions
    function` used earlier in this book and see how different neural networks separate
    different regions depending on the architecture chosen.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以略微修改我们之前在本书中使用的`plot_decision_regions function`中创建的代码，看看不同的神经网络如何根据所选择的架构区分不同的区域。
- en: The output picture will look like the following figures. The circles represent
    the (**True**, **True**) and (**False**, **False**) inputs, while the triangles
    represent the (**True**, **False**) and (**False**, **True**) inputs for the XOR
    function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图片将如下图所示。圆代表（**True**，**True**）和（**False**，**False**）的输入，而三角形代表（**True**，**False**）和（**False**，**True**）的输入对于XOR函数。
- en: '![Code example of a neural network for the function xor](img/00097.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![XOR函数的神经网络代码示例](img/00097.jpeg)'
- en: The same figure, on the left zoomed out, and on the right zoomed in on the selected
    inputs. The neural network learns to separate those points, creating a band containing
    the two **True** output values.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 同一图，左边是缩小的，右边是放大选择的输入。神经网络学会了分离这些点，创建了一个含有两个**True**输出值的带状区域。
- en: Different neural network architectures (for example, implementing a network
    with a different number of neurons in the hidden layer, or with more than just
    one hidden layer) may produce a different separating region. In order to do this,
    the reader can simply change the line in the code `nn = NeuralNetwork([2,2,1]).`
    While the first `2` must be kept (the input does not change), the second `2` can
    be modified to denote a different number of neurons in the hidden layer. Adding
    another integer will add a new hidden layer with as many neurons as indicated
    by the added integer. The last `1` cannot be modified. For example, `([2,4,3,1])`
    will represent a 3-layer neural network, with four neurons in the first hidden
    layer and three neurons in the second hidden layer.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的神经网络架构（例如，实现具有不同隐藏层中神经元数量的网络，或者具有不止一个隐藏层）可能产生不同的分离区域。为了实现这一点，读者只需要改变代码中的一行`nn
    = NeuralNetwork([2,2,1]).`。第一个`2`必须保留（输入不变），但可以修改第二个`2`以表示不同隐藏层中的神经元数量。添加另一个整数将添加一个带有所添加的整数指示的神经元数的新的隐藏层。最后的`1`不能修改。例如，`([2,4,3,1])`将表示一个3层神经网络，第一个隐藏层中有四个神经元，第二隐藏层中有三个神经元。
- en: 'The reader would then see that, while the solution is always the same, the
    curves separating the regions will be quite different depending on the architecture
    chosen. In fact, choosing `nn = NeuralNetwork([2,4,3,1])` will give the following
    figure:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后读者会发现，虽然解决方案是一样的，但是根据所选择的架构，分离区域的曲线会有所不同。实际上，选择`nn = NeuralNetwork([2,4,3,1])`将给出以下图形：
- en: '![Code example of a neural network for the function xor](img/00098.jpeg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![XOR函数的神经网络代码示例](img/00098.jpeg)'
- en: 'While choosing `nn = NeuralNetwork([2,4,1])`, for example, would produce the
    following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，选择`nn = NeuralNetwork([2,4,1])`会产生以下结果：
- en: '![Code example of a neural network for the function xor](img/00099.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络对异或函数的代码示例](img/00099.jpeg)'
- en: The architecture of the neural network defines therefore the way the neural
    net goes about to solve the problem at hand, and different architectures provide
    different approaches (though they may all give the same result) similarly to how
    human thought processes can follow different paths to reach the same conclusion.
    We are now ready to start looking more closely at what deep neural nets are and
    their applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络的架构定义了神经网络解决手头问题的方式，不同的架构提供了不同的方法（尽管它们可能都会产生相同的结果），类似于人类思维过程可以沿着不同的路径达到相同的结论。我们现在准备更仔细地研究深度神经网络及其应用。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced neural networks in detail and we have mentioned
    their success over other competing algorithms. Neural networks are comprised of
    the "units", or neurons, that belong to them or their connections, or weights,
    that characterize the strength of the communication between different neurons
    and their activity functions, that is, how the neurons process the information.
    We have discussed how we can create different architectures, and how a neural
    network can have many layers, and why inner (hidden) layers are important. We
    have explained how the information flows from the input to the output by passing
    from each layer to the next based on the weights and the activity function defined,
    and finally we have shown how we can define a method called back-propagation to
    "tune" the weights to improve the desired level of accuracy. We have also mentioned
    many of the areas where neural networks are and have been employed.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了神经网络，并提到了它们在与其他竞争算法相比的成功。神经网络由它们所属的“单元”或神经元，以及属于它们的连接或权重组成，这些权重表征了不同神经元之间通信的强度以及它们的活动函数，即神经元如何处理信息。我们讨论了如何创建不同的架构，以及神经网络如何可以具有许多层，以及为什么内部（隐藏）层很重要。我们解释了信息如何通过基于权重和定义的活动函数从输入流向输出，最后我们展示了如何定义一种称为反向传播的方法来“调整”权重以提高所需的准确性。我们还提到了许多神经网络被应用的领域。
- en: In the next chapter, we will continue discussing deep neural networks, and in
    particular we will explain the meaning of "deep", as in deep learning, by explaining
    that it not only refers to the number of hidden layers in the network, but more
    importantly to the quality of the learning of the neural network. For this purpose,
    we will show how neural networks learn to recognize features and put them together
    as representations of the objects recognized, which will open the way to use neural
    networks for unsupervised learning. We will also describe a few important deep
    learning libraries, and finally, we will provide a concrete example where we can
    apply neural networks for digit recognition.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论深度神经网络，特别是我们将解释“深度”一词的含义，就像深度学习一样，我们将解释它不仅仅是指网络中隐藏层的数量，更重要的是指神经网络学习的质量。为此，我们将展示神经网络如何学习识别特征并将它们组合成识别对象的表示，这将为使用神经网络进行无监督学习打开大门。我们还将描述几个重要的深度学习库，最后，我们将提供一个具体的例子，说明我们如何应用神经网络进行数字识别。
