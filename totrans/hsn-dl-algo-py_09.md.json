["```py\n def Single_context_CBOW(x, label, W1, W2, loss):\n\n    #forward propagation\n    h = np.dot(W1.T, x)\n    u = np.dot(W2.T, h)\n    y_pred = softmax(u)\n\n    #error\n    e = -label + y_pred\n\n    #backward propagation\n    dW2 = np.outer(h, e)\n    dW1 = np.outer(x, np.dot(W2.T, e))\n\n    #update weights\n    W1 = W1 - lr * dW1\n    W2 = W2 - lr * dW2\n\n    #loss function\n    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))\n\n    return W1, W2, loss\n```", "```py\npip install -U gensim\n```", "```py\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n#data processing\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nstopWords = stopwords.words('english')\n\n#modelling\nfrom gensim.models import Word2Vec\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\n```", "```py\ndata = pd.read_csv('data/text.csv',header=None)\n```", "```py\ndata.head()\n```", "```py\ndef pre_process(text):\n\n    # convert to lowercase\n    text = str(text).lower()\n\n    # remove all special characters and keep only alpha numeric characters and spaces\n    text = re.sub(r'[^A-Za-z0-9\\s.]',r'',text)\n\n    #remove new lines\n    text = re.sub(r'\\n',r' ',text)\n\n    # remove stop words\n    text = \" \".join([word for word in text.split() if word not in stopWords])\n\n    return text\n```", "```py\npre_process(data[0][50])\n```", "```py\n'agree fancy. everything needed. breakfast pool hot tub nice shuttle airport later checkout time. noise issue tough sleep through. awhile forget noisy door nearby noisy guests. complained management later email credit compd us amount requested would return.'\n```", "```py\ndata[0] = data[0].map(lambda x: pre_process(x))\n```", "```py\ndata[0][1].split('.')[:5]\n```", "```py\n['stayed crown plaza april april ',\n ' staff friendly attentive',\n ' elevators tiny ',\n ' food restaurant delicious priced little high side',\n ' course washington dc']\n```", "```py\ncorpus = []\nfor line in data[0][1].split('.'):\n    words = [x for x in line.split()]\n    corpus.append(words)\n```", "```py\ncorpus[:2]\n\n[['stayed', 'crown', 'plaza', 'april', 'april'], ['staff', 'friendly', 'attentive']]\n```", "```py\ndata = data[0].map(lambda x: x.split('.'))\n\ncorpus = []\nfor i in (range(len(data))):\n    for line in data[i]:\n        words = [x for x in line.split()]\n        corpus.append(words)\n\nprint corpus[:2]\n```", "```py\n[['room', 'kind', 'clean', 'strong', 'smell', 'dogs'],\n\n ['generally', 'average', 'ok', 'overnight', 'stay', 'youre', 'fussy']]\n```", "```py\nphrases = Phrases(sentences=corpus,min_count=25,threshold=50)\nbigram = Phraser(phrases)\n\nfor index,sentence in enumerate(corpus):\n    corpus[index] = bigram[sentence]\n```", "```py\ncorpus[111]\n\n[u'connected', u'rivercenter', u'mall', u'downtown', u'san_antonio']\n```", "```py\ncorpus[9]\n\n[u'course', u'washington_dc']\n```", "```py\nsize = 100\nwindow_size = 2\nepochs = 100\nmin_count = 2\nworkers = 4\nsg = 1\n```", "```py\nmodel = Word2Vec(corpus, sg=1,window=window_size,size=size, min_count=min_count,workers=workers,iter=epochs)\n```", "```py\nmodel.save('model/word2vec.model')\n```", "```py\nmodel = Word2Vec.load('model/word2vec.model')\n```", "```py\nmodel.most_similar('san_diego')\n\n[(u'san_antonio', 0.8147615790367126),\n (u'indianapolis', 0.7657858729362488),\n (u'austin', 0.7620342969894409),\n (u'memphis', 0.7541092038154602),\n (u'phoenix', 0.7481759786605835),\n (u'seattle', 0.7471771240234375),\n (u'dallas', 0.7407466769218445),\n (u'san_francisco', 0.7373261451721191),\n (u'la', 0.7354192137718201),\n (u'boston', 0.7213659286499023)]\n```", "```py\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n\n[(u'queen', 0.7255150675773621)]\n```", "```py\ntext = ['los_angeles','indianapolis', 'holiday', 'san_antonio','new_york']\n\nmodel.doesnt_match(text)\n\n'holiday'\n```", "```py\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport tensorflow as tf \nfrom tensorflow.contrib.tensorboard.plugins import projector tf.logging.set_verbosity(tf.logging.ERROR)\n\nimport numpy as np\nimport gensim \nimport os\n```", "```py\nfile_name = \"model/word2vec.model\"\nmodel = gensim.models.keyedvectors.KeyedVectors.load(file_name)\n```", "```py\nmax_size = len(model.wv.vocab)-1\n```", "```py\nw2v = np.zeros((max_size,model.layer1_size))\n```", "```py\nif not os.path.exists('projections'):\n    os.makedirs('projections')\n\nwith open(\"projections/metadata.tsv\", 'w+') as file_metadata:\n\n    for i, word in enumerate(model.wv.index2word[:max_size]):\n\n        #store the embeddings of the word\n        w2v[i] = model.wv[word]\n\n        #write the word to a file \n        file_metadata.write(word + '\\n')\n```", "```py\nsess = tf.InteractiveSession()\n```", "```py\nwith tf.device(\"/cpu:0\"):\n    embedding = tf.Variable(w2v, trainable=False, name='embedding')\n```", "```py\ntf.global_variables_initializer().run()\n```", "```py\nsaver = tf.train.Saver()\n```", "```py\nwriter = tf.summary.FileWriter('projections', sess.graph)\n```", "```py\nconfig = projector.ProjectorConfig()\nembed = config.embeddings.add()\n```", "```py\nembed.tensor_name = 'embedding'\nembed.metadata_path = 'metadata.tsv'\n```", "```py\nprojector.visualize_embeddings(writer, config)\n\nsaver.save(sess, 'projections/model.ckpt', global_step=max_size)\n```", "```py\ntensorboard --logdir=projections --port=8000\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport gensim\nfrom gensim.models.doc2vec import TaggedDocument\n\nfrom nltk import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\nstopWords = set(stopwords.words('english'))\n```", "```py\ndocLabels = []\ndocLabels = [f for f in os.listdir('data/news_dataset') if f.endswith('.txt')]\n\ndata = []\nfor doc in docLabels:\n      data.append(open('data/news_dataset/'+doc).read()) \n```", "```py\ndocLabels[:5]\n\n['Electronics_827.txt',\n 'Electronics_848.txt',\n 'Science829.txt',\n 'Politics_38.txt',\n 'Politics_688.txt']\n```", "```py\nclass DocIterator(object):\n    def __init__(self, doc_list, labels_list):\n        self.labels_list = labels_list\n        self.doc_list = doc_list\n\n    def __iter__(self):\n        for idx, doc in enumerate(self.doc_list):\n            yield TaggedDocument(words=doc.split(), tags=                        [self.labels_list[idx]])\n```", "```py\nit = DocIterator(data, docLabels)\n```", "```py\nsize = 100\nalpha = 0.025\nmin_alpha = 0.025\ndm = 1\nmin_count = 1\n```", "```py\nmodel = gensim.models.Doc2Vec(size=size, min_count=min_count, alpha=alpha, min_alpha=min_alpha, dm=dm)\nmodel.build_vocab(it)\n```", "```py\nfor epoch in range(100):\n    model.train(it,total_examples=120)\n    model.alpha -= 0.002\n    model.min_alpha = model.alpha\n```", "```py\nmodel.save('model/doc2vec.model')\n```", "```py\nd2v_model = gensim.models.doc2vec.Doc2Vec.load('model/doc2vec.model')\n```", "```py\nd2v_model.docvecs.most_similar('Sports_1.txt')\n\n[('Sports_957.txt', 0.719024658203125),\n ('Sports_694.txt', 0.6904895305633545),\n ('Sports_836.txt', 0.6636477708816528),\n ('Sports_869.txt', 0.657712459564209),\n ('Sports_123.txt', 0.6526877880096436),\n ('Sports_4.txt', 0.6499642729759216),\n ('Sports_749.txt', 0.6472041606903076),\n ('Sports_369.txt', 0.6408025026321411),\n ('Sports_167.txt', 0.6392412781715393),\n ('Sports_104.txt', 0.6284008026123047)]\n```"]