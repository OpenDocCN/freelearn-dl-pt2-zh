["```py\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n```", "```py\ndf = spark.read.format(\"csv\" \\\n    .option(\"inferSchema\", True) \\\n    .option(\"header\", True) \\\n    .option(\"sep\", \"\\t\") \\\n    .load(\"/FileStore/tables/HT_Sensor_metadata.dat\")\n```", "```py\npdf = df.toPandas()\n\ny_pred = KMeans(n_clusters=3, \n                random_state=2).fit_predict(pdf[['dt','t0']])\n\nplt.scatter(pdf['t0'],pdf['dt'], c=y_pred)\ndisplay(plt.show())\n```", "```py\nfrom numpy import mean\nfrom numpy import std\n\ndata_mean, data_std = mean(pdf['dt']), std(pdf['dt'])\n\ncut_off = data_std * 3\nlower, upper = data_mean - cut_off, data_mean + cut_off\n\noutliers = [x for x in pdf['dt'] if x < lower or x > upper]\nprint('Identified outliers: %d' % len(outliers))\nprint(outliers)\n```", "```py\nimport pandas as pd\n\nfrom sklearn import neighbors, metrics\nfrom sklearn.metrics import roc_auc_score, classification_report,\\\nprecision_recall_fscore_support,confusion_matrix,precision_score, \\\nroc_curve,precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n```", "```py\ndf = spark.sql(\"select * from BreastCancer\")\npdf = df.toPandas()\n```", "```py\nX = pdf\ny = pdf['diagnosis']\n\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=40) \n```", "```py\ncols = pdf.columns.drop('diagnosis')\nformula = 'diagnosis ~ ' + ' + '.join(cols)\n```", "```py\nmodel = smf.glm(formula=formula, data=X_train, \n                family=sm.families.Binomial())\nlogistic_fit = model.fit()\n```", "```py\npredictions = logistic_fit.predict(X_test)\npredictions_nominal = [ \"M\" if x < 0.5 else \"B\" for x in \\\n                        predictions]\n```", "```py\nprint(classification_report(y_test, predictions_nominal, digits=3))\n```", "```py\ncfm = confusion_matrix(y_test, predictions_nominal)\nprecision,recall,fscore,support=score(y_test, predictions_nominal,\n                                      average='macro')\n\nprint('Confusion Matrix: \\n', cfm, '\\n')\n```", "```py\nimport pickle\nimport mlflow\n\nwith mlflow.start_run():\n    mlflow.set_experiment(\"/Shared/experiments/BreastCancer\")\n\n    mlflow.log_param(\"formula\", formula)\n    mlflow.log_param(\"family\", \"binomial\")\n\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n    mlflow.log_metric(\"fscore\", fscore)\n    filename = 'finalized_model.sav'\n    pickle.dump(model, open(filename, 'wb'))\n\n    mlflow.log_artifact(filename)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import neighbors, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n```", "```py\ndf = spark.sql(\"select * from ChemicalSensor\")\npdf = df.toPandas()\n```", "```py\nlabel_encoder = LabelEncoder()\ninteger_encoded = \\\n    label_encoder.fit_transform(pdf['classification'])\nonehot_encoder = OneHotEncoder(sparse=False)\n\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n```", "```py\nX = pdf[feature_cols]\ny = onehot_encoded\n\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(X, y, test_size=0.2, random_state=5)\n```", "```py\nclf = DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\n```", "```py\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"AUC:\",roc_auc_score(y_test, y_pred))\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.types import *\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\nimport pickle\nimport mlflow\n```", "```py\nfile_location = \"/FileStore/tables/train_FD001.txt\"\nfile_type = \"csv\"\n\nschema = StructType([\n                     StructField(\"engine_id\", IntegerType()),\n                     StructField(\"cycle\", IntegerType()),\n                     StructField(\"setting1\", DoubleType()),\n                     StructField(\"setting2\", DoubleType()),\n                     StructField(\"setting3\", DoubleType()),\n                     StructField(\"s1\", DoubleType()),\n                     StructField(\"s2\", DoubleType()),\n                     StructField(\"s3\", DoubleType()),\n                     StructField(\"s4\", DoubleType()),\n                     StructField(\"s5\", DoubleType()),\n                     StructField(\"s6\", DoubleType()),\n                     StructField(\"s7\", DoubleType()),\n                     StructField(\"s8\", DoubleType()),\n                     StructField(\"s9\", DoubleType()),\n                     StructField(\"s10\", DoubleType()),\n                     StructField(\"s11\", DoubleType()),\n                     StructField(\"s12\", DoubleType()),\n                     StructField(\"s13\", DoubleType()),\n                     StructField(\"s14\", DoubleType()),\n                     StructField(\"s15\", DoubleType()),\n                     StructField(\"s16\", DoubleType()),\n                     StructField(\"s17\", IntegerType()),\n                     StructField(\"s18\", IntegerType()),\n                     StructField(\"s19\", DoubleType()),\n                     StructField(\"s20\", DoubleType()),\n                     StructField(\"s21\", DoubleType())\n                     ])\n\ndf = spark.read.option(\"delimiter\",\" \").csv(file_location,\n                                            schema=schema, \n                                            header=False)\n```", "```py\ndf.createOrReplaceTempView(\"raw_engine\")\n```", "```py\n%sql\n\n drop table if exists engine;\n\n create table engine as\n (select e.*, CASE WHEN mc - e.cycle = 1 THEN 1 ELSE\n CASE WHEN mc - e.cycle < 14 THEN 2 ELSE\n 0 END END as label\n from raw_engine e\n join (select max(cycle) mc, engine_id from raw_engine group by engine_id) m\n on e.engine_id = m.engine_id)\n```", "```py\nnew_input = spark.sql(\"select * from engine\").toPandas()\ntraining_df, test_df = train_test_split(new_input)\n```", "```py\ndtrain = xgb.DMatrix(training_df[['setting1','setting2','setting3', 's1', 's2', 's3',\n 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n 's15', 's16','s17', 's18', 's19', 's20', 's21']], label=training_df[\"label\"])\n param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'multi:softmax'}\n param['nthread'] = 4\n param['eval_metric'] = 'auc'\n param['num_class'] = 3\n```", "```py\nnum_round = 10\n bst = xgb.train(param, dtrain, num_round)\n```", "```py\ndtest = xgb.DMatrix(test_df[['setting1', 'setting2', 'setting3', \n                             's1', 's2', 's3', 's4', 's5', 's6', \n                             's7', 's8', 's9', 's10', 's11', \n                             's12', 's13', 's14', 's15', 's16',\n                             's17', 's18', 's19', 's20', 's21']])\nypred = bst.predict(dtest)\n\npre_score = precision_score(test_df[\"label\"], ypred, \n                            average='micro')\nprint(\"xgb_pre_score:\",pre_score)\n```", "```py\nwith mlflow.start_run():\n    mlflow.set_experiment(\"/Shared/experiments/\\\n                           Predictive_Maintenance\")\n    mlflow.log_param(\"type\", 'XGBoost')\n    mlflow.log_metric(\"precision_score\", pre_score)\n    filename = 'bst.sav'\n    pickle.dump(bst, open(filename, 'wb'))\n    mlflow.log_artifact(filename)\n```", "```py\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom sparkdl import DeepImageFeaturizer\nfrom pyspark.ml.evaluation import \\\nMulticlassClassificationEvaluator\nfrom pyspark.sql.functions import lit\nimport pickle\nimport mlflow\n\nstorage_account_name = \"Your Storage Account Name\"\nstorage_account_access_key = \"Your Key\"\n```", "```py\nsafe_images = \"wasbs://unsafedrivers@\"+storage_account_name+\\\n               \".blob.core.windows.net/safe/\"\nsafe_df = spark.read.format('image').load(safe_images)\\\n          .withColumn(\"label\", lit(0))\n\nunsafe_images = \"wasbs://unsafedrivers@\"+storage_account_name+\\\n                \".blob.core.windows.net/unsafe/\"\nunsafe_df = spark.read.format('image').load(unsafe_images)\\\n            .withColumn(\"label\", lit(1))\n```", "```py\ndisplay(unsafe_df)\n```", "```py\nunsafe_train, unsafe_test = unsafe_df.randomSplit([0.6, 0.4])\nsafe_train, safe_test = safe_df.randomSplit([0.6, 0.4])\n\ntrain_df = unsafe_train.unionAll(safe_train)\ntest_df = safe_test.unionAll(unsafe_test)\n```", "```py\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", \n                                 outputCol=\"features\", \n                                 modelName=\"ResNet50\")\nlr = LogisticRegression(maxIter=20, regParam=0.05, \n                        elasticNetParam=0.3, labelCol=\"label\")\np = Pipeline(stages=[featurizer, lr])\n```", "```py\np_model = p.fit(train_df)\n```", "```py\npredictions = p_model.transform(test_df)\n\npredictions.select(\"filePath\", \"prediction\").show(truncate=False)\ndf = p_model.transform(test_df)\n\npredictionAndLabels = df.select(\"prediction\", \"label\")\nevaluator = \\\nMulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Training set accuracy = \" + \\\n       str(evaluator.evaluate(predictionAndLabels)))\n```", "```py\nwith mlflow.start_run():\n    mlflow.set_experiment(\"/Shared/experiments/Workplace Safety\")\n\n    mlflow.log_param(\"Model Name\", \"ResNet50\")\n    # Log a metric; metrics can be updated throughout the run\n    precision, recall, fscore, support=score(y_test, y_pred,\n                                             average='macro')\n\n    mlflow.log_metric(\"Accuracy\", \\\n                      evaluator.evaluate(predictionAndLabels))\n\n    filename = 'finalized_model.sav'\n    pickle.dump(p_model, open(filename, 'wb'))\n    # Log an artifact (output file)\n    mlflow.log_artifact(filename)\n```", "```py\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", \n                                 modelName=\"ResNet50\")\n```", "```py\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\",\n                                 modelName=\"InceptionV3\")\n```", "```py\nfor m in ['InceptionV3', 'Xception','ResNet50', 'VGG19']:\n    featurizer = DeepImageFeaturizer(inputCol=\"image\", \n                                     outputCol=\"features\", \n                                     modelName=m)\n```", "```py\npip install opencv-python\n```", "```py\nimport cv2\nfrom time import sleep\n\ndebugging = True\nclassifier = \\\ncv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\nvideo = cv2.VideoCapture(0)\n```", "```py\nwhile True:\n    if not video.isOpened():\n    print('Waiting for Camera.')\n    sleep(5)\n    pass\n```", "```py\nret, frame = video.read()\ngray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n```", "```py\nfaces = classifier.detectMultiScale(gray,\n                                    minNeighbors=5,\n                                    minSize=(100, 100)\n                                    )\n```", "```py\nif debugging:\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    cv2.imshow('Video', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n```", "```py\nif len(faces) > 0:\n    # Your advanced code here\n    pass\n```"]