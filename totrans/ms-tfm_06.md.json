["```py\nAutoModel.from_pretrained('facebook/bart-large')\n```", "```py\nsummarizer = pipeline(\"summarization\")\n```", "```py\nfrom transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\nfrom transformers import pipeline\nmodel = \\\nBartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\ntokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\nnlp=pipeline(\"summarization\", model=model, tokenizer=tokenizer)\ntext='''\nWe order two different types of jewelry from this\ncompany the other jewelry we order is perfect.\nHowever with this jewelry I have a few things I\ndon't like. The little Stone comes out of these\nand customers are complaining and bringing them\nback and we are having to put new jewelry in their\nholes. You cannot sterilize these in an autoclave\nas well because it heats up too much and the glue\ndoes not hold up so the second group of these that\nwe used I did not sterilize them that way and the\nstones still came out. When I use a dermal clamp\nto put the top on the stones come out immediately.\nDO not waste your money on this particular product\nbuy the three mm. that has the claws that hold the\njewelry in those are perfect. So now I'm stuck\nwith jewelry that I can't sell not good for\nbusiness.'''\nq=nlp(text)\nimport pprint\npp = pprint.PrettyPrinter(indent=0, width=100)\npp.pprint(q[0]['summary_text'])\n(' The little Stone comes out of these little stones and customers are complaining and bringing ' 'them back and we are having to put new jewelry in their holes . You cannot sterilize these in an ' 'autoclave because it heats up too much and the glue does not hold up so the second group of ' 'these that we used I did not sterilize them that way and the stones still came out .')\n```", "```py\n    wget https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/gutenberg/austen-emma.txt\n    ```", "```py\n    from tokenizers.models import BPE\n    from tokenizers import Tokenizer\n    from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n    from tokenizers.normalizers import Sequence, Lowercase\n    from tokenizers.pre_tokenizers import ByteLevel\n    from tokenizers.trainers import BpeTrainer\n    ```", "```py\n    tokenizer = Tokenizer(BPE())\n    tokenizer.normalizer = Sequence([\n        Lowercase()\n    ])\n    tokenizer.pre_tokenizer = ByteLevel()\n    tokenizer.decoder = ByteLevelDecoder()\n    ```", "```py\n    trainer = BpeTrainer(vocab_size=50000, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n                \"<s>\",\n                \"<pad>\",\n                \"</s>\",\n                \"<unk>\",\n                \"<mask>\"\n            ])\n    tokenizer.train([\"austen-emma.txt\"], trainer)\n    ```", "```py\n    !mkdir tokenizer_gpt\n    ```", "```py\n    tokenizer.save(\"tokenizer_gpt/tokenizer.json\")\n    ```", "```py\n    from transformers import GPT2TokenizerFast, GPT2Config, TFGPT2LMHeadModel\n    ```", "```py\n    tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"tokenizer_gpt\")\n    ```", "```py\n    tokenizer_gpt.add_special_tokens({\n      \"eos_token\": \"</s>\",\n      \"bos_token\": \"<s>\",\n      \"unk_token\": \"<unk>\",\n      \"pad_token\": \"<pad>\",\n      \"mask_token\": \"<mask>\"\n    })\n    ```", "```py\n    tokenizer_gpt.eos_token_id\n    >> 2\n    ```", "```py\n    tokenizer_gpt.encode(\"<s> this is </s>\")\n    >> [0, 265, 157, 56, 2]\n    ```", "```py\n    config = GPT2Config(\n      vocab_size=tokenizer_gpt.vocab_size,\n      bos_token_id=tokenizer_gpt.bos_token_id,\n      eos_token_id=tokenizer_gpt.eos_token_id\n    )\n    model = TFGPT2LMHeadModel(config)\n    ```", "```py\n    config\n    >> GPT2Config {  \"activation_function\": \"gelu_new\",  \"attn_pdrop\": 0.1,  \"bos_token_id\": 0,  \"embd_pdrop\": 0.1,  \"eos_token_id\": 2,  \"gradient_checkpointing\": false,  \"initializer_range\": 0.02,  \"layer_norm_epsilon\": 1e-05,  \"model_type\": \"gpt2\",  \"n_ctx\": 1024,  \"n_embd\": 768,  \"n_head\": 12,  \"n_inner\": null,  \"n_layer\": 12,  \"n_positions\": 1024,  \"resid_pdrop\": 0.1,  \"summary_activation\": null,  \"summary_first_dropout\": 0.1,  \"summary_proj_to_labels\": true,  \"summary_type\": \"cls_index\",  \"summary_use_proj\": true,  \"transformers_version\": \"4.3.2\",  \"use_cache\": true,  \"vocab_size\": 11750}\n    ```", "```py\n    with open(\"austen-emma.txt\", \"r\", encoding='utf-8') as f:\n        content = f.readlines()\n    ```", "```py\n    content_p = []\n    for c in content:\n        if len(c)>10:\n            content_p.append(c.strip())\n    content_p = \" \".join(content_p)+tokenizer_gpt.eos_token\n    ```", "```py\n    tokenized_content = tokenizer_gpt.encode(content_p)\n    ```", "```py\n    sample_len = 100\n    examples = []\n    for i in range(0, len(tokenized_content)):\n        examples.append(tokenized_content[i:i + sample_len])\n    ```", "```py\n    train_data = []\n    labels = []\n    for example in examples:\n        train_data.append(example[:-1])\n        labels.append(example[1:])\n    ```", "```py\n    Import tensorflow as tf\n    buffer = 500\n    batch_size = 16   \n    dataset = tf.data.Dataset.from_tensor_slices((train_data, labels))\n    dataset = dataset.shuffle(buffer).batch(batch_size, drop_remainder=True)\n    ```", "```py\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n    model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n    ```", "```py\n    epochs = 10\n    model.fit(dataset, epochs=epochs)\n    ```", "```py\n    def generate(start, model): \n        input_token_ids = tokenizer_gpt.encode(start, return_tensors='tf') \n        output = model.generate( \n            input_token_ids, \n            max_length = 500, \n            num_beams = 5, \n            temperature = 0.7, \n            no_repeat_ngram_size=2, \n            num_return_sequences=1 \n        ) \n        return tokenizer_gpt.decode(output[0])\n    ```", "```py\n    generate(\" \", model)\n    ```", "```py\n    generate(\"wetson was very good\")\n    >> 'wetson was very good; but it, that he was a great must be a mile from them, and a miss taylor in the house;'\n    ```", "```py\n    model.save_pretrained(\"my_gpt-2/\")\n    ```", "```py\n    model_reloaded = TFGPT2LMHeadModel.from_pretrained(\"my_gpt-2/\")\n    ```", "```py\n    from transformers import WEIGHTS_NAME, CONFIG_NAME, TF2_WEIGHTS_NAME\n    ```", "```py\n    tokenizer_gpt.save_pretrained(\"tokenizer_gpt_auto/\")\n    ```", "```py\n    {\"model_type\":\"gpt2\",\n    ...\n    }\n    ```", "```py\n    model = AutoModel.from_pretrained(\"my_gpt-2/\", from_tf=True)\n    tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_gpt_auto\")\n    ```", "```py\n    pip install simpletransformers\n    ```", "```py\n    import pandas as pd\n    df = pd.read_csv(\"TR2EN.txt\",sep=\"\\t\").astype(str)\n    ```", "```py\n    data = []\n    for item in digitrons():\n        data.append([\"translate english to turkish\", item[1].EN, item[1].TR])\n    ```", "```py\n    df = pd.DataFrame(data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n    ```", "```py\n    from simpletransformers.t5 import T5Model, T5Args\n    ```", "```py\n    model_args = T5Args()\n    model_args.max_seq_length = 96\n    model_args.train_batch_size = 20\n    model_args.eval_batch_size = 20\n    model_args.num_train_epochs = 1\n    model_args.evaluate_during_training = True\n    model_args.evaluate_during_training_steps = 30000\n    model_args.use_multiprocessing = False\n    model_args.fp16 = False\n    model_args.save_steps = -1\n    model_args.save_eval_checkpoints = False\n    model_args.no_cache = True\n    model_args.reprocess_input_data = True\n    model_args.overwrite_output_dir = True\n    model_args.preprocess_inputs = False\n    model_args.num_return_sequences = 1\n    model_args.wandb_project = \"MT5 English-Turkish Translation\"\n    ```", "```py\n    model = T5Model(\"mt5\", \"google/mt5-small\", args=model_args, use_cuda=False)\n    ```", "```py\n    train_df = df[: 470000]\n    eval_df = df[470000:]\n    ```", "```py\n    model.train_model(train_df, eval_data=eval_df)\n    ```", "```py\n    model_args = T5Args()\n    model_args.max_length = 512\n    model_args.length_penalty = 1\n    model_args.num_beams = 10\n    model = T5Model(\"mt5\", \"outputs\", args=model_args, use_cuda=False)\n    ```"]