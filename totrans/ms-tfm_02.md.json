["```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nimport numpy as np\nimport pandas as pd\ntoy_corpus= [\"the fat cat sat on the mat\",\n             \"the big cat slept\",\n             \"the dog chased a cat\"]\nvectorizer=TfidfVectorizer() \ncorpus_tfidf=vectorizer.fit_transform(toy_corpus)\nprint(f\"The vocabulary size is \\\n                 {len(vectorizer.vocabulary_.keys())} \")\nprint(f\"The document-term matrix shape is\\\n                           {corpus_tfidf.shape}\")\ndf=pd.DataFrame(np.round(corpus_tfidf.toarray(),2))\ndf.columns=vectorizer.get_feature_names()\n```", "```py\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nlabels= [0,1,0]\nclf = SVC()\nclf.fit(df.to_numpy(), labels)\n```", "```py\nclf.predict(df.to_numpy())\nOutput: array([0, 1, 0])\n```", "```py\nimport nltk\nfrom nltk.corpus import gutenberg\nfrom nltk.lm import MLE\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nnltk.download('gutenberg')\nnltk.download('punkt')\nmacbeth = gutenberg.sents('shakespeare-macbeth.txt')\nmodel, vocab = padded_everygram_pipeline(2, macbeth)\nlm=MLE(2)\nlm.fit(model,vocab)\nprint(list(lm.vocab)[:10])\nprint(f\"The number of words is {len(lm.vocab)}\")\n```", "```py\nprint(f\"The frequency of the term 'Macbeth' is {lm.counts['Macbeth']}\")\nprint(f\"The language model probability score of 'Macbeth' is {lm.score('Macbeth')}\")\nprint(f\"The number of times 'Macbeth' follows 'Enter' is {lm.counts[['Enter']]['Macbeth']} \")\nprint(f\"P(Macbeth | Enter) is {lm.score('Macbeth', ['Enter'])}\")\nprint(f\"P(shaking | for) is {lm.score('shaking', ['for'])}\")\n```", "```py\nThe frequency of the term 'Macbeth' is 61\nThe language model probability score of 'Macbeth' is 0.00226\nThe number of times 'Macbeth' follows 'Enter' is 15 \nP(Macbeth | Enter) is 0.1875\nP(shaking | for) is 0.0121\n```", "```py\nlm.generate(10, text_seed=['<s>'], random_seed=42)\n```", "```py\n['My', 'Bosome', 'franchis', \"'\", 's', 'of', 'time', ',', 'We', 'are']\n```", "```py\nfrom gensim.models import Word2vec\nmodel = Word2vec(sentences=macbeth, size=100, window= 4, min_count=10, workers=4, iter=10)\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport random\nnp.random.seed(42)\nwords=list([e for e in model.wv.vocab if len(e)>4]) \nrandom.shuffle(words)\nwords3d = PCA(n_components=3,random_state=42).fit_transform(model.wv[words[:100]])\ndef plotWords3D(vecs, words, title):\n   ...\nplotWords3D(words3d, words, \"Visualizing Word2vec Word Embeddings using PCA\")\n```", "```py\n$ wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip \n$ unzip SST-2.zip\n```", "```py\nimport tensorflow as tf\nimport pandas as pd \ndf=pd.read_csv('SST-2/train.tsv',sep=\"\\t\")\nsentences=df.sentence\nlabels=df.label\n```", "```py\nmax_sen_len=max([len(s.split()) for s in sentences])\nwords = [\"PAD\"]+\\\n     list(set([w for s in sentences for w in s.split()]))\nword2idx= {w:i for i,w in enumerate(words)}\nmax_words=max(word2idx.values())+1\nidx2word= {i:w for i,w in enumerate(words)}\ntrain=[list(map(lambda x:word2idx[x], s.split()))\\\n                                 for s in sentences]\n```", "```py\nfrom keras import preprocessing\ntrain_pad = preprocessing.sequence.pad_sequences(train,\n                                    maxlen=max_sen_len)\nprint('Train shape:', train_pad.shape)\nOutput: Train shape: (67349, 52)\n```", "```py\nfrom keras.layers import LSTM, Embedding, Dense\nfrom keras.models import Sequential\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(train_pad,labels, epochs=30, batch_size=32, validation_split=0.2)\n```", "```py\nimport matplotlib.pyplot as plt\ndef plot_graphs(history, string):\n    ...\nplot_graphs(history, 'acc')\nplot_graphs(history, 'loss')\n```", "```py\nfrom keras import layers\nmodel = Sequential()\nmodel.add(layers.Embedding(max_words, 32, input_length=max_sen_len))\nmodel.add(layers.Conv1D(32, 8, activation='relu'))\nmodel.add(layers.MaxPooling1D(4))\nmodel.add(layers.Conv1D(32, 3, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1, activation= 'sigmoid')\nmodel.compile(loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(train_pad,labels, epochs=15, batch_size=32, validation_split=0.2)\n```"]