- en: Chapter 5. Image Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vision is arguably the most important human sense. We rely on our vision to
    recognize our food, to run away from danger, to recognize our friends and family,
    and to find our way in familiar surroundings. We rely on our vision, in fact,
    to read this book and to recognize each and every letter and symbol printed in
    it. However, image recognition has (and in many ways still is) for the longest
    time been one of the most difficult problems in computer science. It is very hard
    to teach a computer programmatically how to recognize different objects, because
    it is difficult to explain to a machine what features make up a specified object.
    In deep learning, however, as we have seen, the neural network learns by itself,
    that is, it learns what features make up each object, and it is therefore well
    suited for a task such as image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Similarities between artificial and biological models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuition and justification for CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers in deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarities between artificial and biological models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Human vision is a complex and heavily structured process. The visual system
    works by hierarchically understanding reality through the retina, the thalamus,
    the visual cortex, and the inferior temporal cortex. The input to the retina is
    a two-dimensional array of color intensities that is sent, through the optical
    nerve, to the thalamus. The thalamus receives sensory information from all of
    our senses with the exception of the olfactory system and then it forwards the
    visual information collected from the retina to the primary visual cortex, which
    is the striate cortex (called V1), which extracts basic information such as lines
    and movement directions. The information then moves to the V2 region that is responsible
    for color interpretation and color constancy under different lighting conditions,
    then to the V3 and V4 regions that improve color and form perception. Finally,
    the information goes down to the **Inferior Temporal** cortex (**IT**) for object
    and face recognition (in reality, the IT region is also further subdivided in
    three sub-regions, the posterior IT, central IT, and anterior IT). It is therefore
    clear that the brain processes visual information by hierarchically processing
    the information at different levels. Our brain then seemingly works by creating
    simple abstract representations of reality at different levels that can then be
    recombined together (see for reference: J. DiCarlo, D. Zoccolan, and N. Rust,
    *How does the brain solve visual object recognition?*, [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Deep Learning neural networks we have seen so far work similarly by creating
    abstract representations, as we have seen in RBMs, for example, but there is another
    important piece to the puzzle for understanding sensory information: the information
    we extract from sensory inputs is often determined mostly by the information most
    closely related. Visually, we can assume that pixels that are close by are most
    closely related and their collective information is more relevant than what we
    can derive from pixels very far from each other. In understanding speech, as another
    example, we have discussed how the study of tri-phones is important, that is,
    the fact that the understanding of a sound is dependent on the sounds preceding
    and following it. To recognize letters or digits, we need to understand the dependency
    of pixels close by, since that is what determines the shape of the element to
    figure out the difference between, say, a 0 or a 1\. Pixels that are very far
    from those making up a 0 hold, in general, little or no relevance for our understanding
    of the digit "0". Convolutional networks are built exactly to address this issue:
    how to make information pertaining to neurons that are closer more relevant than
    information coming from neurons that are farther apart. In visual problems, this
    translates into making neurons process information coming from pixels that are
    near, and ignoring information related to pixels that are far apart.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition and justification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already mentioned in [Chapter 3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 3. Deep Learning Fundamentals"), *Deep Learning Fundamentals*, the paper
    published in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton titled:
    *ImageNet Classification with Deep Convolutional Neural Networks*. Though the
    genesis of convolutional may be traced back to the ''80s, that was one of the
    first papers that highlighted the deep importance of convolutional networks in
    image processing and recognition, and currently almost no deep neural network
    used for image recognition can work without some convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important problem that we have seen when working with classical feed-forward
    networks is that they may overfit, especially when working with medium to large
    images. This is often due to the fact that neural networks have a very large number
    of parameters, in fact in classical neural nets all neurons in a layer are connected
    to each and every neuron in the next. When the number of parameters is large,
    over-fitting is more likely. Let''s look at the following images: we can fit the
    data by drawing a line that goes exactly through all the points, or better, a
    line that will not match exactly the data but is more likely to predict future
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intuition and justification](img/00201.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The points in the figure represent input data points. While they clearly follow
    the shape of a parabola, because of noise in the data, they may not be precisely
    plotted onto a parabola
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example of the two pictures represented, we overfit the data.
    In the second we have matched our prediction to the data in such a way that our
    prediction is more likely to better predict future data. In the first case, we
    just need three parameters to describe the curve: *y = ax² + bx + c*, while in
    the second case we would need many more than just three parameters to write the
    equation for that curve. This gives an intuitive explanation of why, sometimes,
    having too many parameters may not be a good thing and it may lead to over-fitting.
    A classical feed-forward network for an image as small as those in the `cifar10`
    examples (`cifar10` is an established computer-vision dataset consisting of 60000
    32 x 32 images divided in to 10 classes, and we will see a couple of examples
    from this dataset in this chapter) has inputs of size 3 x 32 x 32, which is already
    about four times as large as a simple `mnist` digit image. Larger images, say
    3 x 64 x 64, would have about as many as 16 times the number of input neurons
    multiplying the number of connection weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Intuition and justification](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the left figure we draw a line that matches the data exactly. In the second
    figure we draw a line that approximates the shape of the line connecting the data
    points, but that does not match exactly the data points. The second curve, though
    less precise on the current input, is more likely to predict future data points
    than the curve in the first figure.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional networks reduce the number of parameters needed, since they require
    neurons to only connect locally to neurons corresponding to neighboring pixels,
    and therefore help avoid overfitting. In addition, reducing the number of parameters
    also helps computationally. In the next section, will introduce some convolutional
    layer examples to help the intuition and then we will move to formally define
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A convolutional layer (sometimes referred to in the literature as "filter")
    is a particular type of neural network that manipulates the image to highlight
    certain features. Before we get into the details, let's introduce a convolutional
    filter using some code and some examples. This will make the intuition simpler
    and will make understanding the theory easier. To do this we can use the `keras`
    datasets, which makes it easy to load the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import `numpy`, then the `mnist` dataset, and `matplotlib` to show
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define our main function that takes in an integer, corresponding to
    the image in the `mnist` dataset, and a filter, in this case we will define the
    `blur` filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define a new image `imC`, of size `(im.width-2, im.height-2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point we do the convolution, which we will explain soon (as we will
    see, there are in fact several types of convolutions depending on different parameters,
    for now we will just explain the basic concept and get into the details later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to display the original image and the new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to load the `mnist` dataset using Keras as we did in [Chapter
    3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 3. Deep
    Learning Fundamentals"), *Deep Learning Fundamentals*. Also, let''s define a filter.
    A filter is a small region (in this case 3 x 3) with each entry defining a real
    value. In this case we define a filter with the same value all over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since we have nine entries, we set the value to be 1/9 to normalize the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'And we can call the `main` function on any image (expressed by an integer that
    indicates the position) in such a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at what we did. We multiplied each entry of the filter with an entry
    of the original image, and then we summed them all up to get a single value. Since
    the filter size is smaller than the image size, we moved the filter by 1 pixel
    and kept doing this process until we covered the whole image. Since the filter
    was composed by values that are all equal to 1/9, we have in fact averaged all
    input values with the values that are close to it, and this has the effect of
    blurring the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00203.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On top is the original mnist image, on the bottom is the new image after we
    applied the filter
  prefs: []
  type: TYPE_NORMAL
- en: 'In the choice of the filter we can use any value we want; in this case we have
    used values that are all the same. However, we can instead use different values,
    for example values that only look at the neighboring values of the input, add
    them up, and subtract the value of the center input. Let''s define a new filter,
    and let''s call it edges, in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now apply this filter, instead of the filter `blur` defined earlier,
    we get the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00204.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On top is the original mnist image, on the bottom is the new image after we
    applied the filter
  prefs: []
  type: TYPE_NORMAL
- en: It is clear, therefore, that filters can alter the images, and show "features"
    that can be useful to detect and classify images. For example, to classify digits,
    the color of the inside is not important, and a filter such as "edges" helps identify
    the general shape of the digit which is what is important for a correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of filters in the same way we think about neural networks, and
    think that the filter we have defined is a set of weights, and that the final
    value represents the activation value of a neuron in the next layer (in fact,
    even though we chose particular weights to discuss these examples, we will see
    that the weights will be *learned* by the neural network using back-propagation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00205.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The filter covers a fixed region, and for each neuron in that region, it defines
    a connection weight to a neuron in the next layer. The neuron in the next layer
    will then have an input value equal to the regular activation value calculated
    by summing the contributions of all input neurons mediated by the corresponding
    connection weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then keep the same weights and we slide the filter across, generating a
    new set of neurons, which correspond to the filtered image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can keep repeating the process until we have moved across the whole image,
    and we can repeat this process with as many filters as we like, creating a new
    set of images, each of which will have different features or characteristics highlighted.
    While we have not used a bias in our examples, it is also possible to add a bias
    to the filter, which will be added to the neural network, and we can also define
    different activity functions. In our code example you will notice that we have
    forced the value to be in the range (0, 255), which can be thought of as a simple
    threshold function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00207.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As the filter moves across the image, we define new activation values for the
    neurons in the output image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since one may define many filters, we should think of the output not as a single
    image, but as a set of images, one for each filter defined. If we used just the
    "edges" and the "blur" filter, the output layer would therefore have two images,
    one per filter chosen. The output will therefore have, besides a width and a height,
    also a depth equal to the number of filters chosen. In actuality, the input layer
    can also have a depth if we use color images as input; images are in fact usually
    comprised of three channels, which in computer graphics are represented by RGB,
    the red channel, the green channel, and the blue channel. In our example, the
    filter is represented by a two-dimensional matrix (for example the `blur` filter
    is a 3 x 3 matrix with all entries equal to 1/9\. However, if the input is a color
    image, the filter will also have a depth (in this case equal to three, the number
    of color channels), and it will therefore be represented by three (number of color
    channels) 3 x 3 matrices. In general, the filter will therefore be represented
    by a three-dimensional array, with a width, a height, and a depth, which are sometimes
    called "volumes". In the preceding example, since the `mnist` images are gray-scale
    only, the filter had depth 1\. A general filter of depth *d* is therefore comprised
    of *d* filters of the same width and height. Each of those *d* filters are called
    a "slice" or a "leaf":'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, as before, for each "leaf" or "slice", we connect each neuron in
    the small sub-region, as well as a bias, to a neuron and we calculate its activation
    value defined by the connection weights set in the filter, and we slide the filter
    across the whole area. Such a procedure, as it is easy to calculate, requires
    a number of parameters that are equal to the number of weights defined by the
    filter (in our example above, this would be 3 x 3=9), multiplied by the number
    of "leaves", that is, the depth of the layer, plus one bias. This defines a feature
    map, because it highlights specific features of the input. In our code above we
    defined two feature maps, a "blur" and an "edges". Therefore, we need to multiply
    the number of parameters by the number of feature maps. Note that the weights
    for each filter are fixed; when we slide the filter across the region we do not
    change weights. Therefore, if we start with a layer with size (width, height,
    depth), and a filter of dimension `(filter_w, filter_h)`, the output layer after
    having applied the convolution is `(width - filter_w + 1, height – filter_h +
    1)`. The depth of the new layer depends on how many feature maps we want to create.
    In our `mnist` code example earlier, if we applied both the `blur` and `edges`
    filters, we would have an input layer of size (28 x 28 x 1), since there is only
    one channel because the digits are gray-scale images, and an output layer of dimension
    (26 x 26 x 2), since our filters had dimension (3 x 3) and we used two filters.
    The number of parameters is only 18 (3 x 3 x 2), or 20 (3 x 3 x 2+2) if we add
    a bias. This is way less than what we would need to have with classical feed-forward
    networks, whereas, since the input is 784 pixels, a simple hidden layer with just
    50 neurons would need 784 x 50 = 39200 parameters, or 39250 if we add the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers](img/00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We slide the filter across the image over all the "leaves" comprising the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers moreover can work better, since each neuron gets its input
    only from neighboring neurons, and does not care about collecting input from neurons
    that are distant from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Stride and padding in convolutional layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The examples we have shown, aided by pictures, in fact only tell one particular
    application of filters (as we mentioned earlier, there are different types of
    convolutions, depending on the parameters chosen). In fact, the size of the filter
    may vary, as well as how it moves across the image and its behavior at the edges
    of the image. In our example, we moved the filter across the image 1 pixel at
    a time. How many pixels (neurons) we skip each time we move our filter is called
    the stride. In the above example, we used a stride of 1, but it is not unusual
    to use larger strides, of 2 or even more. In this case the output layer would
    have a smaller width and height:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stride and padding in convolutional layers](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A filter applied with stride 2—the filter is moved by two pixels at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we might also decide to apply the filter partially outside of
    the original picture. In that case, we would assume that the missing neurons would
    have value 0\. This is called padding; that is, we add 0 value neurons outside
    the original image. This can be useful if, for example, we want the output image
    to be the same size as the input image. Above, we wrote the formula for the size
    of the new output image in case of zero padding, and that was (`width - filter_w
    + 1, height – filter_h + 1`) for an input of size (`width, height`) and a filter
    of dimensions (`filter_w, filter_h`). If we use a padding `P` all around the image,
    the output size will be (`width + 2P - filter_w + 1, height + 2P – filter_h +
    1`). To summarize, in each dimension (either width or height), let the size of
    the input slice be called *I=(I[w]*, *I[h])*, the size of the filter *F=(F[w]*,*F[h])*,
    the size of the stride *S=(S[w]*,*S[h])*, and the size of the padding *P=(P[w]*,P[h],
    then the size *O=(O[w]*, O[h] for the output slice is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stride and padding in convolutional layers](img/00211.jpeg)![Stride and padding
    in convolutional layers](img/00212.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This of course identifies one of the constraints for *S*, that it must divide
    *(I + 2P – F)* both in the width direction and the height direction. The dimension
    for the final volume is obtained by multiplying for the number of desired feature
    maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of parameters *W* used, instead, is independent of the stride and
    padding, and it is just a function of the (square) size of the filter, the depth
    *D* (number of slices) of the input, and the number of feature maps *M* chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stride and padding in convolutional layers](img/00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The use of padding (also called zero-padding, as we are padding the image with
    zeros) is sometimes useful if we are seeking to make the output dimension the
    same as the input dimension. If we use a filter of dimension (2 x 2), it is in
    fact clear that by applying a padding of value 1 and a stride of 1, we have the
    dimension of the output slice the same as the size of the input slice.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have derived the formula for the size for each slice
    in a convolutional layer. As we discussed, one of the advantages of convolutional
    layers is that they reduce the number of parameters needed, improving performance
    and reducing over-fitting. After a convolutional operation, another operation
    is often performed—pooling. The most classical example is called max-pooling,
    and this means creating (2 x 2) grids on each slice, and picking the neuron with
    the maximum activation value in each grid, discarding the rest. It is immediate
    that such an operation discards 75% of the neurons, keeping only the neurons that
    contribute the most in each cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parameters for each pooling layer, similar to the stride and
    padding parameters found in convolutional layers, and they are the size of the
    cell and the stride. One typical choice is to choose a cell size of 2 and a stride
    of 2, though it is not uncommon to pick a cell size of 3 and a stride of 2, creating
    some overlap. It should be noted, however, that if the cell size is too large,
    the pooling layer may be discarding too much information and is not helpful. We
    can derive a formula for the output of a pooling layer, similar to the one we
    derived for convolutional layers. Let''s call, like before, *I* the size of the
    input slice, *F* the size of the cell (also called the receptive field), *S* the
    size of the stride, and *O* the size of the output. Pooling layers typically do
    not use any padding. Then we obtain in each dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling layers](img/00214.jpeg)![Pooling layers](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Pooling layers do not change the depth of the volume of the layer, keeping the
    same number of slices, since the pooling operation is performed in each slice
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that, similar to how we can use different activation
    functions, we can also use different pooling operations. Taking the max is one
    of the most common operations, but it is not uncommon to take the average of all
    the values, or even an *L ²* measure, which is the square root of the sum of all
    the squares. In practice, max-pooling often performs better, since it retains
    the most relevant structures in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted, however, that while pooling layers are still very much
    used, one can sometimes achieve similar or better results by simply using convolutional
    layers with larger strides instead of pooling layers (see, for example, J. Springerberg,
    A. Dosovitskiy, T. Brox, and M. Riedmiller, *Striving for Simplicity: The All
    Convolutional Net*, (2015), [https://arxiv.org/pdf/1412.6806.pdf](https://arxiv.org/pdf/1412.6806.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: However, if pooling layers are used, they are generally used in the middle of
    a sequence of a few convolutional layers, generally after every other convolutional
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to note that pooling layers add no new parameters, since
    they are simply extracting values (like the max) without needing additional weights
    or biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling layers](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'An example of a max-pool layer: the maximum from each 2x2 cell is calculated
    to generate a new layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important technique that can be applied after a pooling layer, but
    can also generally be applied to a fully connected layer, is to "drop" some neurons
    and their corresponding input and output connections randomly and periodically.
    In a dropout layer we specify a probability *p* for neurons to "drop out" stochastically.
    During each training period, each neuron has probability *p* to be dropped out
    from the network, and a probability *(1-p)* to be kept. This is to ensure that
    no neuron ends up relying too much on other neurons, and each neuron "learns"
    something useful for the network. This has two advantages: it speeds up the training,
    since we train a smaller network each time, and also helps in preventing over-fitting
    (see N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    *Dropout: A Simple Way to Prevent Neural Networks from Overfitting,* in *Journal
    of Machine Learning Research* 15 (2014), 1929-1958, [http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: It is however, important to note that dropout layers are not strictly restricted
    to convolutional layers; in fact dropout layers find applications in different
    neural network architectures. Dropout layers should be regarded as a regularization
    technique for reducing overfitting, and we mention them since they will be explicitly
    used in our code example.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers in deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we introduced the idea of deep learning, we discussed how the word "deep"
    refers not only to the fact that we use many layers in our neural net, but also
    to the fact that we have a "deeper" learning process. Part of this deeper learning
    process was the ability of the neural net to learn features autonomously. In the
    previous section, we defined specific filters to help the network learn specific
    characteristics. This is not necessarily what we want. As we discussed, the point
    of deep learning is that the system learns on its own, and if we had to teach
    the network what features or characteristics are important, or how to learn to
    recognize digits by applying layers such as the *edges* layer that highlights
    the general shape of a digit, we would be doing most of the work and possibly
    constraining the network to learn features that may be relevant to us but not
    to the network, degrading its performance. The point of Deep Learning is that
    the system needs to learn by itself.
  prefs: []
  type: TYPE_NORMAL
- en: In the [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we showed how the hidden layers
    in a neural net learn the weights by using back-propagation; the weights were
    not set by the operator. Similarly, it makes no sense for the operator to set
    the weights in the filters, rather we want the neural net to learn the weights
    in the filters, again by using back-propagation. All the operator needs to do
    is to set the size of the layer, the stride, and the padding, and decide how many
    feature maps we are asking the network to learn. By using supervised learning
    and back-propagation, the neural net will set the weights (and biases) for each
    filter autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: We should also mention that, while it may be simpler to use the description
    of convolutional layers we have provided, convolutional layers could still be
    thought of as the regular fully connected layers we introduced in [Chapter 3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 3. Deep Learning Fundamentals"), *Deep Learning Fundamentals*. In fact,
    the two main characteristics of convolutional layers are the fact that each neuron
    only connects to a small region of the input layer, and the fact that different
    slices corresponding to the same small region share the same weights. These two
    properties can be rendered with a regular layer by creating a matrix of weights
    which is sparse, that is, with many zeros (due to the local connectivity of the
    convolutional network) and which has many weights repeated (due to the parameter
    sharing properties across slices). Understanding this point makes it clear why
    convolutional layers have much fewer parameters than fully connected layers; in
    convolutional layers the matrix of weights is comprised mostly of zeros entries.
    In practice, however, it helps the intuition to think of convolutional layers
    as they have been described in this chapter, since one can better appreciate how
    convolutional layers can highlight features of the original image, as we have
    shown graphically by blurring the image or highlighting the contours of the digits
    in our examples.
  prefs: []
  type: TYPE_NORMAL
- en: One more important point to make is that convolutional networks should generally
    have a depth equal to a number which is iteratively divisible by 2, such as 32,
    64, 96, 128, and so on. This is important when using pooling layers, such as the
    max-pool layer, since the pooling layer (if it has size (2,2)) will divide the
    size of the input layer, similarly to how we should define "stride" and "padding"
    so that the output image will have integer dimensions. In addition, padding can
    be added to ensure that the output image size is the same as the input.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers in Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the intuition of how convolutional layers work, we are going
    to implement a simple example of a convolutional layer using Theano.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start by importing the modules that are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Theano works by first creating a symbolic representation of the operations we
    define. We will later have another example using Keras, that, while it provides
    a nice interface to make creating neural networks easier, it lacks some of the
    flexibility one can have by using Theano (or TensorFlow) directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the variables needed and the neural network operations, by defining
    the number of feature maps (the depth of the convolutional layer) and the size
    of the filter, then we symbolically define the input using the Theano tensor class.
    Theano treats the image channels as a separate dimension, so we define the input
    as a tensor4\. Next we initialize the weights using a random distribution between
    -0.2 and 0.2\. We are now ready to call the Theano convolution operation and then
    apply the logistic sigmoid function on the output. Finally, we define the function
    `f` that takes an input and defines an output using the operations used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `skimage` module we imported can be used to load an image, we will import
    an image called `lena` *,* then after having reshaped the image to be passed in
    to the Theano function we defined, we can call the Theano function on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is it. We can now print out the original image and the filtered images
    by using this simple code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If the reader were interested in visualizing the weights used, in Theano, it
    is possible to print out the values by using `print W.get_value()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from this code is as follows: (since we have not fixed a random
    seed, and since the weights are initialized randomly, the reader may get slightly
    different images):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional layers in Theano](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The original and filtered images.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer example with Keras to recognize digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the third chapter, we introduced a simple neural network to classify digits
    using Keras and we got 94%. In this chapter, we will work to improve that value
    above 99% using convolutional networks. Actual values may vary slightly due to
    variability in initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we can start by improving the neural network we had defined by
    using 400 hidden neurons and run it for 30 epochs; that should get us already
    up to around 96.5% accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we could try scaling the input. Images are comprised of pixels, and each
    pixel has an integer value between 0 and 255\. We could make that value a float
    and scale it between 0 and 1 by adding these four lines of code right after we
    define our input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run our network now, we get a poorer accuracy, just above 92%, but we
    need not worry. By rescaling, we have in fact changed the values of the gradient
    of our function, which therefore will converge much more slowly, but there is
    an easy work-around. In our code, inside the `model.compile` function, we defined
    an optimizer equal to "sgd". That is the standard stochastic gradient descent,
    which uses the gradient to converge to a minimum. However, Keras allows other
    choices, in particular "adadelta", which automatically uses momentum and adjusts
    the learning rate depending on the gradient, making it larger or smaller in an
    inversely proportional way to the gradient, so that the network does not learn
    too slowly and it does not skip minima by taking too large a step. By using adadelta,
    we dynamically adjust the parameters with time (see also: Matthew D. Zeiler, *Adadelta:
    An Adaptive Learning Rate Method*, arXiv:1212.5701v1 ([https://arxiv.org/pdf/1212.5701v1.pdf](https://arxiv.org/pdf/1212.5701v1.pdf))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the main function, we are now going to change our compile function and
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run our algorithm again, we are now at about 98.25% accuracy. Finally,
    let''s modify our first dense (fully connected) layer and use the `relu` activation
    function instead of the `sigmoid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This will now give around 98.4% accuracy. The problem is that now it becomes
    increasingly difficult to improve our results using a classical feed-forward architecture,
    due to over-fitting, and increasing the number of epochs or modifying the number
    of hidden neurons will not bring any added benefit, as the network will simply
    learn to over-fit the data, rather than learn to generalize better. We are therefore
    now going to introduce convolutional networks in the example.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we keep our input scaled between 0 and 1\. However, we reshape
    the data to a volume of size (28, 28, 1) = (width of image, height of image, number
    of channels) in order to be used by a convolutional layer, and we bring the number
    of hidden neurons down to 200, but we now add a simple convolutional layer at
    the beginning, with a 3 x 3 filter, no padding, and stride 1, followed by a max-pooling
    layer of stride 2 and size 2\. In order to be able to then pass the output to
    the dense layer, we need to flatten the volume (convolutional layers are volumes)
    to pass it to the regular dense layer with 100 hidden neurons by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also reduce the number of epochs down to just 8, and we will get an
    accuracy of around 98.55%. Often it is common to use pairs of convolutional layers,
    so we add a second one similar to the first one, (before the pooling layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: And we will now be at 98.9%.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get to 99%, we add a dropout layer as we have discussed. This does
    not add any new parameters, but helps prevent overfitting, and we add it right
    before the flatten layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example we use a dropout rate of about 25%, so each neuron is randomly
    dropped once every four times.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take us above 99%. If we want to improve more (accuracy may vary
    due to differences in initializations), we can also add more dropout layers, for
    example, after the hidden layer and increase the number of epochs. This would
    force the neurons on the final dense layer, prone to overfit, to be dropped randomly.
    Our final code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to further optimize this network, but the point here is not to
    get an award-winning score, but to understand the process, and understand how
    each step we have taken has improved performance. It is also important to understand
    that by using the convolutional layer, we have in fact also avoided overfitting
    our network, by utilizing fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer example with Keras for cifar10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now try to use the same network on the `cifar10` dataset. In [Chapter
    3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 3. Deep
    Learning Fundamentals"), *Deep Learning Fundamentals*, we were getting a low 50%
    accuracy on test data, and to test the new network we have just used for the `mnist`
    dataset, we need to just make a couple of small changes to our code: we need to
    load the `cifar10` dataset (without doing any re-shaping, those lines will be
    deleted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And then change the input values for the first convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Running this network for 5 epochs will give us around 60% accuracy (up from
    about 50%) and 66% accuracy after 10 epochs, but then the network starts to overfit
    and stops improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course the `cifar10` images have 32 x 32 x 3 = 3072 pixels, instead of 28
    x 28=784 pixels, so we may need to add a couple more convolutional layers, after
    the first two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In general, it is better to split large convolutional layers into smaller-sized
    convolutional layers. For example, if we have two consecutive (3 x 3) convolutional
    layers, the first layer will have a (3 x 3) view of the input image, and the second
    layer will have a (5 x 5) view of the input image for each pixel. However, each
    layer will have non-linear features that will stack up, creating more complex
    and interesting features of the input than we would get by simply creating a single
    (5 x 5) filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run this network for 3 epochs, we are also getting around 60%, but after
    20 epochs we are up to 75% accuracy by using a simple network. The state-of-the-art
    convolutional networks can get around 90% accuracy, but require longer training
    and are more complicated. We will graphically present the architecture of one
    important convolutional neural network, called VGG-16, in the next paragraph so
    that the user can try to implement it using Keras or any other language he or
    she is comfortable with, such as Theano or TensorFlow (the network was originally
    created using Caffe, an important deep learning framework developed at Berkeley,
    see: [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with neural networks, it is important to be able to "see" the
    weights the network has learned. This allows the user to understand what features
    the network is learning and to allow for better tuning. This simple code will
    output all the weights for each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If, for example, we are interested in the weights for layer 0, the first convolutional
    layer, we can apply them to the image to see what features the network is highlighting.
    If we apply these filters to the image `lena`, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A convolutional layer example with Keras for cifar10](img/00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see how each filter is highlighting different features.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, neural networks, and convolutional networks in particular,
    work by tuning the weights of the network as if they were coefficients of a large
    equation in order to get the correct output given a specific input. The tuning
    happens through back-propagation to move the weights towards the best solution
    given the chosen neural net architecture. One of the problems is therefore finding
    the best initialization values for the weights in the neural network. Libraries
    such as Keras can automatically take care of that. However, this topic is important
    enough to be worth discussing this point.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines have been used to pre-train the network by using
    the input as the desired output to make the network automatically learn representations
    of the input and tune its weights accordingly, and this topic has already been
    discussed in [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there exists many pre-trained networks that offer good results.
    As we have mentioned, many people have been working on convolutional neural networks
    and have been getting impressive results, and one can often save time by reutilizing
    the weights learnt by these networks and applying them to other projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VGG-16 model used in K. Simonyan, A. Zisserman, *Very Deep Convolutional
    Networks for Large-Scale Image Recognition* arXiv:1409.1556, [http://arxiv.org/pdf/1409.1556v6.pdf](http://arxiv.org/pdf/1409.1556v6.pdf),
    is an important model for image recognition. In this model, the input is a fixed
    224 x 224 RGB-valued image where the only pre-processing is subtracting the mean
    RGB-value computed on the training set. We outline the architecture for this network
    in the attached diagram, and the user can try to implement by himself or herself
    such a network, but also keep in mind the computationally intensive nature of
    running such a network. In this network the architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pre-training](img/00219.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: VGG-16 convolutional neural network architecture by Simonyan and Zisserman.
  prefs: []
  type: TYPE_NORMAL
- en: We also refer the interested reader to another noteworthy example, the AlexNet
    network, contained in Alex Krizhevsky, Ilya Sutskeve, Geoffrey Hinton, *ImageNet
    Classification with Deep Convolutional Networks*, in Advances in Neural Information
    Processing Systems 25 (NIPS 2012), [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf),
    that we will not be discussing this here for the sake of brevity, but we invite
    the interested reader to look at it. We also invite the interested reader to look
    at [https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)
    for code examples of the VGG-16 and other networks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be noted, as it may have become clear, that there is no general architecture
    for a convolutional neural network. However, there are general guidelines. Normally,
    pooling layers follow convolutional layers, and often it is customary to stack
    two or more successive convolutional layers to detect more complex features, as
    it is done in the VGG-16 neural net example shown earlier. Convolutional networks
    are very powerful. However, they can be quite resource-heavy (the VGG-16 example
    above, for example, is relatively complex), and usually require a long training
    time, which is why the use of GPU can help speed up performance. Their strength
    comes from the fact that they do not focus on the entire image, rather they focus
    on smaller sub-regions to find interesting features that make up the image in
    order to be able to find discriminating elements between different inputs. Since
    convolutional layers are very resource-heavy, we have introduced pooling layers
    that help reduce the number of parameters without adding complexity, while the
    use of dropout layers helps insure that no neuron will rely too heavily on other
    neurons, and therefore each element in the neural net will contribute to learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, starting from drawing an analogy with how our visual cortex
    works, we have introduced convolutional layers and followed up with a descriptive
    intuition of why they work. We have introduced filters, we have also covered how
    filters can be of different sizes and can have different padding, and we have
    looked at how setting zero-padding can ensure that the resulting image has the
    same size as the original image. As mentioned above, pooling layers can help reduce
    the complexity, while dropout layers can make the neural network much more effective
    at recognizing patterns and features, and in particular can be quite effective
    at reducing the risk of over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: In general, in the examples given, and in the `mnist` example in particular,
    we have shown how convolutional layers in neural networks can achieve much better
    accuracy than regular deep neural networks when dealing with images, reaching
    over 99% accuracy in digit recognition, without overfitting the model, by limiting
    the use of parameters. In the next chapters, we will look at speech recognition
    and then start looking at examples of models that use reinforcement learning,
    rather than supervised or unsupervised learning, by introducing deep learning
    for board games and deep learning for video games.
  prefs: []
  type: TYPE_NORMAL
