- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Training Simple Machine Learning Algorithms for Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为分类训练简单的机器学习算法
- en: 'In this chapter, we will make use of two of the first algorithmically described
    machine learning algorithms for classification: the perceptron and adaptive linear
    neurons. We will start by implementing a perceptron step by step in Python and
    training it to classify different flower species in the Iris dataset. This will
    help us to understand the concept of machine learning algorithms for classification
    and how they can be efficiently implemented in Python.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用两种最早被算法描述的用于分类的机器学习算法：感知器和自适应线性神经元。我们将从头开始在Python中逐步实现感知器，并训练它来对鸢尾花数据集中的不同花种进行分类。这将帮助我们理解分类的机器学习算法的概念以及它们如何在Python中高效实现。
- en: Discussing the basics of optimization using adaptive linear neurons will then
    lay the groundwork for using more sophisticated classifiers via the scikit-learn
    machine learning library in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using Scikit-Learn*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨使用自适应线性神经元进行优化基础，为通过scikit-learn机器学习库使用更复杂分类器奠定基础，《第3章》*使用scikit-learn进行机器学习分类器之旅*。
- en: 'The topics that we will cover in this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Building an understanding of machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立对机器学习算法的理解
- en: Using pandas, NumPy, and Matplotlib to read in, process, and visualize data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas、NumPy和Matplotlib读取、处理和可视化数据
- en: Implementing linear classifiers for 2-class problems in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中为2类问题实现线性分类器
- en: Artificial neurons – a brief glimpse into the early history of machine learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元——对机器学习早期历史的简要介绍
- en: 'Before we discuss the perceptron and related algorithms in more detail, let’s
    take a brief tour of the beginnings of machine learning. Trying to understand
    how the biological brain works in order to design an **artificial intelligence**
    (**AI**), Warren McCulloch and Walter Pitts published the first concept of a simplified
    brain cell, the so-called **McCulloch-Pitts** (**MCP**) neuron, in 1943 (*A Logical
    Calculus of the Ideas Immanent in Nervous Activity* by *W. S. McCulloch* and *W.
    Pitts*, *Bulletin of Mathematical Biophysics*, 5(4): 115-133, 1943).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细讨论感知器及相关算法之前，让我们简要回顾一下机器学习的起源。为了设计一种**人工智能**（**AI**），沃伦·麦卡洛克和沃尔特·皮茨在1943年发表了第一个简化脑细胞的概念，即所谓的**麦卡洛克-皮茨**（**MCP**）神经元（参见《神经活动中内在思想的逻辑演算》由*W.
    S. 麦卡洛克*和*W. 皮茨*，*数学生物物理学公报*，5(4)：115-133，1943年）。
- en: 'Biological neurons are interconnected nerve cells in the brain that are involved
    in the processing and transmitting of chemical and electrical signals, which is
    illustrated in *Figure 2.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元是大脑中相互连接的神经细胞，参与处理和传递化学和电信号，如*图2.1*所示：
- en: '![Diagram  Description automatically generated](img/B17582_02_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_02_01.png)'
- en: 'Figure 2.1: A neuron processing chemical and electrical signals'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：神经元处理化学和电信号
- en: McCulloch and Pitts described such a nerve cell as a simple logic gate with
    binary outputs; multiple signals arrive at the dendrites, they are then integrated
    into the cell body, and, if the accumulated signal exceeds a certain threshold,
    an output signal is generated that will be passed on by the axon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 麦卡洛克和皮茨将这样的神经元描述为具有二进制输出的简单逻辑门；多个信号到达树突，然后被细胞体整合，如果累积信号超过一定阈值，则会生成一个输出信号，该信号将通过轴突传递。
- en: 'Only a few years later, Frank Rosenblatt published the first concept of the
    perceptron learning rule based on the MCP neuron model (*The Perceptron: A Perceiving
    and Recognizing Automaton* by *F. Rosenblatt*, *Cornell Aeronautical Laboratory*,
    1957). With his perceptron rule, Rosenblatt proposed an algorithm that would automatically
    learn the optimal weight coefficients that would then be multiplied with the input
    features in order to make the decision of whether a neuron fires (transmits a
    signal) or not. In the context of supervised learning and classification, such
    an algorithm could then be used to predict whether a new data point belongs to
    one class or the other.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 仅几年后，弗兰克·罗森布拉特基于MCP神经元模型发布了感知器学习规则的第一个概念（参见《感知器：一个感知和识别自动机》由*F. 罗森布拉特*，*康奈尔航空实验室*，1957年）。通过他的感知器规则，罗森布拉特提出了一种算法，可以自动学习最优权重系数，然后将其与输入特征相乘，以决定神经元是否发火（传递信号）。在监督学习和分类的背景下，这样的算法可以用于预测新数据点属于哪一类。
- en: The formal definition of an artificial neuron
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元的正式定义
- en: 'More formally, we can put the idea behind **artificial neurons** into the context
    of a binary classification task with two classes: 0 and 1\. We can then define
    a decision function, ![](img/B17582_02_001.png), that takes a linear combination
    of certain input values, *x*, and a corresponding weight vector, *w*, where *z*
    is the so-called net input *z* = *w*[1]*x*[1] + *w*[2]*x*[2] + ... + *w*[m]*x*[m]:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，我们可以将**人工神经元**的概念放入二分类任务（类别为0和1）的背景中。我们可以定义一个决策函数，![](img/B17582_02_001.png)，它接受某些输入值*x*和相应的权重向量*w*的线性组合，其中*z*称为所谓的净输入*z* = *w*[1]*x*[1] + *w*[2]*x*[2] + ... + *w*[m]*x*[m]：
- en: '![](img/B17582_02_002.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_002.png)'
- en: 'Now, if the net input of a particular example, **x**^(^i^), is greater than
    a defined threshold, ![](img/B17582_02_003.png), we predict class 1, and class
    0 otherwise. In the perceptron algorithm, the decision function, ![](img/B17582_02_004.png),
    is a variant of a **unit step function**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果特定示例**x**^(^i^)的净输入大于定义的阈值，![](img/B17582_02_003.png)，我们预测类别1；否则，预测类别0。在感知器算法中，决策函数，![](img/B17582_02_004.png)，是**单位阶跃函数**的一个变体：
- en: '![](img/B17582_02_005.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_005.png)'
- en: 'To simplify the code implementation later, we can modify this setup via a couple
    of steps. First, we move the threshold, ![](img/B17582_02_006.png), to the left
    side of the equation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化后续的代码实现，我们可以通过几个步骤修改此设置。首先，我们将阈值，![](img/B17582_02_006.png)，移动到方程的左侧：
- en: '![](img/B17582_02_007.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_007.png)'
- en: 'Second, we define a so-called *bias unit* as ![](img/B17582_02_008.png) and
    make it part of the net input:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将所谓的*偏置单元*定义为 ![](img/B17582_02_008.png)，并将其作为净输入的一部分：
- en: '*z* = *w*[1]*x*[1] + ... + *w*[m]*x*[m] + *b* = **w**^T**x** + *b*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*z* = *w*[1]*x*[1] + ... + *w*[m]*x*[m] + *b* = **w**^T**x** + *b*'
- en: 'Third, given the introduction of the bias unit and the redefinition of the
    net input *z* above, we can redefine the decision function as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，考虑到引入偏置单元和上述净输入*z*的重新定义，我们可以如下重新定义决策函数：
- en: '![](img/B17582_02_009.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_009.png)'
- en: '**Linear algebra basics: dot product and matrix transpose**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性代数基础：点积和矩阵转置**'
- en: 'In the following sections, we will often make use of basic notations from linear
    algebra. For example, we will abbreviate the sum of the products of the values
    in **x** and **w** using a vector dot product, whereas the superscript *T* stands
    for transpose, which is an operation that transforms a column vector into a row
    vector and vice versa. For example, assume we have the following two column vectors:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们经常会使用线性代数的基本符号。例如，我们将使用向量点积来简写**x**和**w**中值的乘积之和，而上标*T*代表转置，这是一种将列向量转换为行向量及其相反操作。例如，假设我们有以下两个列向量：
- en: '![](img/B17582_02_010.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_010.png)'
- en: Then, we can write the transpose of vector **a** as **a**^T = [*a*[1] *a*[2] *a*[3]]
    and write the dot product as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将向量**a**的转置表示为**a**^T = [*a*[1] *a*[2] *a*[3]]，并将点积表示为
- en: '![](img/B17582_02_011.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_011.png)'
- en: 'Furthermore, the transpose operation can also be applied to matrices to reflect
    it over its diagonal, for example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，转置操作也可以应用于矩阵，以反映其沿对角线的镜像，例如：
- en: '![](img/B17582_02_012.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_012.png)'
- en: Please note that the transpose operation is strictly only defined for matrices;
    however, in the context of machine learning, we refer to *n* × 1 or 1 × *m* matrices
    when we use the term “vector.”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，转置操作严格来说只适用于矩阵；然而，在机器学习的背景下，当我们使用术语“向量”时，我们指的是*n* × 1或1 × *m*矩阵。
- en: In this book, we will only use very basic concepts from linear algebra; however,
    if you need a quick refresher, please take a look at Zico Kolter’s excellent *Linear
    Algebra Review and Reference*, which is freely available at [http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们只会使用线性代数中非常基础的概念；然而，如果您需要快速复习，请查看Zico Kolter的出色的*线性代数复习和参考*，可在[http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf)免费获取。
- en: '*Figure 2.2* illustrates how the net input *z* = **w**^T**x** + *b* is squashed
    into a binary output (0 or 1) by the decision function of the perceptron (left
    subfigure) and how it can be used to discriminate between two classes separable
    by a linear decision boundary (right subfigure):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.2*说明了如何将净输入*z* = **w**^T**x** + *b*通过感知器的决策函数（左子图）压缩为二进制输出（0或1），以及如何使用它来区分可以通过线性决策边界分离的两个类别（右子图）：'
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_02_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17582_02_02.png)'
- en: 'Figure 2.2: A threshold function producing a linear decision boundary for a
    binary classification problem'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：阈值函数为二元分类问题生成线性决策边界
- en: The perceptron learning rule
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器学习规则
- en: 'The whole idea behind the MCP neuron and Rosenblatt’s *thresholded* perceptron
    model is to use a reductionist approach to mimic how a single neuron in the brain
    works: it either *fires* or it doesn’t. Thus, Rosenblatt’s classic perceptron
    rule is fairly simple, and the perceptron algorithm can be summarized by the following
    steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MCP神经元和Rosenblatt的*阈值*感知器模型背后的整个理念是使用还原主义方法模拟大脑中的单个神经元的工作方式：它要么*发射*，要么不发射。因此，Rosenblatt的经典感知器规则非常简单，感知器算法可以总结为以下步骤：
- en: Initialize the weights and bias unit to 0 or small random numbers
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重和偏置单元初始化为0或小的随机数。
- en: 'For each training example, **x**^(^i^):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练示例**x**^(^i^)，
- en: Compute the output value, ![](img/B17582_02_013.png)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出值，![图](img/B17582_02_013.png)
- en: Update the weights and bias unit
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重和偏置单元。
- en: 'Here, the output value is the class label predicted by the unit step function
    that we defined earlier, and the simultaneous update of the bias unit and each
    weight, *w*[j], in the weight vector, **w**, can be more formally written as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输出值是由我们之前定义的单位阶跃函数预测的类标签，而权重向量**w**中的偏置单元和每个权重*w*[j]的同时更新，可以更正式地写成：
- en: '![](img/B17582_02_014.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17582_02_014.png)'
- en: 'The update values (“deltas”) are computed as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更新值（“增量”）计算如下：
- en: '![](img/B17582_02_015.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17582_02_015.png)'
- en: 'Note that unlike the bias unit, each weight, *w*[j], corresponds to a feature,
    *x*[j], in the dataset, which is involved in determining the update value, ![](img/B17582_02_016.png),
    defined above. Furthermore, ![](img/B17582_02_017.png) is the **learning rate**
    (typically a constant between 0.0 and 1.0), *y*^(^i^) is the **true class label**
    of the *i*th training example, and ![](img/B17582_02_018.png) is the **predicted
    class label**. It is important to note that the bias unit and all weights in the
    weight vector are being updated simultaneously, which means that we don’t recompute
    the predicted label, ![](img/B17582_02_019.png), before the bias unit and all
    of the weights are updated via the respective update values, ![](img/B17582_02_020.png)
    and ![](img/B17582_02_021.png). Concretely, for a two-dimensional dataset, we
    would write the update as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与偏置单元不同，权重*w*[j]对应于数据集中的特征*x*[j]，它们参与确定上面定义的更新值![图](img/B17582_02_016.png)。此外，![图](img/B17582_02_017.png)是**学习率**（通常是一个介于0.0和1.0之间的常数），*y*^(^i^)是第*i*个训练示例的**真实类标签**，![图](img/B17582_02_018.png)是**预测类标签**。重要的是要注意，偏置单元和权重向量中的所有权重是同时更新的，这意味着在更新之前我们不重新计算预测标签![图](img/B17582_02_019.png)，直到通过相应的更新值![图](img/B17582_02_020.png)和![图](img/B17582_02_021.png)更新偏置单元和所有权重。具体来说，对于二维数据集，我们可以将更新写成：
- en: '![](img/B17582_02_022.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17582_02_022.png)'
- en: 'Before we implement the perceptron rule in Python, let’s go through a simple
    thought experiment to illustrate how beautifully simple this learning rule really
    is. In the two scenarios where the perceptron predicts the class label correctly,
    the bias unit and weights remain unchanged, since the update values are 0:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在Python中实现感知器规则之前，让我们通过一个简单的思想实验来说明这个学习规则有多简单。在感知器正确预测类标签的两种情况下，由于更新值为0，偏置单元和权重保持不变：
- en: (1) ![](img/B17582_02_023.png)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (1) ![图](img/B17582_02_023.png)
- en: (2) ![](img/B17582_02_024.png)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (2) ![图](img/B17582_02_024.png)
- en: 'However, in the case of a wrong prediction, the weights are being pushed toward
    the direction of the positive or negative target class:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在预测错误的情况下，权重被推向正类或负类的方向：
- en: (3) ![](img/B17582_02_025.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (3) ![图](img/B17582_02_025.png)
- en: (4) ![](img/B17582_02_026.png)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (4) ![图](img/B17582_02_026.png)
- en: 'To get a better understanding of the feature value as a multiplicative factor,
    ![](img/B17582_02_027.png), let’s go through another simple example, where:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解特征值作为乘法因子，![图](img/B17582_02_027.png)，让我们通过另一个简单的例子来说明：
- en: '![](img/B17582_02_028.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17582_02_028.png)'
- en: 'Let’s assume that ![](img/B17582_02_029.png) and we misclassify this example
    as *class 0*. In this case, we would increase the corresponding weight by 2.5
    in total so that the net input, ![](img/B17582_02_030.png), would be more positive
    the next time we encounter this example, and thus be more likely to be above the
    threshold of the unit step function to classify the example as *class 1*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设![](img/B17582_02_029.png)，我们将这个例子误分类为*类 0*。在这种情况下，我们会总共增加相应的权重 2.5，以便下次我们遇到这个例子时，净输入![](img/B17582_02_030.png)会更加正向，因此更有可能超过单位阶跃函数的阈值，将例子分类为*类
    1*：
- en: '![](img/B17582_02_031.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_031.png)'
- en: 'The weight update, ![](img/B17582_02_032.png), is proportional to the value
    of ![](img/B17582_02_027.png). For instance, if we have another example, ![](img/B17582_02_034.png),
    that is incorrectly classified as *class 0*, we will push the decision boundary
    by an even larger extent to classify this example correctly the next time:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新，![](img/B17582_02_032.png)，与![](img/B17582_02_027.png)的值成比例。例如，如果我们有另一个例子，![](img/B17582_02_034.png)，被错误地分类为*类
    0*，那么我们将进一步推动决策边界，以便下次正确分类这个例子：
- en: '![](img/B17582_02_035.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_035.png)'
- en: 'It is important to note that the convergence of the perceptron is only guaranteed
    if the two classes are linearly separable, which means that the two classes cannot
    be perfectly separated by a linear decision boundary. (Interested readers can
    find the convergence proof in my lecture notes: [https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf)).
    *Figure 2.3* shows visual examples of linearly separable and linearly inseparable
    scenarios:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，感知器的收敛仅在两类线性可分的情况下才保证，这意味着两类不能通过线性决策边界完全分离。（感兴趣的读者可以在我的讲义中找到收敛证明：[https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf)）。*图
    2.3*展示了线性可分和线性不可分场景的视觉示例：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_02_03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17582_02_03.png)'
- en: 'Figure 2.3: Examples of linearly and nonlinearly separable classes'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：线性可分和非线性可分类示例
- en: If the two classes can’t be separated by a linear decision boundary, we can
    set a maximum number of passes over the training dataset (**epochs**) and/or a
    threshold for the number of tolerated misclassifications—the perceptron would
    never stop updating the weights otherwise. Later in this chapter, we will cover
    the Adaline algorithm that produces linear decision boundaries and converges even
    if the classes are not perfectly linearly separable. In *Chapter 3*, we will learn
    about algorithms that can produce nonlinear decision boundaries.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两类不能通过线性决策边界分开，我们可以设置对训练数据集的最大遍历次数（**epochs**）和/或允许的误分类数量阈值。否则，感知器将永远不会停止更新权重。本章后面，我们将介绍Adaline算法，该算法产生线性决策边界，即使类别不完全线性可分。在*第三章*，我们将学习可以产生非线性决策边界的算法。
- en: '**Downloading the example code**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: If you bought this book directly from Packt, you can download the example code
    files from your account at [http://www.packtpub.com](http://www.packtpub.com).
    If you purchased this book elsewhere, you can download all code examples and datasets
    directly from [https://github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您直接从Packt购买了本书，可以从您在[http://www.packtpub.com](http://www.packtpub.com)的帐户中下载示例代码文件。如果您在其他地方购买了本书，可以直接从[https://github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book)下载所有代码示例和数据集。
- en: 'Now, before we jump into the implementation in the next section, what you just
    learned can be summarized in a simple diagram that illustrates the general concept
    of the perceptron:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们进入下一节的实现之前，你刚刚学到的内容可以用一个简单的图表来总结，说明感知器的一般概念：
- en: '![Diagram  Description automatically generated](img/B17582_02_04.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成的描述](img/B17582_02_04.png)'
- en: 'Figure 2.4: Weights and bias of the model are updated based on the error function'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：模型的权重和偏置根据误差函数进行更新
- en: The preceding diagram illustrates how the perceptron receives the inputs of
    an example (**x**) and combines them with the bias unit (**b**) and weights (**w**)
    to compute the net input. The net input is then passed on to the threshold function,
    which generates a binary output of 0 or 1—the predicted class label of the example.
    During the learning phase, this output is used to calculate the error of the prediction
    and update the weights and bias unit.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上图说明了感知器如何接收示例（**x**）的输入，并将其与偏置单元（**b**）和权重（**w**）结合起来计算净输入。然后，净输入传递给阈值函数，该函数生成0或1的二进制输出——示例的预测类标签。在学习阶段，此输出用于计算预测错误并更新权重和偏置单元。
- en: Implementing a perceptron learning algorithm in Python
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中实现感知器学习算法
- en: In the previous section, we learned how Rosenblatt’s perceptron rule works;
    let’s now implement it in Python and apply it to the Iris dataset that we introduced
    in *Chapter 1*, *Giving Computers the Ability to Learn from Data*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们学习了Rosenblatt的感知器规则的工作原理；现在让我们在Python中实现它，并将其应用于我们在*第1章*，*使计算机能够从数据中学习*中介绍的鸢尾花数据集。
- en: An object-oriented perceptron API
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个面向对象的感知器API
- en: We will take an object-oriented approach to defining the perceptron interface
    as a Python class, which will allow us to initialize new `Perceptron` objects
    that can learn from data via a `fit` method and make predictions via a separate
    `predict` method. As a convention, we append an underscore (`_`) to attributes
    that are not created upon the initialization of the object, but we do this by
    calling the object’s other methods, for example, `self.w_`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以面向对象的方式定义感知器接口作为Python类，这将允许我们通过`fit`方法初始化新的`Perceptron`对象，该对象可以从数据中学习，并通过单独的`predict`方法进行预测。作为惯例，我们在未初始化对象时通过调用对象的其他方法为属性添加下划线（`_`），例如`self.w_`。
- en: '**Additional resources for Python’s scientific computing stack**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python科学计算栈的其他资源**'
- en: 'If you are not yet familiar with Python’s scientific libraries or need a refresher,
    please see the following resources:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对Python的科学库还不熟悉或需要复习，请参阅以下资源：
- en: '**NumPy**: [https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**: [https://sebastianraschka.com/blog/2020/numpy-intro.html](https://sebastianraschka.com/blog/2020/numpy-intro.html)'
- en: '**pandas**: [https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**: [https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)'
- en: '**Matplotlib**: [https://matplotlib.org/stable/tutorials/introductory/usage.html](https://matplotlib.org/stable/tutorials/introductory/usage.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**: [https://matplotlib.org/stable/tutorials/introductory/usage.html](https://matplotlib.org/stable/tutorials/introductory/usage.html)'
- en: 'The following is the implementation of a perceptron in Python:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Python中感知器的实现：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this perceptron implementation, we can now initialize new `Perceptron`
    objects with a given learning rate, `eta` (![](img/B17582_02_036.png)), and the
    number of epochs, `n_iter` (passes over the training dataset).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此感知器实现，我们现在可以通过给定的学习率`eta`（![](img/B17582_02_036.png)）和迭代次数`n_iter`（训练数据集的遍数）来初始化新的`Perceptron`对象。
- en: Via the `fit` method, we initialize the bias `self.b_` to an initial value 0
    and the weights in `self.w_` to a vector, ![](img/B17582_02_037.png), where *m*
    stands for the number of dimensions (features) in the dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`fit`方法，我们将偏置`self.b_`初始化为初始值0，并将`self.w_`中的权重初始化为向量，![](img/B17582_02_037.png)，其中*m*表示数据集中的维度（特征）数量。
- en: Notice that the initial weight vector contains small random numbers drawn from
    a normal distribution with a standard deviation of 0.01 via `rgen.normal(loc=0.0,
    scale=0.01, size=1 + X.shape[1])`, where `rgen` is a NumPy random number generator
    that we seeded with a user-specified random seed so that we can reproduce previous
    results if desired.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，初始权重向量包含从标准偏差为0.01的正态分布中提取的小随机数，通过`rgen.normal(loc=0.0, scale=0.01, size=1
    + X.shape[1])`，其中`rgen`是一个NumPy随机数生成器，我们使用用户指定的随机种子进行了初始化，以便在需要时可以重现以前的结果。
- en: 'Technically, we could initialize the weights to zero (in fact, this is done
    in the original perceptron algorithm). However, if we did that, then the learning
    rate ![](img/B17582_02_036.png) (`eta`) would have no effect on the decision boundary.
    If all the weights are initialized to zero, the learning rate parameter, `eta`,
    affects only the scale of the weight vector, not the direction. If you are familiar
    with trigonometry, consider a vector, *v*1 =[1 2 3], where the angle between *v*1
    and a vector, *v*2 = 0.5 × *v*1, would be exactly zero, as demonstrated by the
    following code snippet:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们可以将权重初始化为零（事实上，这是原始感知器算法中所做的）。然而，如果我们这样做，学习率 ![](img/B17582_02_036.png)
    (`eta`) 将不会对决策边界产生影响。如果所有权重都初始化为零，则学习率参数 `eta` 只影响权重向量的规模，而不影响方向。如果你熟悉三角学，考虑一个向量
    *v*1 =[1 2 3]，其中向量 *v*2 = 0.5 × *v*1 的角度将完全为零，如以下代码片段所示：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `np.arccos` is the trigonometric inverse cosine, and `np.linalg.norm`
    is a function that computes the length of a vector. (Our decision to draw the
    random numbers from a random normal distribution—for example, instead of from
    a uniform distribution—and to use a standard deviation of `0.01` was arbitrary;
    remember, we are just interested in small random values to avoid the properties
    of all-zero vectors, as discussed earlier.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`np.arccos` 是反余弦三角函数，`np.linalg.norm` 是计算向量长度的函数。（我们决定从随机正态分布中抽取随机数，例如，而不是从均匀分布中抽取，并且使用标准偏差为
    `0.01`，这是任意的；请记住，我们只是希望获得小的随机值，以避免全零向量的特性，正如前面讨论的。）
- en: As an optional exercise after reading this chapter, you can change `self.w_
    = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])` to `self.w_ = np.zeros(X.shape[1])`
    and run the perceptron training code presented in the next section with different
    values for `eta`. You will observe that the decision boundary does not change.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章后，作为可选练习，你可以将`self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])`改为`self.w_
    = np.zeros(X.shape[1])`，然后使用不同的`eta`值运行下一节中呈现的感知器训练代码。你会观察到决策边界不会改变。
- en: '**NumPy array indexing**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy 数组索引**'
- en: NumPy indexing for one-dimensional arrays works similarly to Python lists using
    the square-bracket (`[]`) notation. For two-dimensional arrays, the first indexer
    refers to the row number and the second indexer to the column number. For example,
    we would use `X[2, 3]` to select the third row and fourth column of a two-dimensional
    array, `X`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一维数组，NumPy 的索引工作方式类似于 Python 列表，使用方括号 (`[]`) 表示法。对于二维数组，第一个索引器指定行号，第二个索引器指定列号。例如，我们使用
    `X[2, 3]` 来选择二维数组 `X` 中的第三行第四列。
- en: After the weights have been initialized, the `fit` method loops over all individual
    examples in the training dataset and updates the weights according to the perceptron
    learning rule that we discussed in the previous section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在权重初始化完成后，`fit` 方法遍历训练数据集中的所有单个示例，并根据我们在前一节讨论的感知器学习规则更新权重。
- en: The class labels are predicted by the `predict` method, which is called in the
    `fit` method during training to get the class label for the weight update; but
    `predict` can also be used to predict the class labels of new data after we have
    fitted our model. Furthermore, we also collect the number of misclassifications
    during each epoch in the `self.errors_` list so that we can later analyze how
    well our perceptron performed during the training. The `np.dot` function that
    is used in the `net_input` method simply calculates the vector dot product, **w**^T**x** + *b*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 类标签由 `predict` 方法预测，在训练期间在 `fit` 方法中调用以获取权重更新的类标签；但是 `predict` 也可以用于预测我们拟合模型后新数据的类标签。此外，我们还在
    `self.errors_` 列表中收集每个时期中的误分类数量，以便稍后分析我们的感知器在训练期间的表现。在 `net_input` 方法中使用的 `np.dot`
    函数简单地计算向量点积，**w**^T**x** + *b*。
- en: '**Vectorization: Replacing for loops with vectorized code**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量化：用矢量化代码替代for循环**'
- en: Instead of using NumPy to calculate the vector dot product between two arrays,
    `a` and `b`, via `a.dot(b)` or `np.dot(a, b)`, we could also perform the calculation
    in pure Python via `sum([i * j for i, j in zip(a, b)])`. However, the advantage
    of using NumPy over classic Python `for` loop structures is that its arithmetic
    operations are vectorized. Vectorization means that an elemental arithmetic operation
    is automatically applied to all elements in an array. By formulating our arithmetic
    operations as a sequence of instructions on an array, rather than performing a
    set of operations for each element at a time, we can make better use of our modern
    **central processing unit** (**CPU**) architectures with **single instruction,
    multiple data** (**SIMD**) support. Furthermore, NumPy uses highly optimized linear
    algebra libraries, such as **Basic Linear Algebra Subprograms** (**BLAS**) and
    **Linear Algebra Package** (**LAPACK**), that have been written in C or Fortran.
    Lastly, NumPy also allows us to write our code in a more compact and intuitive
    way using the basics of linear algebra, such as vector and matrix dot products.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用NumPy计算两个数组`a`和`b`之间的向量点积，可以通过`a.dot(b)`或`np.dot(a, b)`执行计算，我们也可以通过纯Python在`sum([i
    * j for i, j in zip(a, b)])`中执行计算。然而，使用NumPy而不是经典的Python `for`循环结构的优势在于，其算术操作是矢量化的。矢量化意味着将元素算术操作自动应用于数组中的所有元素。通过将我们的算术操作表述为对数组的一系列指令，而不是一次对每个元素执行一组操作，我们可以更好地利用具有**单指令多数据**（**SIMD**）支持的现代**中央处理单元**（**CPU**）架构。此外，NumPy使用高度优化的线性代数库，如**基本线性代数子程序**（**BLAS**）和**线性代数包**（**LAPACK**），这些库是用C或Fortran编写的。最后，NumPy还允许我们使用线性代数的基础以更紧凑和直观的方式编写代码，如向量和矩阵点积。
- en: Training a perceptron model on the Iris dataset
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在鸢尾花数据集上训练感知器模型
- en: To test our perceptron implementation, we will restrict the following analyses
    and examples in the remainder of this chapter to two feature variables (dimensions).
    Although the perceptron rule is not restricted to two dimensions, considering
    only two features, sepal length and petal length, will allow us to visualize the
    decision regions of the trained model in a scatterplot for learning purposes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的感知器实现，我们将在本章的其余部分限制以下分析和示例到两个特征变量（维度）。虽然感知器规则不限于两个维度，但仅考虑两个特征，萼片长度和花瓣长度，将允许我们在散点图中可视化训练模型的决策区域，以便学习目的。
- en: Note that we will also only consider two flower classes, setosa and versicolor,
    from the Iris dataset for practical reasons—remember, the perceptron is a binary
    classifier. However, the perceptron algorithm can be extended to multi-class classification—for
    example, the **one-versus-all** (**OvA**) technique.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，出于实际原因，我们还将仅考虑鸢尾花数据集中的两个花类别，山鸢尾和变色鸢尾——记住，感知器是一个二元分类器。然而，感知器算法可以扩展到多类分类，例如**一对全**（**OvA**）技术。
- en: '**The OvA method for multi-class classification**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**多类分类的OvA方法**'
- en: OvA, which is sometimes also called **one-versus-rest** (**OvR**), is a technique
    that allows us to extend any binary classifier to multi-class problems. Using
    OvA, we can train one classifier per class, where the particular class is treated
    as the positive class and the examples from all other classes are considered negative
    classes. If we were to classify a new, unlabeled data instance, we would use our
    *n* classifiers, where *n* is the number of class labels, and assign the class
    label with the highest confidence to the particular instance we want to classify.
    In the case of the perceptron, we would use OvA to choose the class label that
    is associated with the largest absolute net input value.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: OvA，有时也称为**一对多**（**OvR**），是一种技术，允许我们将任何二元分类器扩展到多类问题。使用OvA，我们可以为每个类别训练一个分类器，其中特定类别被视为正类，所有其他类别的示例被视为负类。如果我们要对新的未标记数据实例进行分类，我们将使用我们的*n*个分类器，其中*n*是类标签的数量，并将具有最高置信度的类标签分配给我们要分类的特定实例。在感知器的情况下，我们将使用OvA来选择与最大绝对净输入值相关联的类标签。
- en: 'First, we will use the `pandas` library to load the Iris dataset directly from
    the *UCI Machine Learning Repository* into a `DataFrame` object and print the
    last five lines via the `tail` method to check that the data was loaded correctly:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用`pandas`库直接从*UCI机器学习库*加载鸢尾花数据集到`DataFrame`对象，并通过`tail`方法打印最后五行来检查数据是否加载正确：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After executing the previous code, we should see the following output, which
    shows the last five lines of the Iris dataset:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，我们应该看到以下输出，显示了Iris数据集的最后五行：
- en: '![A picture containing calendar  Description automatically generated](img/B17582_02_05.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图，日历描述自动生成](img/B17582_02_05.png)'
- en: 'Figure 2.5: The last five lines of the Iris dataset'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：Iris数据集的最后五行
- en: '**Loading the Iris dataset**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载Iris数据集**'
- en: You can find a copy of the Iris dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or if the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)
    is temporarily unavailable. For instance, to load the Iris dataset from a local
    directory, you can replace this line,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在离线工作或者UCI服务器在[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)暂时不可用时，你可以在本书的代码包中找到Iris数据集（以及本书中使用的所有其他数据集）。例如，要从本地目录加载Iris数据集，可以替换此行，
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'with the following one:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下一行：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we extract the first 100 class labels that correspond to the 50 Iris-setosa
    and 50 Iris-versicolor flowers and convert the class labels into the two integer
    class labels, `1` (versicolor) and `0` (setosa), that we assign to a vector, `y`,
    where the `values` method of a pandas `DataFrame` yields the corresponding NumPy
    representation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取与50朵山鸢尾花和50朵变色鸢尾花对应的前100个类标签，并将这些类标签转换为两个整数类标签`1`（变色鸢尾花）和`0`（山鸢尾花），然后将其分配给向量`y`，其中pandas
    `DataFrame`的`values`方法产生了相应的NumPy表示。
- en: 'Similarly, we extract the first feature column (sepal length) and the third
    feature column (petal length) of those 100 training examples and assign them to
    a feature matrix, `X`, which we can visualize via a two-dimensional scatterplot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们从这100个训练示例中提取第一个特征列（萼片长度）和第三个特征列（花瓣长度），并将它们分配给特征矩阵`X`，我们可以通过二维散点图来可视化：
- en: '[PRE5]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After executing the preceding code example, we should see the following scatterplot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码示例后，我们应该看到以下散点图：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_02_06.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图，散点图描述自动生成](img/B17582_02_06.png)'
- en: 'Figure 2.6: Scatterplot of setosa and versicolor flowers by sepal and petal
    length'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：以萼片和花瓣长度分类的山鸢尾花和变色鸢尾花的散点图
- en: '*Figure 2.6* shows the distribution of flower examples in the Iris dataset
    along the two feature axes: petal length and sepal length (measured in centimeters).
    In this two-dimensional feature subspace, we can see that a linear decision boundary
    should be sufficient to separate setosa from versicolor flowers. Thus, a linear
    classifier such as the perceptron should be able to classify the flowers in this
    dataset perfectly.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.6*显示了Iris数据集中花例子在两个特征轴上的分布：花瓣长度和萼片长度（以厘米为单位）。在这个二维特征子空间中，我们可以看到线性决策边界应该足以将山鸢尾花和变色鸢尾花分开。因此，感知器这样的线性分类器应该能够完美地分类这个数据集中的花。'
- en: 'Now, it’s time to train our perceptron algorithm on the Iris data subset that
    we just extracted. Also, we will plot the misclassification error for each epoch
    to check whether the algorithm converged and found a decision boundary that separates
    the two Iris flower classes:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候在我们刚刚提取的Iris数据子集上训练感知器算法了。此外，我们将绘制每个epoch的误分类错误，以检查算法是否收敛并找到了能够分离两种Iris花类的决策边界：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that the number of misclassification errors and the number of updates
    is the same, since the perceptron weights and bias are updated each time it misclassifies
    an example. After executing the preceding code, we should see the plot of the
    misclassification errors versus the number of epochs, as shown in *Figure 2.7*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，误分类错误的数量和更新次数是相同的，因为每当感知器错误分类一个示例时，感知器的权重和偏置就会更新。执行上述代码后，我们应该能看到误分类错误与迭代次数的图示，如*图
    2.7*所示：
- en: '![](img/B17582_02_07.png)Figure 2.7: A plot of the misclassification errors
    against the number of epochs'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7：误分类错误与迭代次数的图示](img/B17582_02_07.png)'
- en: 'As we can see in *Figure 2.7*, our perceptron converged after the sixth epoch
    and should now be able to classify the training examples perfectly. Let’s implement
    a small convenience function to visualize the decision boundaries for two-dimensional
    datasets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图 2.7*中所看到的，我们的感知器在第六次epoch后收敛，现在应该能够完美地对训练示例进行分类。让我们实现一个小便捷函数来可视化二维数据集的决策边界：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we define a number of `colors` and `markers` and create a colormap from
    the list of colors via `ListedColormap`. Then, we determine the minimum and maximum
    values for the two features and use those feature vectors to create a pair of
    grid arrays, `xx1` and `xx2`, via the NumPy `meshgrid` function. Since we trained
    our perceptron classifier on two feature dimensions, we need to flatten the grid
    arrays and create a matrix that has the same number of columns as the Iris training
    subset so that we can use the `predict` method to predict the class labels, `lab`,
    of the corresponding grid points.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了一些`colors`和`markers`并通过`ListedColormap`从颜色列表创建了一个色彩映射。然后，我们确定了两个特征的最小值和最大值，并使用这些特征向量通过NumPy的`meshgrid`函数创建了一对网格数组`xx1`和`xx2`。由于我们在两个特征维度上训练了感知器分类器，我们需要展平网格数组并创建一个与鸢尾花训练子集相同列数的矩阵，以便我们可以使用`predict`方法来预测相应网格点的类标签`lab`。
- en: 'After reshaping the predicted class labels, `lab`, into a grid with the same
    dimensions as `xx1` and `xx2`, we can now draw a contour plot via Matplotlib’s
    `contourf` function, which maps the different decision regions to different colors
    for each predicted class in the grid array:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测的类标签`lab`重新整形为与`xx1`和`xx2`相同维度的网格后，我们现在可以通过Matplotlib的`contourf`函数绘制等高线图，该函数将不同决策区域映射到网格数组中每个预测类的不同颜色：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the preceding code example, we should now see a plot of the
    decision regions, as shown in *Figure 2.8*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码示例后，我们现在应该能看到一个决策区域的绘图，如*图2.8*所示：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_02_08.png)Figure
    2.8: A plot of the perceptron’s decision regions'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，散点图  自动生成描述](img/B17582_02_08.png)图2.8：感知器决策区域的绘图'
- en: As we can see in the plot, the perceptron learned a decision boundary that can
    classify all flower examples in the Iris training subset perfectly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，感知器学习了一个能够完美分类鸢尾花训练子集中所有样本的决策边界。
- en: '**Perceptron convergence**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器收敛性**'
- en: Although the perceptron classified the two Iris flower classes perfectly, convergence
    is one of the biggest problems of the perceptron. Rosenblatt proved mathematically
    that the perceptron learning rule converges if the two classes can be separated
    by a linear hyperplane. However, if the classes cannot be separated perfectly
    by such a linear decision boundary, the weights will never stop updating unless
    we set a maximum number of epochs. Interested readers can find a summary of the
    proof in my lecture notes at [https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器完美地分类了两类鸢尾花，但收敛是感知器的最大问题之一。罗森布拉特在数学上证明了，如果两个类可以通过线性超平面分开，感知器学习规则将收敛。然而，如果这些类不能通过线性决策边界完美分开，权重将永远不会停止更新，除非我们设置最大迭代次数。有兴趣的读者可以在我的讲义中找到这个证明的摘要，链接在[https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L03_perceptron_slides.pdf)。
- en: Adaptive linear neurons and the convergence of learning
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应线性神经元和学习的收敛
- en: 'In this section, we will take a look at another type of single-layer **neural
    network** (**NN**): **ADAptive LInear NEuron** (**Adaline**). Adaline was published
    by Bernard Widrow and his doctoral student Tedd Hoff only a few years after Rosenblatt’s
    perceptron algorithm, and it can be considered an improvement on the latter (*An
    Adaptive “Adaline” Neuron Using Chemical “Memistors”*, *Technical Report Number
    1553-2* by *B. Widrow and colleagues*, *Stanford Electron Labs*, Stanford, CA,
    *October* 1960).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍另一种单层**神经网络**（**NN**）：**自适应线性神经元**（**Adaline**）。 Adaline由伯纳德·维德罗和他的博士生泰德·霍夫在罗森布拉特感知器算法几年后发布，可以看作是后者的改进（*An
    Adaptive “Adaline” Neuron Using Chemical “Memistors”*, *Technical Report Number
    1553-2* by *B. Widrow and colleagues*, *Stanford Electron Labs*, Stanford, CA,
    *October* 1960）。
- en: The Adaline algorithm is particularly interesting because it illustrates the
    key concepts of defining and minimizing continuous loss functions. This lays the
    groundwork for understanding other machine learning algorithms for classification,
    such as logistic regression, support vector machines, and multilayer neural networks,
    as well as linear regression models, which we will discuss in future chapters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline算法特别有趣，因为它展示了定义和最小化连续损失函数的关键概念。这为理解其他用于分类的机器学习算法奠定了基础，如逻辑回归、支持向量机和多层神经网络，以及线性回归模型，我们将在后续章节中讨论。
- en: The key difference between the Adaline rule (also known as the **Widrow-Hoff
    rule**) and Rosenblatt’s perceptron is that the weights are updated based on a
    linear activation function rather than a unit step function like in the perceptron.
    In Adaline, this linear activation function, ![](img/B17582_02_039.png), is simply
    the identity function of the net input, so that ![](img/B17582_02_040.png).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline规则（也称为**Widrow-Hoff规则**）与Rosenblatt的感知器之间的关键区别在于权重的更新是基于线性激活函数而不是感知器中的单位阶跃函数。在Adaline中，这个线性激活函数
    ![](img/B17582_02_039.png) 简单地是净输入的恒等函数，因此 ![](img/B17582_02_040.png)。
- en: While the linear activation function is used for learning the weights, we still
    use a threshold function to make the final prediction, which is similar to the
    unit step function that we covered earlier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管线性激活函数用于学习权重，但我们仍然使用阈值函数来进行最终预测，这类似于我们之前讨论过的单位阶跃函数。
- en: 'The main differences between the perceptron and Adaline algorithm are highlighted
    in *Figure 2.9*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器和Adaline算法之间的主要区别在 *图 2.9* 中被突出显示：
- en: '![Diagram  Description automatically generated](img/B17582_02_09.png)Figure
    2.9: A comparison between a perceptron and the Adaline algorithm'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：感知器与Adaline算法的比较](img/B17582_02_09.png)Figure 2.9: 感知器和Adaline算法的比较'
- en: As *Figure 2.9* indicates, the Adaline algorithm compares the true class labels
    with the linear activation function’s continuous valued output to compute the
    model error and update the weights. In contrast, the perceptron compares the true
    class labels to the predicted class labels.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 2.9* 所示，Adaline算法将真实类标签与线性激活函数的连续值输出进行比较以计算模型误差并更新权重。相比之下，感知器将真实类标签与预测类标签进行比较。
- en: Minimizing loss functions with gradient descent
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用梯度下降最小化损失函数
- en: 'One of the key ingredients of supervised machine learning algorithms is a defined
    **objective function** that is to be optimized during the learning process. This
    objective function is often a loss or cost function that we want to minimize.
    In the case of Adaline, we can define the loss function, *L*, to learn the model
    parameters as the **mean squared error** (**MSE**) between the calculated outcome
    and the true class label:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习算法的一个关键组成部分是一个定义好的**目标函数**，在学习过程中要进行优化。这个目标函数通常是我们想要最小化的损失或成本函数。在Adaline的情况下，我们可以定义损失函数
    *L* 为模型参数学习的**均方误差**（**MSE**），即计算结果与真实类标签之间的平均平方误差：
- en: '![](img/B17582_02_041.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_041.png)'
- en: The term ![](img/B17582_02_042.png) is just added for our convenience and will
    make it easier to derive the gradient of the loss function with respect to the
    weight parameters, as we will see in the following paragraphs. The main advantage
    of this continuous linear activation function, in contrast to the unit step function,
    is that the loss function becomes differentiable. Another nice property of this
    loss function is that it is convex; thus, we can use a very simple yet powerful
    optimization algorithm called **gradient descent** to find the weights that minimize
    our loss function to classify the examples in the Iris dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语 ![](img/B17582_02_042.png) 只是为了方便起见，并且会使得推导损失函数对权重参数的梯度更加容易，正如我们将在下面的段落中看到的那样。这种连续的线性激活函数与单位阶跃函数相比的主要优势在于损失函数的可微性。这个损失函数的另一个好处是它是凸的；因此，我们可以使用一个非常简单但功能强大的优化算法，称为**梯度下降**，来找到最小化我们的损失函数以对Iris数据集中的示例进行分类的权重。
- en: 'As illustrated in *Figure 2.10*, we can describe the main idea behind gradient
    descent as *climbing down a hill* until a local or global loss minimum is reached.
    In each iteration, we take a step in the opposite direction of the gradient, where
    the step size is determined by the value of the learning rate, as well as the
    slope of the gradient (for simplicity, the following figure visualizes this only
    for a single weight, *w*):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 2.10* 所示，我们可以将梯度下降的主要思想描述为在达到局部或全局损失最小值之前 *向下爬山*。在每次迭代中，我们沿着梯度的反方向迈出一步，步长由学习速率的值以及梯度的斜率决定（为简单起见，以下图仅为单个权重
    *w* 可视化此过程）：
- en: '![Diagram  Description automatically generated](img/B17582_02_10.png)Figure
    2.10: How gradient descent works'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10：梯度下降的工作原理](img/B17582_02_10.png)Figure 2.10: 梯度下降的工作原理'
- en: 'Using gradient descent, we can now update the model parameters by taking a
    step in the opposite direction of the gradient, ![](img/B17582_02_043.png), of
    our loss function, *L*(**w**, *b*):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降，我们现在可以通过沿着损失函数 *L* 的梯度 ![](img/B17582_02_043.png) 的反方向来更新模型参数 *w* 和 *b*：
- en: '![](img/B17582_02_044.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_044.png)'
- en: 'The parameter changes, ![](img/B17582_02_045.png) and ![](img/B17582_02_046.png),
    are defined as the negative gradient multiplied by the learning rate, ![](img/B17582_02_036.png):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数变化，![](img/B17582_02_045.png) 和 ![](img/B17582_02_046.png)，被定义为负梯度乘以学习率 ![](img/B17582_02_036.png)：
- en: '![](img/B17582_02_048.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_048.png)'
- en: 'To compute the gradient of the loss function, we need to compute the partial
    derivative of the loss function with respect to each weight, *w*[j]:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算损失函数的梯度，我们需要计算损失函数对每个权重 *w*[j] 的偏导数：
- en: '![](img/B17582_02_049.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_049.png)'
- en: 'Similarly, we compute the partial derivative of the loss with respect to the
    bias as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们计算损失对偏置的偏导数为：
- en: '![](img/B17582_02_050.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_050.png)'
- en: Please note that the 2 in the numerator above is merely a constant scaling factor,
    and we could omit it without affecting the algorithm. Removing the scaling factor
    has the same effect as changing the learning rate by a factor of 2\. The following
    information box explains where this scaling factor originates.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分子中的2仅仅是一个常数缩放因子，我们可以省略它而不影响算法。去除缩放因子的效果与将学习率乘以2相同。以下信息框解释了这个缩放因子的来源。
- en: 'So we can write the weight update as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将权重更新写为：
- en: '![](img/B17582_02_051.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_051.png)'
- en: 'Since we update all parameters simultaneously, our Adaline learning rule becomes:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们同时更新所有参数，我们的Adaline学习规则变为：
- en: '![](img/B17582_02_052.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_052.png)'
- en: '**The mean squared error derivative**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差导数**'
- en: 'If you are familiar with calculus, the partial derivative of the MSE loss function
    with respect to the *j*th weight can be obtained as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉微积分，可以得到MSE损失函数对第*j*个权重的偏导数如下：
- en: '![](img/B17582_02_053.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_053.png)'
- en: The same approach can be used to find partial derivative ![](img/B17582_02_054.png)
    except that ![](img/B17582_02_055.png) is equal to –1 and thus the last step simplifies
    to ![](img/B17582_02_056.png).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的方法可以用来找到部分导数 ![](img/B17582_02_054.png)，除了 ![](img/B17582_02_055.png) 等于
    -1，因此最后一步简化为 ![](img/B17582_02_056.png)。
- en: Although the Adaline learning rule looks identical to the perceptron rule, we
    should note that ![](img/B17582_02_057.png) with ![](img/B17582_02_058.png) is
    a real number and not an integer class label. Furthermore, the weight update is
    calculated based on all examples in the training dataset (instead of updating
    the parameters incrementally after each training example), which is why this approach
    is also referred to as **batch gradient descent**. To be more explicit and avoid
    confusion when talking about related concepts later in this chapter and this book,
    we will refer to this process as **full batch gradient descent**.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Adaline学习规则看起来与感知器规则相同，我们应该注意 ![](img/B17582_02_057.png) 与 ![](img/B17582_02_058.png)
    是一个实数，而不是整数类标签。此外，权重更新是基于训练数据集中的所有示例计算的（而不是在每个训练示例之后逐步更新参数），这也是为什么这种方法被称为 **批量梯度下降**。为了更加明确，并且在本章和本书后续讨论相关概念时避免混淆，我们将这个过程称为
    **全批量梯度下降**。
- en: Implementing Adaline in Python
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现Adaline
- en: 'Since the perceptron rule and Adaline are very similar, we will take the perceptron
    implementation that we defined earlier and change the `fit` method so that the
    weight and bias parameters are now updated by minimizing the loss function via
    gradient descent:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于感知器规则和Adaline非常相似，我们将采用先前定义的感知器实现，并更改`fit`方法，以便通过梯度下降来最小化损失函数更新权重和偏置参数：
- en: '[PRE9]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Instead of updating the weights after evaluating each individual training example,
    as in the perceptron, we calculate the gradient based on the whole training dataset.
    For the bias unit, this is done via `self.eta * 2.0 * errors.mean()`, where `errors`
    is an array containing the partial derivative values ![](img/B17582_02_059.png).
    Similarly, we update the weights. However note that the weight updates via the
    partial derivatives ![](img/B17582_02_060.png) involve the feature values *x*[j],
    which we can compute by multiplying `errors` with each feature value for each
    weight:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与感知器不同，我们不是在评估每个单独的训练样例后更新权重，而是基于整个训练数据集计算梯度。对于偏置单元，这是通过`self.eta * 2.0 * errors.mean()`完成的，其中`errors`是包含偏导数值的数组![](img/B17582_02_059.png)。类似地，我们更新权重。然而，请注意，通过偏导数![](img/B17582_02_060.png)更新权重涉及特征值*x*[j]，我们可以通过将`errors`与每个权重的每个特征值相乘来计算它们：
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To implement the weight update more efficiently without using a `for` loop,
    we can use a matrix-vector multiplication between our feature matrix and the error
    vector instead:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更有效地实现权重更新而不使用`for`循环，我们可以在特征矩阵和误差向量之间进行矩阵-向量乘法：
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Please note that the `activation` method has no effect on the code since it
    is simply an identity function. Here, we added the activation function (computed
    via the `activation` method) to illustrate the general concept with regard to
    how information flows through a single-layer NN: features from the input data,
    net input, activation, and output.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`activation`方法对代码没有影响，因为它仅仅是一个恒等函数。在这里，我们添加了激活函数（通过`activation`方法计算得到）来说明信息如何通过单层神经网络传播的一般概念：来自输入数据的特征，净输入，激活和输出。
- en: In the next chapter, we will learn about a logistic regression classifier that
    uses a non-identity, nonlinear activation function. We will see that a logistic
    regression model is closely related to Adaline, with the only difference being
    its activation and loss function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于逻辑回归分类器的内容，它使用了非恒等、非线性的激活函数。我们将看到逻辑回归模型与Adaline密切相关，唯一的区别在于它们的激活和损失函数。
- en: Now, similar to the previous perceptron implementation, we collect the loss
    values in a `self.losses_` list to check whether the algorithm converged after
    training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，类似于之前的感知器实现，我们将损失值收集在`self.losses_`列表中，以检查算法在训练后是否收敛。
- en: '**Matrix multiplication**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵乘法**'
- en: 'Performing a matrix multiplication is similar to calculating a vector dot-product
    where each row in the matrix is treated as a single row vector. This vectorized
    approach represents a more compact notation and results in a more efficient computation
    using NumPy. For example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 执行矩阵乘法类似于计算向量点积，其中矩阵中的每一行被视为单行向量。这种向量化方法代表了更紧凑的表示法，并且利用NumPy进行更有效的计算。例如：
- en: '![](img/B17582_02_061.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_061.png)'
- en: Please note that in the preceding equation, we are multiplying a matrix with
    a vector, which is mathematically not defined. However, remember that we use the
    convention that this preceding vector is regarded as a 3×1 matrix.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述方程中，我们正在将一个矩阵与一个向量相乘，从数学上讲这是没有定义的。然而，请记住，我们使用的惯例是将此前述向量视为一个3×1矩阵。
- en: In practice, it often requires some experimentation to find a good learning
    rate, ![](img/B17582_02_036.png), for optimal convergence. So, let’s choose two
    different learning rates, ![](img/B17582_02_063.png) and ![](img/B17582_02_064.png),
    to start with and plot the loss functions versus the number of epochs to see how
    well the Adaline implementation learns from the training data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常需要一些实验来找到一个良好的学习率，![](img/B17582_02_036.png)，以实现最佳收敛。因此，让我们选择两个不同的学习率，![](img/B17582_02_063.png)和![](img/B17582_02_064.png)，开始，并绘制损失函数与迭代次数的图表，以查看Adaline实现从训练数据中学习的效果。
- en: '**Hyperparameters**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**超参数**'
- en: The learning rate, ![](img/B17582_02_065.png) (`eta`), as well as the number
    of epochs (`n_iter`), are the so-called hyperparameters (or tuning parameters)
    of the perceptron and Adaline learning algorithms. In *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*, we will take a look
    at different techniques to automatically find the values of different hyperparameters
    that yield optimal performance of the classification model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率，![](img/B17582_02_065.png)（`eta`），以及迭代次数（`n_iter`），是感知器和Adaline学习算法的所谓超参数（或调参参数）。在*第6章*，*学习模型评估和超参数调优的最佳实践*中，我们将看到不同的技术来自动找到不同超参数值，以获得分类模型的最佳性能。
- en: 'Let’s now plot the loss against the number of epochs for the two different
    learning rates:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制两种不同学习率下的损失随着迭代次数的变化图表：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we can see in the resulting loss function plots, we encountered two different
    types of problems. The left chart shows what could happen if we choose a learning
    rate that is too large. Instead of minimizing the loss function, the MSE becomes
    larger in every epoch, because we *overshoot* the global minimum. On the other
    hand, we can see that the loss decreases on the right plot, but the chosen learning
    rate, ![](img/B17582_02_064.png), is so small that the algorithm would require
    a very large number of epochs to converge to the global loss minimum:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在得到的损失函数图中所看到的，我们遇到了两种不同类型的问题。左图显示了如果选择的学习率过大可能会发生的情况。在每个 epoch 中，而不是最小化损失函数，MSE
    都会变大，因为我们*超调*了全局最小值。另一方面，我们可以看到右图中损失在减少，但是所选的学习率 ![](img/B17582_02_064.png) 太小，算法需要非常多的
    epoch 才能收敛到全局损失最小值：
- en: '![A picture containing icon  Description automatically generated](img/B17582_02_11.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图标描述](img/B17582_02_11.png)'
- en: 'Figure 2.11: Error plots for suboptimal learning rates'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.11: 子优化学习率下的误差图'
- en: '*Figure 2.12* illustrates what might happen if we change the value of a particular
    weight parameter to minimize the loss function, *L*. The left subfigure illustrates
    the case of a well-chosen learning rate, where the loss decreases gradually, moving
    in the direction of the global minimum.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.12* 描述了如果我们改变某个权重参数的值以最小化损失函数 *L*，可能会发生的情况。左侧的子图说明了选择得当的学习率的情况，损失逐渐减小，朝着全局最小值的方向移动。'
- en: 'The subfigure on the right, however, illustrates what happens if we choose
    a learning rate that is too large—we overshoot the global minimum:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的子图说明了，如果我们选择一个过大的学习率，会发生什么情况——我们会超调全局最小值：
- en: '![Diagram  Description automatically generated](img/B17582_02_12.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.12: 自动化生成的图表描述](img/B17582_02_12.png)'
- en: 'Figure 2.12: A comparison of a well-chosen learning rate and a learning rate
    that is too large'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.12: 选用适当学习率与学习率过大的比较'
- en: Improving gradient descent through feature scaling
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过特征缩放改进梯度下降
- en: Many machine learning algorithms that we will encounter throughout this book
    require some sort of feature scaling for optimal performance, which we will discuss
    in more detail in *Chapter 3*, *A Tour of Machine Learning Classifiers Using Scikit-Learn*,
    and *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中我们会遇到许多需要进行某种形式特征缩放以获得最佳性能的机器学习算法，在*第 3 章*，*使用 Scikit-Learn 的机器学习分类器概览*，和*第
    4 章*，*构建良好的训练数据集——数据预处理*中会详细讨论这一点。
- en: 'Gradient descent is one of the many algorithms that benefit from feature scaling.
    In this section, we will use a feature scaling method called **standardization**.
    This normalization procedure helps gradient descent learning to converge more
    quickly; however, it does not make the original dataset normally distributed.
    Standardization shifts the mean of each feature so that it is centered at zero
    and each feature has a standard deviation of 1 (unit variance). For instance,
    to standardize the *j*th feature, we can simply subtract the sample mean, ![](img/B17582_02_067.png),
    from every training example and divide it by its standard deviation, ![](img/B17582_02_068.png):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是许多算法之一，受益于特征缩放。在本节中，我们将使用一种称为**标准化**的特征缩放方法。这种标准化过程有助于梯度下降学习更快地收敛；然而，并不使原始数据集成为正态分布。标准化将每个特征的均值平移到零，并使每个特征具有标准偏差为
    1（单位方差）。例如，要对第 *j* 个特征进行标准化，我们可以简单地从每个训练样本中减去样本均值 ![](img/B17582_02_067.png)，并将其除以其标准差
    ![](img/B17582_02_068.png)：
- en: '![](img/B17582_02_069.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.11: 子优化学习率下的误差图](img/B17582_02_069.png)'
- en: Here, *x*[j] is a vector consisting of the *j*th feature values of all training
    examples, *n*, and this standardization technique is applied to each feature,
    *j*, in our dataset.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*[j] 是由所有训练样本 *n* 的第 *j* 个特征值组成的向量，这种标准化技术应用于数据集中的每个特征 *j*。
- en: 'One of the reasons why standardization helps with gradient descent learning
    is that it is easier to find a learning rate that works well for all weights (and
    the bias). If the features are on vastly different scales, a learning rate that
    works well for updating one weight might be too large or too small to update the
    other weight equally well. Overall, using standardized features can stabilize
    the training such that the optimizer has to go through fewer steps to find a good
    or optimal solution (the global loss minimum). *Figure 2.13* illustrates possible
    gradient updates with unscaled features (left) and standardized features (right),
    where the concentric circles represent the loss surface as a function of two model
    weights in a two-dimensional classification problem:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化有助于梯度下降学习的一个原因是，更容易找到适合所有权重（和偏差）的学习率。如果特征在非常不同的尺度上，那么对一个权重进行更新有效的学习率可能对另一个权重来说太大或太小。总体而言，使用标准化特征可以稳定训练过程，使优化器无需经过更多步骤就能找到良好或最优解（全局损失最小值）。*图
    2.13* 展示了未经标准化特征（左侧）和经过标准化特征（右侧）可能的梯度更新情况，其中同心圆代表了在二维分类问题中两个模型权重的损失表面函数：
- en: '![Diagram, engineering drawing  Description automatically generated](img/B17582_02_13.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![工程绘图描述自动生成](img/B17582_02_13.png)'
- en: 'Figure 2.13: A comparison of unscaled and standardized features on gradient
    updates'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：未经标准化和标准化特征上的梯度更新比较
- en: 'Standardization can easily be achieved by using the built-in NumPy methods
    `mean` and `std`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用内置的 NumPy 方法 `mean` 和 `std` 轻松实现标准化：
- en: '[PRE13]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After standardization, we will train Adaline again and see that it now converges
    after a small number of epochs using a learning rate of ![](img/B17582_02_070.png):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 经过标准化后，我们将再次训练 Adaline，并看到它现在在使用学习率为 ![](img/B17582_02_070.png) 的少量周期后收敛：
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After executing this code, we should see a figure of the decision regions,
    as well as a plot of the declining loss, as shown in *Figure 2.14*:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们应该看到决策区域的图以及损失下降的图，如 *图 2.14* 所示：
- en: '![Chart  Description automatically generated](img/B17582_02_14.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成描述的图表](img/B17582_02_14.png)'
- en: 'Figure 2.14: Plots of Adaline’s decision regions and MSE by number of epochs'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：Adaline 的决策区域和随着周期数变化的 MSE 的绘图
- en: As we can see in the plots, Adaline has now converged after training on the
    standardized features. However, note that the MSE remains non-zero even though
    all flower examples were classified correctly.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图中所示，Adaline 现在在训练标准化特征后已经收敛。但请注意，即使所有花的例子都被正确分类，MSE 仍保持非零。
- en: Large-scale machine learning and stochastic gradient descent
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大规模机器学习和随机梯度下降
- en: In the previous section, we learned how to minimize a loss function by taking
    a step in the opposite direction of the loss gradient that is calculated from
    the whole training dataset; this is why this approach is sometimes also referred
    to as full batch gradient descent. Now imagine that we have a very large dataset
    with millions of data points, which is not uncommon in many machine learning applications.
    Running full batch gradient descent can be computationally quite costly in such
    scenarios, since we need to reevaluate the whole training dataset each time we
    take one step toward the global minimum.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了通过沿着从整个训练数据集计算出的损失梯度的反方向迈出步伐来最小化损失函数；这也是为什么有时这种方法被称为全批量梯度下降。现在想象一下，我们有一个包含数百万数据点的非常大的数据集，在许多机器学习应用中这并不罕见。在这种情况下，运行全批量梯度下降可能在计算上非常昂贵，因为每次朝全局最小值迈出一步时，我们都需要重新评估整个训练数据集。
- en: 'A popular alternative to the batch gradient descent algorithm is **stochastic
    gradient descent** (**SGD**), which is sometimes also called iterative or online
    gradient descent. Instead of updating the weights based on the sum of the accumulated
    errors over all training examples, **x**^(^i^):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降算法的一种流行替代方案是**随机梯度下降**（**SGD**），有时也称为迭代或在线梯度下降。而不是基于所有训练样本上累积误差的和来更新权重，**x**^(^i^)：
- en: '![](img/B17582_02_071.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_071.png)'
- en: 'we update the parameters incrementally for each training example, for instance:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个训练样本逐步更新参数，例如：
- en: '![](img/B17582_02_072.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_072.png)'
- en: Although SGD can be considered as an approximation of gradient descent, it typically
    reaches convergence much faster because of the more frequent weight updates. Since
    each gradient is calculated based on a single training example, the error surface
    is noisier than in gradient descent, which can also have the advantage that SGD
    can escape shallow local minima more readily if we are working with nonlinear
    loss functions, as we will see later in *Chapter 11*, *Implementing a Multilayer
    Artificial Neural Network from Scratch*. To obtain satisfying results via SGD,
    it is important to present training data in a random order; also, we want to shuffle
    the training dataset for every epoch to prevent cycles.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SGD 可以被看作是梯度下降的一种近似，但它通常会因为更频繁的权重更新而更快地达到收敛。由于每个梯度是基于单个训练样本计算的，误差曲面比梯度下降中的要嘈杂，这也可以作为一个优势，即如果我们使用非线性损失函数，SGD
    更容易逃离浅层局部最小值，正如我们稍后在 *第11章* *从零开始实现多层人工神经网络* 中将看到的。通过 SGD 获得令人满意的结果，重要的是要以随机顺序呈现训练数据；此外，我们希望在每个
    epoch 中对训练数据集进行洗牌，以防止循环。
- en: '**Adjusting the learning rate during training**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整训练中的学习率**'
- en: 'In SGD implementations, the fixed learning rate, ![](img/B17582_02_065.png),
    is often replaced by an adaptive learning rate that decreases over time, for example:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SGD 的实现中，固定的学习率，![](img/B17582_02_065.png)，通常会被随着时间推移逐渐减小的自适应学习率所取代，例如：
- en: '![](img/B17582_02_074.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_02_074.png)'
- en: where *c*[1] and *c*[2] are constants. Note that SGD does not reach the global
    loss minimum but an area very close to it. And using an adaptive learning rate,
    we can achieve further annealing to the loss minimum.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *c*[1] 和 *c*[2] 是常数。注意，SGD 并不会达到全局损失最小值，而是非常接近它的某个区域。通过使用自适应学习率，我们可以进一步接近损失最小值。
- en: Another advantage of SGD is that we can use it for **online learning**. In online
    learning, our model is trained on the fly as new training data arrives. This is
    especially useful if we are accumulating large amounts of data, for example, customer
    data in web applications. Using online learning, the system can immediately adapt
    to changes, and the training data can be discarded after updating the model if
    storage space is an issue.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 的另一个优势是我们可以用它进行**在线学习**。在在线学习中，我们的模型会随着新的训练数据的到来而实时训练。如果我们积累了大量的数据，比如 Web
    应用中的客户数据，这尤其有用。利用在线学习，系统可以立即适应变化，并且如果存储空间有限，更新模型后可以丢弃训练数据。
- en: '**Mini-batch gradient descent**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**'
- en: A compromise between full batch gradient descent and SGD is so-called **mini-batch
    gradient descent**. Mini-batch gradient descent can be understood as applying
    full batch gradient descent to smaller subsets of the training data, for example,
    32 training examples at a time. The advantage over full batch gradient descent
    is that convergence is reached faster via mini-batches because of the more frequent
    weight updates. Furthermore, mini-batch learning allows us to replace the `for`
    loop over the training examples in SGD with vectorized operations leveraging concepts
    from linear algebra (for example, implementing a weighted sum via a dot product),
    which can further improve the computational efficiency of our learning algorithm.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在全批量梯度下降和 SGD 之间的一种折中方案是所谓的**小批量梯度下降**。小批量梯度下降可以理解为将全批量梯度下降应用于训练数据的较小子集，例如每次32个训练样本。与全批量梯度下降相比，通过小批量可以更快地达到收敛，因为权重更新更频繁。此外，小批量学习允许我们将在
    SGD 中遍历训练样本的 `for` 循环替换为利用线性代数概念的向量化操作（例如通过点积实现加权和），这可以进一步提高学习算法的计算效率。
- en: 'Since we already implemented the Adaline learning rule using gradient descent,
    we only need to make a few adjustments to modify the learning algorithm to update
    the weights via SGD. Inside the `fit` method, we will now update the weights after
    each training example. Furthermore, we will implement an additional `partial_fit`
    method, which does not reinitialize the weights, for online learning. In order
    to check whether our algorithm converged after training, we will calculate the
    loss as the average loss of the training examples in each epoch. Furthermore,
    we will add an option to shuffle the training data before each epoch to avoid
    repetitive cycles when we are optimizing the loss function; via the `random_state`
    parameter, we allow the specification of a random seed for reproducibility:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用梯度下降实现了 Adaline 学习规则，我们只需进行一些调整即可修改学习算法以通过 SGD 更新权重。在 `fit` 方法内部，我们现在会在每个训练示例后更新权重。此外，我们将实现一个额外的
    `partial_fit` 方法，该方法不会重新初始化权重，用于在线学习。为了在训练后检查算法是否收敛，我们将计算每个 epoch 中训练示例的平均损失。此外，我们将添加一个选项，在每个
    epoch 前对训练数据进行洗牌，以避免优化损失函数时出现重复循环；通过 `random_state` 参数，我们允许指定一个随机种子以实现可重现性。
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `_shuffle` method that we are now using in the `AdalineSGD` classifier
    works as follows: via the `permutation` function in `np.random`, we generate a
    random sequence of unique numbers in the range 0 to 100\. Those numbers can then
    be used as indices to shuffle our feature matrix and class label vector.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在 `AdalineSGD` 分类器中使用的 `_shuffle` 方法工作原理如下：通过 `np.random` 中的 `permutation`
    函数，我们生成一个在 0 到 100 范围内的唯一数字的随机序列。这些数字可以用作我们特征矩阵和类标签向量的索引，以对它们进行洗牌。
- en: 'We can then use the `fit` method to train the `AdalineSGD` classifier and use
    our `plot_decision_regions` to plot our training results:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `fit` 方法来训练 `AdalineSGD` 分类器，并使用我们的 `plot_decision_regions` 来绘制我们的训练结果：
- en: '[PRE16]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The two plots that we obtain from executing the preceding code example are
    shown in *Figure 2.15*:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从执行前述代码示例中获得的两个图表如 *图 2.15* 所示：
- en: '![Chart  Description automatically generated](img/B17582_02_15.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成描述的图表](img/B17582_02_15.png)'
- en: 'Figure 2.15: Decision regions and average loss plots after training an Adaline
    model using SGD'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：使用 SGD 训练 Adaline 模型后的决策区域和平均损失图
- en: As you can see, the average loss goes down pretty quickly, and the final decision
    boundary after 15 epochs looks similar to the batch gradient descent Adaline.
    If we want to update our model, for example, in an online learning scenario with
    streaming data, we could simply call the `partial_fit` method on individual training
    examples—for instance, `ada_sgd.partial_fit(X_std[0, :], y[0])`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，平均损失迅速下降，经过 15 个 epoch 后的最终决策边界看起来类似于批量梯度下降的 Adaline。如果我们想要在流式数据的在线学习场景中更新我们的模型，我们可以简单地对单个训练示例调用
    `partial_fit` 方法，例如 `ada_sgd.partial_fit(X_std[0, :], y[0])`。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we gained a good understanding of the basic concepts of linear
    classifiers for supervised learning. After we implemented a perceptron, we saw
    how we can train adaptive linear neurons efficiently via a vectorized implementation
    of gradient descent and online learning via SGD.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对监督学习的线性分类器的基本概念有了很好的理解。在实现了感知器之后，我们看到了如何通过梯度下降的向量化实现和通过 SGD 的在线学习高效地训练自适应线性神经元。
- en: Now that we have seen how to implement simple classifiers in Python, we are
    ready to move on to the next chapter, where we will use the Python scikit-learn
    machine learning library to get access to more advanced and powerful machine learning
    classifiers, which are commonly used in academia as well as in industry.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在 Python 中实现简单的分类器，我们准备进入下一章，在那里我们将使用 Python 的 scikit-learn 机器学习库来获取更先进和强大的机器学习分类器，这些分类器在学术界和工业界都广泛使用。
- en: 'The object-oriented approach that we used to implement the perceptron and Adaline
    algorithms will help with understanding the scikit-learn API, which is implemented
    based on the same core concepts that we used in this chapter: the `fit` and `predict`
    methods. Based on these core concepts, we will learn about logistic regression
    for modeling class probabilities and support vector machines for working with
    nonlinear decision boundaries. In addition, we will introduce a different class
    of supervised learning algorithms, tree-based algorithms, which are commonly combined
    into robust ensemble classifiers.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实现感知器和Adaline算法时采用的面向对象方法将有助于理解基于相同核心概念实现的 scikit-learn API，这些概念也是本章节的基础：`fit`
    和 `predict` 方法。基于这些核心概念，我们将学习用于建模类概率的逻辑回归，以及用于处理非线性决策边界的支持向量机。此外，我们还将介绍一类不同的监督学习算法，即基于树的算法，它们通常组合成强大的集成分类器。
- en: Join our book’s Discord space
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的 *问答* 会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
