- en: Other NN Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent networks are essentially feedforward networks that retain state. All
    the networks we have looked at so far require an input of a fixed size, such as
    an image, and give a fixed size output, such as the probabilities of a particular
    class. Recurrent networks are different in that they accept a sequence, of arbitrary
    size, as the input and produce a sequence as output. Moreover, the internal state
    of the network's hidden layers is updated as a result of a learned function and
    the input. In this way, a recurrent network remembers its state. Subsequent states
    are a function of previous states.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to recurrent networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent networks have been shown to be very powerful in predicting time series
    data. This is something fundamental to biological brains that enables us to do
    things such as safely drive a car, play a musical instrument, evade predators,
    understand language, and interact with a dynamic world. This sense of the flow
    of time and the understanding of how things change over time is fundamental to
    intelligent life, so it is no surprise that in artificial systems this ability
    is important.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to understand time series data is also important in creative endeavors,
    and recurrent networks have shown some ability in things such as composing a melody,
    constructing grammatically correct sentences, and creating visually pleasing images.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward and convolutional networks achieve very good results, as we have
    seen, in tasks such as the classification of static images. However, working with
    continuous data, as is required for tasks such as speech or handwriting recognition,
    predicting stock market prices, or forecasting the weather requires a different
    approach. In these types of tasks, both the input and the output are no longer
    a fixed size of data, but a sequence of arbitrary length.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent artificial neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For artificial neurons in feedforward networks, the flow of activation is simply
    from the input to the output. **Recurrent artificial neurons** (**RANs**) have
    a connection from the output of the activation layer to its linear input, essentially
    summing the output back into the input. A RAN can be *unrolled* in time: each
    subsequent state is a function of previous states. In this way, a RAN can be said
    to have a memory of its previous states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4845bc8-64be-4387-b11c-4fb06a3bbe93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, the diagram on the left illustrates a single recurrent
    neuron. It sums its input, `x`, with the output, `y`, to produce a new output.
    The diagram on the right shows this same unit unrolled over three time steps.
    We can write an equation for the output with respect to the input for any given
    time step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/287b4383-2d94-4118-885a-e8a9f9ee6b03.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y(t)* is the output vector at time *t*, *x[(t)]* is the input at time *t*, *y[(t-1)]*
    is the output of the previous time step, *b* is the bias term, and *Φ* is the
    activation, usually tanh or RelU. Notice that each unit has two sets of weights, *w[x]*
    and *w[y]*, for the inputs and the outputs respectively. This is, essentially,
    the formula we used for our linear networks with an added term to represent the
    output, fed back into the input at time *t-1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way that with CNNs (Convolutional Neural Networks) we could compute
    the outputs of an entire layer over a batch, using a vectorized form of the previous
    equation, this is also possible with recurrent networks. The following is the
    vectorized form for a recurrent layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd96e382-4f68-43c9-aab9-5ab570a90f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *Y[(t)]* is the output at time *t*. This is a matrix of size (*m*, *n*),
    where *m* is the number of instances in the batch and *n* is the number of units
    in the layer. *X[(t)]* is a matrix of size (*m, i)* where *i* is the number of
    input features. *W[x]* is a matrix of size (*i, n),* containing the input weights
    of the current time step. *W[y]* is a matrix of size (*n*, *n*), containing the
    weights of the outputs for the previous time step.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a recurrent network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So we can concentrate on the models, we will use the same dataset we are familiar
    with. Even though we are working with static images, we can treat these as a time
    series by unrolling each 28 pixel input size over 28 time steps, enabling the
    network to make a computation on the complete image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/695e96a7-25f6-4472-b4fa-30e2dde92cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding model, we use the `nn.RNN` class to create a model with two
    recurrent layers. The `nn.RNN` class has the following default signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The input is our 28 x 28 MNIST images. This model takes 28 pixels of each image,
    unrolling them over 28 time steps to make a computation over the entirety of all
    images in the batch. The `hidden_size` parameter is the dimension of the hidden
    layers, and this is something we choose. Here, we use a size of `100`. The `batch_first`
    parameter specifies the expected shape of the input and output tensors. We want
    this to have the shape in the form of (batch, sequence, features). In this example,
    the expected input/output tensor shape we want is (`100, 28, 28`). That is the
    batch size, the length of the sequence, and the number of features at each step;
    however, by default the `nn.RNN` class uses input/output tensors of the form (sequence,
    batch, features). Setting `batch_first = True` ensures the input/output tensors
    are of the shape (batch, sequence, features).
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward` method, we initialize a tensor for the hidden layer, `h0`,
    that is updated on every iteration of the model. The shape of this hidden tensor,
    representing the hidden state, is of the form (layers, batch, hidden). In this
    example, we have two layers. The second dimension of the hidden state is the batch
    size. Remember, we are using batch first so this is the first dimension of the
    input tensor, `x`, indexed using `x[0]`. The final dimension is the hidden size,
    which in this example we have set to `100`.
  prefs: []
  type: TYPE_NORMAL
- en: The `nn.RNN` class requires an input consisting of the input tensor, `x`, and
    the `h0` hidden state. This is why in the `forward` method, we pass in these two
    variables. The `forward` method is called once every iteration, updating the hidden
    state and giving an output. Remember, number of iterations is the number of epochs
    multiplied by the data size divided by the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, as you can see, we need to index the output using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We are only interested in the output of the last time step, since this is the
    accumulated knowledge of all the images in the batch. If you remember, the output
    shape is of the form (batch, sequence, features) and in our model this is `(100,
    28, 100)`. The number of features is simply the number of hidden dimensions or
    number of units in the hidden layer. Now, we require all batches: this is why
    we use the colon as the first index. Here, `-1` indicates we only want the last
    element of the sequence. The last index, the colon, indicates we want all of the
    features. Hence, our output is all the features of the last time step in the sequence,
    for one entire batch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use almost identical training code; however, we do need to reshape the
    output when we call the model. Remember that for linear models, we reshaped the
    output using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For convolution networks, by using `nn.CNN` we could pass in the unflattened
    image and for recurrent networks, when using `nn.RNN` we need the output to be
    of the form (batch, sequence, features). Therefore, we can use the following to
    reshape the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember, we need to change this line in both our training code and testing
    code. The following printout is the result of running three recurrent models using
    different layer and hidden size configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a0d6d67-9bb8-4cea-b1f0-b6cc9def140e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To get a better understanding of how this model works, consider the following
    diagram, representing our two-layer model with a hidden size of `100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb219ae4-87ab-4346-b7fb-8005c36ad8fc.png)'
  prefs: []
  type: TYPE_IMG
- en: At each of the **28** time steps the network takes an input, consisting of **28**
    pixels—the features—from each of the images in the **100** image batch. Each of
    the time steps are basically a two-layer feedforward network. The only difference
    is that there is an extra input to each of the hidden layers. This input consists
    of the output from the equivalent layer in the previous time step. At each time
    step, another **28** pixels are sampled from each of the **100** images in the
    batch. Finally, when the entirety of all the images in the batch have been processed,
    the weights of the model are updated and the next iteration begins. Once all iterations
    are complete, we read the output to obtain a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of what happens when we run the code, consider
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/967a651a-c2c3-460e-8342-2e4fe6d4566f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we print out the size of the weight vectors for a two-layer RNN model
    with a hidden layer size of `100`.
  prefs: []
  type: TYPE_NORMAL
- en: We retrieve the weights as a list containing `10` tensors. The first tensor
    of size `[100, 28]` consists of the inputs to the hidden layer, `100` units, and
    the `28` features, or pixels, of the input images. This is the *W*[*x* ]term in
    the vectorized form equation of the recurrent network. The next group of parameters,
    size `[100, 100]`, represented by the *W[y]* term in the preceding equation, is
    the output weights of the hidden layer, consisting of the `100` units each of
    size `100`. The next two single-dimension tensors, each of size `100`, are the
    bias units of the input and the hidden layer respectively. Next, we have the input
    weights, output weights, and biases of the second layer. Finally, we have the
    read out layer weights, a tensor of size `[10, 100]`, for `10` possible predictions
    using `100` features. The final single-dimension tensor of size `[10]` includes
    the bias units for the read out layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we have replicated what is happening in the recurrent
    layers of our model over a single batch of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31b1e0d3-e2f6-4389-a44a-2d7a6b1776d4.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that we have simply created an iterator out of the `trainLoader`
    dataset object and assigned an `images` variable to a batch of images, as we did
    for our training code. The hidden layer, `h0`, needs to contain two tensors, one
    for each layer. In each of these tensors, for each image in the batch, the weights
    of the `100` hidden units are stored. This explains why we need a three-dimensional
    tensor. The first dimension of size `2` for the number of layers, the second dimension
    is of size `100` for the batch size, obtained from `images.size(0)`, and the third
    dimension is of size `100` for the number of hidden units. We then pass a reshaped
    image tensor and our hidden tensor to the model. This calls the model's `forward()`
    function, making the necessary computations, and returning two tensors an output
    tensor, and an updated hidden state tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following confirms these output sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b42a599-054b-4f56-9760-219f9ce3a197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This should help you understand why we need to resize the `images` tensor.
    Note that the features for the input are the `28` pixels for each of the images
    in the batch, which are unrolled over the sequence of `28` time steps. Next, let''s
    pass the output of the recurrent layer to our fully connected linear layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51843d49-3dd9-46f3-8ad6-5f898419c92e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that this will give us `10` predictions for each of the `100` features
    present in the output. This is why we need to index only the last element in the
    sequence. Remember the output from `nn.RNN` is of size (`100, 28, 100`). Note
    what happens to the size of this tensor when we index it using `-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4eb2a84-ea32-4a76-9483-38538e42e7ad.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the tensor containing the `100` features, the outputs of the hidden
    units, for each of the `100` images in the batch. This is passed to our linear
    layer to give the required `10` predictions for each image.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Long short-term memory networks (****LSTMS**), are a special type of RNN
    capable of learning long-term dependencies. While standard RNNs can remember previous
    states to some extent, they did this on a fairly basic level by updating a hidden
    state on each time step. This enabled the network to remember short-term dependencies.
    The hidden state, being a function of previous states, retains information about
    these previous states. However, the more time steps there are between the current
    state and a previous state, it diminishes the effect that this earlier state will
    have on the current state. Far less information is retained on a state that is
    say `10` time steps before the time step immediately preceding the current step.
    This is despite that fact that earlier time steps may contain important information
    with direct relevance to a particular problem or task we are trying to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: Biological brains have a remarkable ability to remember long-term dependencies,
    forming meaning and understanding using these dependencies. Consider how we follow
    the plot of a movie. We recall events that occurred at the beginning of the movie
    and immediately understand their relevance as the plot develops. Not only that,
    but we can apply context to the movie by recalling events in our own lives that
    give relevance and meaning to a story line. This ability to selectively apply
    memories to current context, yet at the same time filter out irrelevant details,
    is the strategy behind the design of LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM network is an attempt to incorporate these long-term dependencies into
    an artificial network. It is considerably more complex than a standard RNN; however,
    it is still based on recurrent feedforward networks and understanding this theory
    should enable you to understand LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an LSTM over one single time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fd63014-2c20-4b17-b7a2-ea6d998a8054.png)'
  prefs: []
  type: TYPE_IMG
- en: As with normal RNNs, each subsequent time step takes the hidden state of the
    previous time step, **h[t-1]**, along with the data, **x**[**t**][,] as its input.
    An LSTM also passes on a cell state that is calculated on each time step. You
    can see that **h[t-1]** and **x[t]** are each passed to four separate linear functions.
    Each pair of these linear functions is summed. Central to an LSTM are the four
    gates that these summations are passed in to. First, we have the **Forget Gate**.
    This uses a **sigmoid** for activation and is element-wise multiplied by the **Old
    Cell State**. Remember the **sigmoid** is effectively squashing the **linear**
    output values to values between zero and one. Multiplying by zero will effectively
    eliminate that particular value in the cell state and multiplying by one will
    keep this value. The **Forget Gate** essentially decides what information is passed
    to the next time step. This is achieved by element-wise multiplication with the
    **Old Cell State**.
  prefs: []
  type: TYPE_NORMAL
- en: The **Input Gate** and the **Scaled new candidate** gate together determine
    what information is retained. The **Input Gate** also uses a **sigmoid** function
    and this is multiplied by the output of a **New Candidate** gate, creating a temporary
    tensor, the **Scaled new candidate**, **c[2]**. Note that the **New Candidate**
    gate uses **tanh** activation. Remember the **tanh** function outputs a value
    between `-1` and `1`. Using **tanh** and **sigmoid** activation in such a way—that
    is, by element-wise multiplication of their outputs—helps prevent the vanishing
    gradient problem, where outputs become saturated and their gradients repeatedly
    become close to zero, making them unable to perform meaningful computations. A
    **New Cell State** is calculated by summing the **Scaled new candidate** with
    the **Scaled Old Cell State**, and in this way is able to amplify important components
    of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: The final gate, the output gate, **O***,* is another **sigmoid**. The new cell
    state is passed through a **tanh** function and this is element-wise multiplied
    by the output gate to calculate the **Hidden State**. This **Hidden State**, as
    with standard RNNs, is passed through a final non-linearity, a **sigmoid**, and
    a **Softmax** function to give the outputs. This has the overall effect of reinforcing
    high energy components, eliminating the lower energy components, as well as reducing
    the opportunities for vanishing gradients and reducing overfitting of the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the equations for each of the LSTM gates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b00d02f-bda6-4d4b-a621-f57869adb2ae.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/50eec000-b45e-40c4-8af2-855a00f47712.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/d99dc7e7-9c88-42dd-99c3-bc0ebd847d75.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5a23b88d-addc-4eb6-a049-9870c24edeab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that these equations have an identical form to those of the RNN. The
    only difference is that we require eight separate weight tensors and eight bias
    tensors. It is these extra weight dimensions that give LSTMs their extra ability
    to learn and retain important features of the input data, as well as discard less
    important features. We can write the output of the linear output layer, of a particular
    time step, *t*, as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16677b56-d59e-4b67-be36-f1740851bf1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing an LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the LSTM model class we will use for MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b1e1030-3e8b-47f9-a58c-fffa6bc4cfa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that `nn.LSTM` is passed the same arguments as the previous RNN. This
    is not surprising, since LSTM is a recurrent network that works on sequences of
    data. Remember the input tensor has an axis of the form `(batch, sequence, feature)`,
    so we set `batch_first = True`. We initialize a fully connected linear layer for
    the output layer. Notice in the `forward` method that, as well as initializing
    a hidden state tensor, `h0`, we also initialize a tensor to hold the cell state,
    `c0`. Remember also the `out` tensor contains all `28` time steps. For our prediction,
    we are only interested in the last index in the sequence. This is why we apply
    the `[:, -1, :]` indexing to the `out` tensor before passing it to the linear
    output layer. We can print out the parameters for this model in the same way as
    for the RNN previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b260399-17b5-49ae-a0d7-b743ce335ff5.png)'
  prefs: []
  type: TYPE_IMG
- en: These are the parameters for a single-layer LSTM with `100` hidden layers. There
    are six groups of parameters for this single-layer LSTM. Notice that instead of
    the input and hidden weight tensors having a size of `100` in the first dimension,
    as was the case for the RNN, for an LSTM, this is a size of `400`, representing `100`
    hidden units for each of the four LSTM gates.
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter tensor is for the input layer and is of size `[400,28]`.
    The first index, `400`, corresponds to the weights *w[1]*, *w[3]*, *w[5]*, and *w[7]*,
    each of which is of size `100`, for the inputs into the `100` hidden units specified.
    The `28` is the number of features, or pixels, present at the input. The next
    tensor, of size `[400,100]`, are the weights *w[2]*, *w[4]*, *w[6]*, and *w[8]* for
    each of `100` hidden units. The following two single-dimension tensors of size
    `[400]` are the two sets of bias units, *b[1]*, *b[3]*, *b[5]*, *b[7]* and *b[2]*,
    *b[4]*, *b[6]*, *b[8]*, for each of the LSTM gates. Finally, we have the output
    tensor of size `[10, 100]`. This is our output size, `10`, and the weight tensor
    *w[9. ]*The last single-dimension tensor of size `[10]` is the bias, *b9.*
  prefs: []
  type: TYPE_NORMAL
- en: Building a language model with a gated recurrent unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate the flexibility of recurrent networks, we are going to do something
    different in the final section of this chapter. Up until now, we have been working
    with probably the most-used testing data set, MNIST. This dataset has characteristics
    that are well known and it is extremely useful for comparing different types of
    models and testing different architectures and parameter sets. However, there
    are some tasks, such as natural language processing, that quite obviously require
    an entirely different type of dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the models we have built so far have been focused on one simple task:
    classification. This is the most straightforward machine learning task. To give
    you a flavor of other machine learning tasks, and to demonstrate the potential
    of recurrent networks, the model we are going to build is a character-based prediction
    model that attempts to predict each subsequent character based on the previous
    character, forming a learned body of text. The model first learns to create correct
    vowel—consonant sequences, words, and eventually sentences and paragraphs that
    mimic the form (but not the meaning) of those constructed by human authors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an adaption of code written by Sean Robertson and Pratheek
    that can be found here: [https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb).
    Here is the model definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab215786-2550-43f4-9ab7-7d2818b425d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The purpose of this model is to take an input character at each time step, and
    output the most likely next character. Over subsequent training, it begins to
    build up sequences of characters that mimic text from a training sample. Our input
    and output sizes are simply the number of characters in the input text, and this
    is calculated and passed in as a parameter to the model. We initialize an encoder
    tensor using the `nn.Embedding` class. In a similar way to how we used one hot
    encoding to define a unique index for each word, the `nn.Embedding` module stores
    each word in a vocabulary as a multidimensional tensor. This enables us to encode
    semantic information in the word embedding. We need to pass the `nn.Embedding`
    module a vocabulary size—here, this is the number of characters in the input text—and
    a dimensionality in which to encode each character—here, this is the hidden size
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The word embedding model we are using is based on the `nn.GRU` module, or GRU.
    This is very similar to the LSTM module we used in the previous section. The difference
    is that GRU is a slightly simplified version of LSTM. It combines the input and
    forget gates into a single update gate, and combines the hidden state with the
    cell state. The result is that the GRU is more efficient than LSTM for many tasks.
    Finally, a linear output layer is initialized to decode the output from the GRU.
    In the `forward` method, we resize the input and pass it through the linear embedding
    layer, the GRU, and the final linear output layer, returning the hidden state
    and the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to import the data, and initialize variables containing the printable
    characters of our input text and the number of characters in the input text. Note
    the use of `unidecode` to remove non-unicode characters. You will need to import
    this module and possibly install it on your system if it is not installed already.
    We also define two convenience functions: a function to convert a character string
    into the integer equivalent of each Unicode character, and another function to
    sample random chunks of training text. The `random_training_set` function returns
    two tensors. The `inp` tensor contains all characters in the chunk, excluding
    the last character. The `target` tensor contains all elements of the chunk offset
    by one and so includes the last character. For example, if we were using a chunk
    size of `4`, and this chunk consisted of the Unicode character representations
    of `[41, 27, 29, 51]`, then the `inp` tensor would be `[41, 27, 29]` and the `target`
    tensor `[27, 29, 51]`. In this way, the target can train a model to make a prediction
    on the next character using target data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96c4d2ad-bf56-4b88-9bd2-f491fb53ce27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we write a method to evaluate the model. This is done by passing it one
    character at a time: the model outputs a multinomial probability distribution
    for the next most likely character. This is repeated to build up a sequence of
    characters, storing them in the `predicted` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f7deb61-d682-40cc-985f-cd504b962315.png)'
  prefs: []
  type: TYPE_IMG
- en: The `evaluate` function takes a `temperature` argument that divides the output
    and finds the exponent to create a probability distribution. The `temperature`
    argument has the effect of determining the level of probability required for each
    prediction. For temperature values above `1`, characters with lower probabilities
    are generated, the resulting text being more random. For lower temperature values
    below `1`, higher probability characters are generated. With temperature values
    close to `0`, only the most likely characters will be generated. For each iteration,
    a character is added to the `predicted` string until the required length, determined
    by the `predict_len` variable, is reached and the `predicted` string is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function trains the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/647f4ef9-1734-4c6b-963a-66b3076408af.png)'
  prefs: []
  type: TYPE_IMG
- en: We pass it the input chunk and the target chunk. The `for` loop runs the model
    through one iteration for each character in the chunk, updating the `hidden` state
    and returning the average loss for each character.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to instantiate and run the model. This is done with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f027928-48d8-4ee1-be2c-31400e1cabd1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the usual variables are initialized. Notice that we are not using stochastic
    gradient descent for our optimizer, but rather use the Adam optimizer. The term
    Adam stands for *adaptive moment estimator*. Gradient descent uses a single fixed
    learning rate for all learnable parameters. The Adam optimizer uses an adaptive
    learning rate that maintains a per parameter learning rate. It can improve learning
    efficiency, particularly in sparse representations such as those used for natural
    language processing. Sparse representations are those where most of the values
    in a tensor are zero, for example in one-hot encoding or word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Once we run the model, it will print out the predicted text. At first, the text
    appears as almost random sequences of characters; however, after a few cycles
    of training, the model learns to format the text into English-like sentences and
    phrases. Generative models are powerful tools, enabling us to uncover probability
    distributions in input data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced recurrent neural networks and demonstrated how
    to use an RNN on the MNIST dataset. RNNs are particularly useful for working with
    time series data, since they are essentially feedforward networks that are unrolled
    over time. This makes them very suitable for tasks such as handwriting and speech
    recognition, as they operate on sequences of data. We also looked at a more powerful
    variant of the RNN, the LSTM. The LSTM uses four gates to decide what information
    to pass on to the next time step, enabling it to uncover long-term dependencies
    in data. Finally, in this chapter we built a simple language model, enabling us
    to generate text from sample input text. We used a model based on the GRU. The
    GRU is a slightly simplified version of the LSTM, containing three gates and combining
    the input and forget gates of the LSTM. This model used probability distributions
    to generate text from a sample input.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will examine some advanced features of PyTorch, such
    as using PyTorch in multiprocessor and distributed environments. We also see how
    to fine-tune PyTorch models and use pre-trained models for flexible image classification.
  prefs: []
  type: TYPE_NORMAL
